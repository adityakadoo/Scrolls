<!doctype html><html lang=en-us><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="ie=edge"><meta name=author content="Aditya Kadoo"><meta name=description content="Aditya Kadoo (200050055)"><link rel=icon href=https://adityakadoo.github.io/Scrolls/favicon.ico><meta name=keywords content=" hugo  latex  theme "><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css integrity=sha384-KiWOvVjnN8qwAZbuQyWDIbfCLFhLXNETzBQjA/92pIowpC0d2O3nppDGQVgwd2nB crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js integrity=sha384-0fdwu/T/EQMsQlrHCCHoH10pkPLlKA1jL5dFyUOvB3lfeT2540/2g6YgSi2BL14p crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/contrib/auto-render.min.js integrity=sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR crossorigin=anonymous></script>
<script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"\\[",right:"\\]",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1}],ignoredTags:["script","noscript","style","textarea","pre","code","option"],throwOnError:!1})})</script><meta property="og:title" content="CS 747 - Assignment 1"><meta property="og:description" content="Aditya Kadoo (200050055)"><meta property="og:type" content="article"><meta property="og:url" content="https://adityakadoo.github.io/Scrolls/assignment_1/"><meta property="article:section" content><meta property="article:published_time" content="2023-09-10T12:06:12+05:30"><meta property="article:modified_time" content="2023-09-10T12:06:12+05:30"><link rel=canonical href=https://adityakadoo.github.io/Scrolls/assignment_1/><meta itemprop=name content="CS 747 - Assignment 1"><meta itemprop=description content="Aditya Kadoo (200050055)"><meta itemprop=datePublished content="2023-09-10T12:06:12+05:30"><meta itemprop=dateModified content="2023-09-10T12:06:12+05:30"><meta itemprop=wordCount content="690"><meta itemprop=keywords content><link media=screen rel=stylesheet href=https://adityakadoo.github.io/Scrolls/css/common.css><link media=screen rel=stylesheet href=https://adityakadoo.github.io/Scrolls/css/content.css><title>CS 747 - Assignment 1 - Scrolls</title><meta name=twitter:card content="summary"><meta name=twitter:title content="CS 747 - Assignment 1"><meta name=twitter:description content="Aditya Kadoo (200050055)"><link rel=stylesheet href=https://adityakadoo.github.io/Scrolls/css/single.css></head><body><div id=wrapper><header id=header><h1>CS 747 - Assignment 1</h1><nav><span class=nav-bar-item><a class=link href></a></span></nav><p>Aditya Kadoo (200050055)</p></header><main id=main class=post><article class=content><div class=tableofcontent><h2>Contents</h2><nav id=TableOfContents><ol><li><a href=#task-1>Task 1</a><ol><li><a href=#ucb>UCB</a></li><li><a href=#kl-ucb>KL-UCB</a></li><li><a href=#thompson-sampling>Thompson Sampling</a></li></ol></li><li><a href=#task-2>Task 2</a><ol><li><a href=#part-a>Part A</a></li><li><a href=#part-b>Part B</a></li></ol></li><li><a href=#task-3>Task 3</a></li><li><a href=#task-4>Task 4</a></li></ol></nav></div><h2 id=task-1>Task 1</h2><h3 id=ucb>UCB</h3><table><thead><tr><th>Plot</th></tr></thead><tbody><tr><td><img src=images/task1-UCB-20230908-135429.png alt=UCB-regret></td></tr></tbody></table><p>The UCB algorithm works by pulling the arm $a$ at iteration $t$ which maximizes the following,
$$
\text{ucb}_a^t = \hat p_a^t + \sqrt{\frac{2\ln(t)}{u_a^t}}
$$</p><p>The required emperical means $(\hat p_a^t)$ and count of pulls $(u_a^t)$ are computed in the variable <code>values</code> and <code>counts</code> respectively in the same way as done for the epsilon greedy algorithm. For $t$ another variable <code>total_pulls</code> is stored and incremented on every call of <code>get_reward</code>. The arm to be pulled is chosen by finding $\argmax_a\text{ucb}_a^t$.</p><h3 id=kl-ucb>KL-UCB</h3><table><thead><tr><th>Plot</th></tr></thead><tbody><tr><td><img src=images/task1-KL_UCB-20230908-190726.png alt=KL-UCB-regret></td></tr></tbody></table><p>The KL-UCB algorithm works by pulling the arm $a$ at iteration $t$ which maximizes the following,
$$
\text{ucb-kl}_a^t = \max\{q\in[\hat p_a^t,1]:u_a^t\text{KL}(\hat p_a^t, q)\le \ln(t)+c\ln(\ln(t))\}
$$
where for Bernoulli distributions,
$$
\text{KL}(p,q) = (1-p)\log\left(\frac{1-p}{1-q}\right) + p\log\left(\frac{p}{q}\right)
$$</p><p>The variables <code>counts</code>, <code>values</code> and <code>total_pulls</code> from UCB are reused. While implementing $c$ is taken to be 0. To find the optimal $q$ for each arm, a vectorized version of the binary search algorithm is implemented with numpy. The $q$ found has a maximum precision of $10^{-2}$. The arm to be pulled is chosen by finding $\argmax_a\text{ucb-kl}_a^t$.</p><h3 id=thompson-sampling>Thompson Sampling</h3><table><thead><tr><th>Plot</th></tr></thead><tbody><tr><td><img src=images/task1-Thompson_Sampling-20230908-173914.png alt=Thompson-regret></td></tr></tbody></table><p>The Thompson Sampling algorithm works by selecting arm $a$ at iteration $t$ with the maximum draw from beta likelihood distributions given by,
$$
x_a^t\sim\text{Beta}(s_a^t+1, f_a^t+1)
$$</p><p>The success and failure counts, $s_a^t$ and $f_a^t$, are stored in the vectors <code>succeses</code> and <code>failures</code> respectively and updated during the <code>get_reward</code> call. The arm to be pulled is chosen by drawing a vector from the numpy&rsquo;s beta distributions with proper parameters and picking the arm with highest drawn value.</p><h2 id=task-2>Task 2</h2><h3 id=part-a>Part A</h3><table><thead><tr><th>Plot</th></tr></thead><tbody><tr><td><img src=images/task2-A-20230909-110607.png alt=t2pA></td></tr></tbody></table><p>The above plot clearly shows that in the described 2-arm bandit instance, as $p_2$ varies from 0 to 0.9 with $p_1$ fixed at 0.9, the regret rapidly rises and finally becomes 0.</p><p>This can be explained by the observation that as $p_2$ increases it becomes more and more difficult to tell the two arms apart. For UCB algorithm as shown in class,
$$
\text{Regret}_T = \mathcal{O}\left(\frac{1}{p_1-p_2}\log(T)\right)
$$
From this bound we can infer that the plot follows a rectangular hyperbola with asymptotes at $p_2=p_1$ and $\text{Regret}=0$. When $p_2=p_1$, it makes sense that the regret is 0 as both arms are the correct choice.</p><h3 id=part-b>Part B</h3><table><thead><tr><th>UCB</th><th>KL-UCB</th></tr></thead><tbody><tr><td><img src=images/task2-B-UCB-20230910-124649.png alt=t2pB-UCB></td><td><img src=images/task2-B-KL-UCB-20230910-124650.png alt=t2pB-Kl-UCB></td></tr></tbody></table><p>In the plot shown below we can see that the regret from KL-UCB algorithm is roughly a fraction of the regret from UCB algorithm.</p><p>This agrees with what we had seen in class that both algorithms are able to get asymptotically logarithmic regrets but differ by a constant factor, KL-UCB being the one to achieve optimal regret of the Lai-Robbins lower bound.</p><table><thead><tr><th>Comparison</th></tr></thead><tbody><tr><td><img src=images/task2-B-20230909-114549.png alt=t2pB></td></tr></tbody></table><h2 id=task-3>Task 3</h2><p>In the faulty multi-arm bandit instance, for an arm with Bernoulli parameter as $p$ (without fault) and fault probability $q$, the probability of getting 1 reward is,
$$
P(1;p,q) = (1-q)\cdot P_{\text{bernoulli}}(1;p) + q\cdot P_{\text{uniform}}(1;\{1,0\})\\
\ \\
= (1-q)\cdot p + q\cdot \frac{1}{2}
$$</p><p>Similarly probability for getting 0,
$$
P(0;p,q) = (1-q)\cdot P_{\text{bernoulli}}(0;p) + q\cdot P_{\text{uniform}}(0;\{1,0\})\\
\ \\
= (1-q)\cdot (1-p) + q\cdot \frac{1}{2}\\
\ \\
\therefore P(0;p,q)+P(1;p,q) = 1\\
\ \\
$$
Hence a faulty arm still follows a Bernoulli distribution with parameter $(1-q)\cdot p+q/2$. This means any algorithm that manages to get optimal regret for the original multi-arm bandit instance with Bernoulli arms should also work for this case. That&rsquo;s why I have implement the Thompson Sampling with Beta distribution.</p><h2 id=task-4>Task 4</h2><p>In the multi-multi-arm bandit instance, for a particular $n^{\text{th}}$ arm with Bernoulli parameter as $p_1^{(n)}$ in one set and $p_2^{(n)}$ in the other, the probability of getting 1 reward is,
$$
P(1;p_1^{(n)},p_2^{(n)}) = \frac{1}{2}\cdot P_{\text{bernoulli}}(1;p_1^{(n)}) + \frac{1}{2}\cdot P_{\text{bernoulli}}(1;p_2^{(n)})\\
\ \\
= \frac{p_1^{(n)} + p_2^{(n)}}{2}
$$</p><p>Similarly probability for getting 0,
$$
P(0;p_1^{(n)},p_2^{(n)}) = \frac{1}{2}\cdot P_{\text{bernoulli}}(0;p_1^{(n)}) + \frac{1}{2}\cdot P_{\text{bernoulli}}(0;p_2^{(n)})\\
\ \\
= \frac{(1-p_1^{(n)}) + (1-p_2^{(n)})}{2}
= 1 - \frac{p_1^{(n)} + p_2^{(n)}}{2}\\
\ \\
\therefore P(0;p_1^{(n)},p_2^{(n)})+P(1;p_1^{(n)},p_2^{(n)}) = 1\\
\ \\
$$
Hence any such $n^{\text{th}}$ arm still follows a Bernoulli distribution with parameter $(p_1^{(n)} + p_2^{(n)})/2$. This means any algorithm that manages to get optimal regret for the original multi-arm bandit instance with Bernoulli arms should also work for this case. That&rsquo;s why again, I have implement the Thompson Sampling with Beta distribution.</p></article></main><footer id=footer></footer></div></body></html>