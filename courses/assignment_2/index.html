<!doctype html><html lang=en-us><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="ie=edge"><meta name=author content="Aditya Kadoo"><meta name=description content="Aditya Kadoo (200050055)"><link rel=icon href=https://adityakadoo.github.io/Scrolls/favicon.ico><meta name=keywords content=" hugo  latex  theme "><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css integrity=sha384-KiWOvVjnN8qwAZbuQyWDIbfCLFhLXNETzBQjA/92pIowpC0d2O3nppDGQVgwd2nB crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js integrity=sha384-0fdwu/T/EQMsQlrHCCHoH10pkPLlKA1jL5dFyUOvB3lfeT2540/2g6YgSi2BL14p crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/contrib/auto-render.min.js integrity=sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR crossorigin=anonymous></script>
<script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"\\[",right:"\\]",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1}],ignoredTags:["script","noscript","style","textarea","pre","code","option"],throwOnError:!1})})</script><meta property="og:title" content="CS 747 - Assignment 2"><meta property="og:description" content="Aditya Kadoo (200050055)"><meta property="og:type" content="article"><meta property="og:url" content="https://adityakadoo.github.io/Scrolls/courses/assignment_2/"><meta property="article:section" content="courses"><meta property="article:published_time" content="2023-10-15T14:18:17+05:30"><meta property="article:modified_time" content="2023-10-15T14:18:17+05:30"><link rel=canonical href=https://adityakadoo.github.io/Scrolls/courses/assignment_2/><meta itemprop=name content="CS 747 - Assignment 2"><meta itemprop=description content="Aditya Kadoo (200050055)"><meta itemprop=datePublished content="2023-10-15T14:18:17+05:30"><meta itemprop=dateModified content="2023-10-15T14:18:17+05:30"><meta itemprop=wordCount content="645"><meta itemprop=keywords content><link media=screen rel=stylesheet href=https://adityakadoo.github.io/Scrolls/css/common.css><link media=screen rel=stylesheet href=https://adityakadoo.github.io/Scrolls/css/content.css><title>CS 747 - Assignment 2 - Scrolls</title><meta name=twitter:card content="summary"><meta name=twitter:title content="CS 747 - Assignment 2"><meta name=twitter:description content="Aditya Kadoo (200050055)"><link rel=stylesheet href=https://adityakadoo.github.io/Scrolls/css/single.css></head><body><div id=wrapper><header id=header><h1>CS 747 - Assignment 2</h1><nav><span class=nav-bar-item><a class=link href></a></span></nav><p>Aditya Kadoo (200050055)</p></header><main id=main class=post><article class=content><div class=tableofcontent><h2>Contents</h2><nav id=TableOfContents><ol><li><a href=#task-1>Task 1</a></li><li><a href=#task-2>Task 2</a><ol><li><a href=#plots-and-observations>Plots and Observations</a></li></ol></li></ol></nav></div><h2 id=task-1>Task 1</h2><p>Within the <code>planner.py</code> file, all the three algorithms of value iteration, linear programming and Howard&rsquo;s policy improvement are implemented under the function <code>optimal_policy()</code>. The code is not parallelized much due to memory constraints and simply loops through all the transitions in the worst case. Assuming the number of states in below 10,000, the number of actions is below 100 and the number of transitions is below 3,50,000, this code should execute within a few seconds.</p><p>The three algorithms are implemented as described in the course slides. Linear programming algorithm makes use of the <code>pulp</code> library. For this algorithm the library function for solving the linear program was found to be the bottle-neck and hence it was the slowest one out of the three algorithms. It was also observed that both of the iteration based algorithms, value iteration and Howard&rsquo;s policy improvement, converge to the optimal policy very fast i.e. under 5-10 iterations. The default algorithm is value iteration as it was observed to be the fastest one out of the three.</p><p>There is another function called <code>value_function()</code> which returns the values for all the states in the given mdp instance for a given policy. This function basically solves the Bellman equations using the <code>pulp</code> library. It is called whenever <code>--policy</code> argument is passed to this program.</p><h2 id=task-2>Task 2</h2><p>Let the mdp corresponding to the football problem be $\langle S, A, T, R, \gamma\rangle$. We define each component as,</p><ul><li><strong>Set of states</strong>: Here the first 2 states, $\text{GOAL}$ and $\text{NO-GOAL}$ are terminal states. Rest are for a particular game state.
$$
S = \{\text{GOAL}, \text{NO-GOAL}\}\cup\{[b_1,b_2,r,s]\ |\ b_1,b_2,r\in\{01, 02, \dots, 16\}, s\in\{1,2\}\}\\
\ \\
\therefore |S| = 2 + 16^3\cdot 2 = 8194
$$</li><li><strong>Set of actions</strong>: As defined in the problem statement, at each game state the agent can control the two players $b_1$ and $b_2$. These 2 players can perform 10 actions. $0,1,2,3$ correspond to $b_1$&rsquo;s movement, $4,5,6,7$ correspond to $b_2$&rsquo;s movement, $8$ for passing and $9$ for shooting.
$$
\therefore A = \{0, 1, \dots, 9\}
$$</li><li><strong>Transition Function</strong>: $T:S\times A\times S\to[0,1]$ is the transition function. This is defined as given in the problem statement with parameters $p$ and $q$. Any action that leads to the episode&rsquo;s unsuccessful end has a transition into the $\text{NO-GOAL}$ state. Similarly a successful shot has a transition into the $\text{GOAL}$ state.</li><li><strong>Reward Function</strong>: $R:S\times A\times S\to\R$ is the reward function which is defined to be 1 for a transition $s_1\xrightarrow{a}s_2$ when $s_2=\text{GOAL}$ and otherwise 0.</li><li><strong>Discount Factor</strong>: Since this is an episodic task it&rsquo;s $\gamma=1$. This is a fair assumption since we only want to find the expected goals from any game state i.e. the probability of reaching to the $\text{GOAL}$ state. This state is terminal by definition and the reward function is also defined with this in mind.</li></ul><h3 id=plots-and-observations>Plots and Observations</h3><p>The expected goals from the starting state [05, 09, 08, 1] for the 3 test-cases is,</p><ol><li><strong>Greedy Defense</strong> ($p=0.25$, $q=0.75$): 0.175000 with optimal action being 1 i.e. moving $b_1$ towards right.</li><li><strong>Parking the Bus</strong> ($p=0.15$, $q=0.95$): 0.203203 with opimal action being 5 i.e. moving $b_2$ towards right.</li><li><strong>Random Policy</strong> ($p=0.35$, $q=0.65$): 0.080244 with opitmal action being 5 i.e. moving $b_2$ towards right.</li></ol><blockquote><p>The first plot is between different values of $p$ and the $xG$ for [05, 09, 08, 1] game state with $q=0.7$. For lower values of $p$ the players $b_1$ and $b_2$ are better at dribbling, avoiding tackles and overall movement. Hence I call $p$ the <strong>inverse dribbling factor</strong>. This effect can also be seen in the plot below.</p><p><img src=images/plot-p-vs-xG.png alt=p-vs-xG></p></blockquote><blockquote><p>The second plot is between different values of $q$ and the $xG$ for [05, 09, 08, 1] game state with $p=0.3$. For higher values of $q$ the players $b_1$ and $b_2$ are better at passing and shooting. Hence I call $q$ the <strong>passing factor</strong>. This effect can also be seen in the plot below.</p><p><img src=images/plot-q-vs-xG.png alt=q-vs-xG></p></blockquote></article></main><footer id=footer></footer></div><link media=screen rel=stylesheet href=https://adityakadoo.github.io/Scrolls/css/syntax.css></body></html>