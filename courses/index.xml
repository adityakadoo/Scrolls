<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Courses on Scrolls</title><link>https://adityakadoo.github.io/Scrolls/courses/</link><description>Recent content in Courses on Scrolls</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Mon, 05 Sep 2022 06:52:35 +0530</lastBuildDate><atom:link href="https://adityakadoo.github.io/Scrolls/courses/index.xml" rel="self" type="application/rss+xml"/><item><title>Artificial Intelligence and Machine Learning</title><link>https://adityakadoo.github.io/Scrolls/courses/ai_ml/</link><pubDate>Mon, 05 Sep 2022 06:52:35 +0530</pubDate><guid>https://adityakadoo.github.io/Scrolls/courses/ai_ml/</guid><description>Probability Basic Terms Sample Space Sample Space $(S)$ The set of all possible outcomes of an experiment. $$ P(S)=1,P(\empty)=0 $$ Probability Distribution Probability Distribution $(p)$ A function that gives the probabilities of occurence of different possible outcomes of an experiment. $$ p:S\rightarrow[0,1]\\ \sum_{x\in S}p(x)=1 $$ Event Event $(E)$ A set of outcomes of an experiement i.e. a subset of the sample space. $$ E\sube S $$ Probability of an Event Probability of an event $P(E)$ The likelihood of an event happening.</description></item><item><title>Natural Language Processing</title><link>https://adityakadoo.github.io/Scrolls/courses/natural_language_processing/</link><pubDate>Wed, 24 Aug 2022 09:49:01 +0530</pubDate><guid>https://adityakadoo.github.io/Scrolls/courses/natural_language_processing/</guid><description>Part of Speech Tagging HMM-based Tagging Parameters Input: A sequence of words and labels Output: A sequence of labels for every word Penn tag-set is generally used for POS tagging in english language.
Hidden Markov Model There are 2 kinds of probabilities:
Bigram Probabilities $(P(t_1|t_0))$ : Probability of current word being tag $t_1$ when previous word was tagged $t_0$. Lexical Probabilities $(P(w|t))$: Probability of word $w$ given it is tagged $t$.</description></item><item><title>Automata Theory</title><link>https://adityakadoo.github.io/Scrolls/courses/automata_theory/</link><pubDate>Fri, 19 Aug 2022 17:00:34 +0530</pubDate><guid>https://adityakadoo.github.io/Scrolls/courses/automata_theory/</guid><description>Resources Hopcrot-Motwani-Ullman Central Concepts Alphabets Alphabet $ (\Sigma) $ An alphabet is a finite nonempty set of symbols. $ \Sigma= \{1, 0\} $ $ \Sigma = \{a,b,&amp;hellip;,z\}$ Strings String $(w)$ A string is a finite sequence of symbols chosen from $ \Sigma $. Empty String Empty String $(\epsilon)$ The empty string is a string with zero symbols. Length of a String $|w|=\#$ Symbols in $w$
$|\epsilon|=0$
Powers of Alphabets $\Sigma^k=\{w:|w|=k\}$ $\Sigma^+=\Sigma^1\cup\Sigma^2\cup&amp;hellip;$ $\Sigma^*=\Sigma^+\cup\{\epsilon\}$ $\Sigma^0= \{\epsilon\}$</description></item><item><title>Operating Systems</title><link>https://adityakadoo.github.io/Scrolls/courses/operating_systems/</link><pubDate>Fri, 19 Aug 2022 10:31:05 +0530</pubDate><guid>https://adityakadoo.github.io/Scrolls/courses/operating_systems/</guid><description>Resources Operating Systems - Three Easy Pieces Introduction to OS Main objectives of OS include:-
CPU Virtualization : Making using the processor easy. Memory Virtualization : Making storage in memory easy. Concurrency : Ensuring correctness when multiple programs run together. Persistence : Ensuring permanent memory does not get erased and stays organised. Design Goals : Abstractions, performance, isolation, reliability, energy-efficiency Virtualization Abstraction : Process The abstraction provided by the OS of a running program is called a process.</description></item></channel></rss>