<!doctype html><html lang=en-us><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="ie=edge"><meta name=author content="Aditya Kadoo"><meta name=description content="Part of Speech Tagging HMM-based Tagging Parameters Input: A sequence of words and labels Output: A sequence of labels for every word Penn tag-set is generally used for POS tagging in english language.
Hidden Markov Model There are 2 kinds of probabilities:
Bigram Probabilities $(P(t_1|t_0))$ : Probability of current word being tag $t_1$ when previous word was tagged $t_0$. Lexical Probabilities $(P(w|t))$: Probability of word $w$ given it is tagged $t$."><link rel=icon href=https://adityakadoo.github.io/Scrolls/favicon.ico><meta name=keywords content=" hugo  latex  theme "><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css integrity=sha384-KiWOvVjnN8qwAZbuQyWDIbfCLFhLXNETzBQjA/92pIowpC0d2O3nppDGQVgwd2nB crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js integrity=sha384-0fdwu/T/EQMsQlrHCCHoH10pkPLlKA1jL5dFyUOvB3lfeT2540/2g6YgSi2BL14p crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/contrib/auto-render.min.js integrity=sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR crossorigin=anonymous></script>
<script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"\\[",right:"\\]",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1}],ignoredTags:["script","noscript","style","textarea","pre","code","option"],throwOnError:!1})})</script><meta property="og:title" content="Natural Language Processing"><meta property="og:description" content="Part of Speech Tagging HMM-based Tagging Parameters Input: A sequence of words and labels Output: A sequence of labels for every word Penn tag-set is generally used for POS tagging in english language.
Hidden Markov Model There are 2 kinds of probabilities:
Bigram Probabilities $(P(t_1|t_0))$ : Probability of current word being tag $t_1$ when previous word was tagged $t_0$. Lexical Probabilities $(P(w|t))$: Probability of word $w$ given it is tagged $t$."><meta property="og:type" content="article"><meta property="og:url" content="https://adityakadoo.github.io/Scrolls/courses/natural_language_processing/"><meta property="article:section" content="courses"><meta property="article:published_time" content="2022-08-24T09:49:01+05:30"><meta property="article:modified_time" content="2022-08-24T09:49:01+05:30"><link rel=canonical href=https://adityakadoo.github.io/Scrolls/courses/natural_language_processing/><meta itemprop=name content="Natural Language Processing"><meta itemprop=description content="Part of Speech Tagging HMM-based Tagging Parameters Input: A sequence of words and labels Output: A sequence of labels for every word Penn tag-set is generally used for POS tagging in english language.
Hidden Markov Model There are 2 kinds of probabilities:
Bigram Probabilities $(P(t_1|t_0))$ : Probability of current word being tag $t_1$ when previous word was tagged $t_0$. Lexical Probabilities $(P(w|t))$: Probability of word $w$ given it is tagged $t$."><meta itemprop=datePublished content="2022-08-24T09:49:01+05:30"><meta itemprop=dateModified content="2022-08-24T09:49:01+05:30"><meta itemprop=wordCount content="702"><meta itemprop=keywords content="Computer-Science,Machine-Learning,"><link media=screen rel=stylesheet href=https://adityakadoo.github.io/Scrolls/css/common.css><link media=screen rel=stylesheet href=https://adityakadoo.github.io/Scrolls/css/content.css><title>Natural Language Processing - Scrolls</title><meta name=twitter:card content="summary"><meta name=twitter:title content="Natural Language Processing"><meta name=twitter:description content="Part of Speech Tagging HMM-based Tagging Parameters Input: A sequence of words and labels Output: A sequence of labels for every word Penn tag-set is generally used for POS tagging in english language.
Hidden Markov Model There are 2 kinds of probabilities:
Bigram Probabilities $(P(t_1|t_0))$ : Probability of current word being tag $t_1$ when previous word was tagged $t_0$. Lexical Probabilities $(P(w|t))$: Probability of word $w$ given it is tagged $t$."><link rel=stylesheet href=https://adityakadoo.github.io/Scrolls/css/single.css></head><body><div id=wrapper><header id=header><h1><a href=https://adityakadoo.github.io/Scrolls/>Natural Language Processing</a></h1><nav><span class=nav-bar-item><a class=link href></a></span></nav><p></p></header><main id=main class=post><article class=content><div class=tableofcontent></div><h2 id=part-of-speech-tagging>Part of Speech Tagging</h2><h3 id=hmm-based-tagging>HMM-based Tagging</h3><h4 id=parameters>Parameters</h4><ul><li><em>Input</em>: A sequence of words and labels</li><li><em>Output</em>: A sequence of labels for every word</li></ul><blockquote><p>Penn tag-set is generally used for POS tagging in english language.</p></blockquote><h4 id=hidden-markov-model>Hidden Markov Model</h4><p>There are 2 kinds of probabilities:</p><ol><li>Bigram Probabilities $(P(t_1|t_0))$ : Probability of current word being tag $t_1$ when previous word was tagged $t_0$.</li><li>Lexical Probabilities $(P(w|t))$: Probability of word $w$ given it is tagged $t$.</li></ol><blockquote><p>By Markov assumption, current word&rsquo;s tag only depends on previous word&rsquo;s tag.</p></blockquote><h4 id=viterbi-algorithm>Viterbi Algorithm</h4><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-cpp data-lang=cpp><span style=display:flex><span><span style=color:#080>#define BProb(t1,t0) () </span><span style=color:#080;font-style:italic>// Bigram
</span></span></span><span style=display:flex><span><span style=color:#080;font-style:italic></span><span style=color:#080>#define LProb(w,t) () </span><span style=color:#080;font-style:italic>// Lexical
</span></span></span><span style=display:flex><span><span style=color:#080;font-style:italic></span>
</span></span><span style=display:flex><span>vector<span style=color:#666>&lt;</span>string<span style=color:#666>&gt;</span> viterbi(vector<span style=color:#666>&lt;</span>string<span style=color:#666>&gt;</span> sentence, vector<span style=color:#666>&lt;</span>string<span style=color:#666>&gt;</span> labels){
</span></span><span style=display:flex><span>    <span style=color:#0b0;font-weight:700>int</span> n <span style=color:#666>=</span> sentence.size();
</span></span><span style=display:flex><span>    <span style=color:#0b0;font-weight:700>int</span> l <span style=color:#666>=</span> labels.size();
</span></span><span style=display:flex><span>    <span style=color:#080;font-style:italic>// labels[0] = &#34;^&#34;
</span></span></span><span style=display:flex><span><span style=color:#080;font-style:italic></span>    vector<span style=color:#666>&lt;</span>vector<span style=color:#666>&lt;</span><span style=color:#0b0;font-weight:700>float</span><span style=color:#666>&gt;&gt;</span> dp(n<span style=color:#666>+</span><span style=color:#666>1</span>, vector<span style=color:#666>&lt;</span><span style=color:#0b0;font-weight:700>float</span><span style=color:#666>&gt;</span>(l, <span style=color:#666>0</span>));
</span></span><span style=display:flex><span>    vector<span style=color:#666>&lt;</span>vector<span style=color:#666>&lt;</span>string<span style=color:#666>&gt;&gt;</span> plabel(n<span style=color:#666>+</span><span style=color:#666>1</span>, vector<span style=color:#666>&lt;</span><span style=color:#0b0;font-weight:700>int</span><span style=color:#666>&gt;</span>(l,<span style=color:#666>0</span>));
</span></span><span style=display:flex><span>    dp[<span style=color:#666>0</span>][<span style=color:#666>0</span>] <span style=color:#666>=</span> <span style=color:#666>1</span>;
</span></span><span style=display:flex><span>    <span style=color:#a2f;font-weight:700>for</span>(<span style=color:#0b0;font-weight:700>int</span> i<span style=color:#666>=</span><span style=color:#666>0</span>;i<span style=color:#666>&lt;</span>n;i<span style=color:#666>++</span>)
</span></span><span style=display:flex><span>        <span style=color:#a2f;font-weight:700>for</span>(<span style=color:#0b0;font-weight:700>int</span> j<span style=color:#666>=</span><span style=color:#666>0</span>;j<span style=color:#666>&lt;</span>l;j<span style=color:#666>++</span>);
</span></span><span style=display:flex><span>            <span style=color:#a2f;font-weight:700>for</span>(<span style=color:#0b0;font-weight:700>int</span> k<span style=color:#666>=</span><span style=color:#666>0</span>;k<span style=color:#666>&lt;</span>l;k<span style=color:#666>++</span>){
</span></span><span style=display:flex><span>                <span style=color:#0b0;font-weight:700>float</span> p <span style=color:#666>=</span> dp[i][k]
</span></span><span style=display:flex><span>                    <span style=color:#666>*</span> BProb(labels[j],labels[k])
</span></span><span style=display:flex><span>                    <span style=color:#666>*</span> LProb(sentence[i],labels[j]);
</span></span><span style=display:flex><span>                <span style=color:#a2f;font-weight:700>if</span>(dp[i<span style=color:#666>+</span><span style=color:#666>1</span>][j]<span style=color:#666>&lt;</span>p){
</span></span><span style=display:flex><span>                    dp[i<span style=color:#666>+</span><span style=color:#666>1</span>][j] <span style=color:#666>=</span> p;
</span></span><span style=display:flex><span>                    plabel[i<span style=color:#666>+</span><span style=color:#666>1</span>][j] <span style=color:#666>=</span> k;
</span></span><span style=display:flex><span>                }
</span></span><span style=display:flex><span>            }
</span></span><span style=display:flex><span>    vector<span style=color:#666>&lt;</span>string<span style=color:#666>&gt;</span> res(sentence.begin(),sentence.end());
</span></span><span style=display:flex><span>    <span style=color:#0b0;font-weight:700>int</span> ptr<span style=color:#666>=</span><span style=color:#666>0</span>;
</span></span><span style=display:flex><span>    <span style=color:#a2f;font-weight:700>for</span>(<span style=color:#0b0;font-weight:700>int</span> i<span style=color:#666>=</span><span style=color:#666>0</span>;i<span style=color:#666>&lt;</span>l;i<span style=color:#666>++</span>)
</span></span><span style=display:flex><span>        ptr <span style=color:#666>=</span> dp[n][ptr]<span style=color:#666>&lt;</span>dp[n][i] <span style=color:#666>?</span> <span style=color:#a0a000>i</span> : ptr;
</span></span><span style=display:flex><span>    res[n<span style=color:#666>-</span><span style=color:#666>1</span>] <span style=color:#666>=</span> res[n<span style=color:#666>-</span><span style=color:#666>1</span>]<span style=color:#666>+</span><span style=color:#b44>&#34;_.&#34;</span>;
</span></span><span style=display:flex><span>    <span style=color:#a2f;font-weight:700>for</span>(<span style=color:#0b0;font-weight:700>int</span> i<span style=color:#666>=</span>n<span style=color:#666>-</span><span style=color:#666>1</span>;i<span style=color:#666>&gt;</span><span style=color:#666>0</span>;i<span style=color:#666>--</span>){
</span></span><span style=display:flex><span>        ptr <span style=color:#666>=</span> slabel[i][ptr];
</span></span><span style=display:flex><span>        res[i<span style=color:#666>-</span><span style=color:#666>1</span>] <span style=color:#666>+=</span> <span style=color:#b44>&#34;_&#34;</span><span style=color:#666>+</span>labels[ptr];
</span></span><span style=display:flex><span>    }
</span></span><span style=display:flex><span>    <span style=color:#a2f;font-weight:700>return</span> res;
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><h3 id=discriminative-learning>Discriminative Learning</h3><p>HMM based POS tagging cannot handle &ldquo;free word order&rdquo; and &ldquo;agglutination&rdquo; well.</p><h4 id=feature-engineering>Feature Engineering</h4><ol><li>Word-based features<ul><li>$f_{21}$: Dictionary index of the current word</li><li>$f_{22}$: Dictionary index of the previous word</li><li>$f_{23}$: Dictionary index of the next word</li></ul></li><li>Part of Speech tag-based features<ul><li>$f_{24}$: Index of POS of previous word</li></ul></li><li>Morphology-based features<ul><li>$f_{25}$: does the current word have a noun suffix like &rsquo;s&rsquo;, &rsquo;es&rsquo;, &lsquo;ies&rsquo;, etc.</li><li>$f_{26}$: does the current word have a verbal suffix like &rsquo;d&rsquo;, &rsquo;ed&rsquo;, &rsquo;t&rsquo;, etc.</li><li>$f_{27}$ and $f_{28}$: above two for previous word.</li><li>$f_{29}$ and $f_{2,10}$: above two for next word.</li></ul></li></ol><h4 id=morphology>Morphology</h4><dl><dt>Morphemes</dt><dd>Smallest meaning-bearing units forming a word.
e.g.: In &ldquo;quickly&rdquo;, &ldquo;quick&rdquo; and &ldquo;ly&rdquo;.</dd></dl><ul><li><strong>Analytic Languages</strong>: Morphemes largely separate from one another.</li><li><strong>Synthetic Languages</strong>: Joins the morphemes.</li></ul><dl><dt>Syncretism</dt><dd>Overloading of roles per morpheme is called <em><strong>syncretism</strong></em>.
e.g.: &ldquo;will go&rdquo;: since number and person are indeterminate here</dd></dl><h4 id=maximum-entropy-markov-model>Maximum Entropy Markov Model</h4><p>$$
P(t_i=t|F_i)=\dfrac{e^{\sum_{j=1.k}\lambda_jf_{ij}}}{{\sum_{t&rsquo;\in S}}e^{\sum_{j=1.k}\lambda_jf_{ij}(t&rsquo;)}}
$$</p><h4 id=beam-search-algorithm>Beam Search Algorithm</h4><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-cpp data-lang=cpp><span style=display:flex><span><span style=color:#080>#define Prob(t,w) ()
</span></span></span><span style=display:flex><span><span style=color:#080></span>
</span></span><span style=display:flex><span>vector<span style=color:#666>&lt;</span>string<span style=color:#666>&gt;</span> beam_search(vector<span style=color:#666>&lt;</span>string<span style=color:#666>&gt;</span> sentence, vector<span style=color:#666>&lt;</span>string<span style=color:#666>&gt;</span> labels){
</span></span><span style=display:flex><span>    <span style=color:#0b0;font-weight:700>int</span> n <span style=color:#666>=</span> sentence.size();
</span></span><span style=display:flex><span>    <span style=color:#0b0;font-weight:700>int</span> l <span style=color:#666>=</span> labels.size();
</span></span><span style=display:flex><span>    <span style=color:#0b0;font-weight:700>int</span> k <span style=color:#666>=</span> <span style=color:#666>3</span>; <span style=color:#080;font-style:italic>// Beam size
</span></span></span><span style=display:flex><span><span style=color:#080;font-style:italic></span>    <span style=color:#080;font-style:italic>// labels[0] = &#34;^&#34;
</span></span></span><span style=display:flex><span><span style=color:#080;font-style:italic></span>    vector<span style=color:#666>&lt;</span>vector<span style=color:#666>&lt;</span>pair<span style=color:#666>&lt;</span><span style=color:#0b0;font-weight:700>float</span>,vector<span style=color:#666>&lt;</span><span style=color:#0b0;font-weight:700>int</span><span style=color:#666>&gt;&gt;&gt;&gt;</span> best(n<span style=color:#666>+</span><span style=color:#666>1</span>, vector<span style=color:#666>&lt;</span>pair<span style=color:#666>&lt;</span><span style=color:#0b0;font-weight:700>float</span>,
</span></span><span style=display:flex><span>        vector<span style=color:#666>&lt;</span><span style=color:#0b0;font-weight:700>int</span><span style=color:#666>&gt;&gt;&gt;</span>(k, make_pair(<span style=color:#666>0</span>,vector<span style=color:#666>&lt;</span><span style=color:#0b0;font-weight:700>int</span><span style=color:#666>&gt;</span>(n))));
</span></span><span style=display:flex><span>    best[<span style=color:#666>0</span>][<span style=color:#666>0</span>].first <span style=color:#666>=</span> <span style=color:#666>1</span>;
</span></span><span style=display:flex><span>    best[<span style=color:#666>0</span>][<span style=color:#666>0</span>].second[<span style=color:#666>0</span>] <span style=color:#666>=</span> <span style=color:#666>0</span>;
</span></span><span style=display:flex><span>    <span style=color:#a2f;font-weight:700>for</span>(<span style=color:#0b0;font-weight:700>int</span> i<span style=color:#666>=</span><span style=color:#666>1</span>;i<span style=color:#666>&lt;</span>k;i<span style=color:#666>++</span>){
</span></span><span style=display:flex><span>        best[<span style=color:#666>0</span>][i].first <span style=color:#666>=</span> <span style=color:#666>0</span>;
</span></span><span style=display:flex><span>        best[<span style=color:#666>0</span>][i].second[<span style=color:#666>0</span>] <span style=color:#666>=</span> min(i,l<span style=color:#666>-</span><span style=color:#666>1</span>);
</span></span><span style=display:flex><span>    }
</span></span><span style=display:flex><span>    <span style=color:#a2f;font-weight:700>for</span>(<span style=color:#0b0;font-weight:700>int</span> i<span style=color:#666>=</span><span style=color:#666>0</span>;i<span style=color:#666>&lt;</span>n;i<span style=color:#666>++</span>)
</span></span><span style=display:flex><span>        <span style=color:#a2f;font-weight:700>for</span>(<span style=color:#0b0;font-weight:700>int</span> j<span style=color:#666>=</span><span style=color:#666>0</span>;j<span style=color:#666>&lt;</span>k;j<span style=color:#666>++</span>)
</span></span><span style=display:flex><span>            <span style=color:#a2f;font-weight:700>for</span>(<span style=color:#0b0;font-weight:700>int</span> u<span style=color:#666>=</span><span style=color:#666>0</span>;u<span style=color:#666>&lt;</span>l;u<span style=color:#666>++</span>){
</span></span><span style=display:flex><span>                pair<span style=color:#666>&lt;</span><span style=color:#0b0;font-weight:700>float</span>,vector<span style=color:#666>&lt;</span><span style=color:#0b0;font-weight:700>int</span><span style=color:#666>&gt;&gt;</span> p <span style=color:#666>=</span> make_pair(best[i][j].first<span style=color:#666>*</span>
</span></span><span style=display:flex><span>                    Prob(best[i][j].second, i<span style=color:#666>+</span><span style=color:#666>1</span>, label[u]),best[i][j].second);
</span></span><span style=display:flex><span>                labls[i] <span style=color:#666>=</span> u;
</span></span><span style=display:flex><span>                <span style=color:#a2f;font-weight:700>for</span>(<span style=color:#0b0;font-weight:700>int</span> v<span style=color:#666>=</span><span style=color:#666>0</span>;v<span style=color:#666>&lt;</span>k;v<span style=color:#666>++</span>)
</span></span><span style=display:flex><span>                    <span style=color:#a2f;font-weight:700>if</span>(p<span style=color:#666>&gt;</span>best[i<span style=color:#666>+</span><span style=color:#666>1</span>][v])
</span></span><span style=display:flex><span>                        swap(p,best[i<span style=color:#666>+</span><span style=color:#666>1</span>][v]);
</span></span><span style=display:flex><span>            }
</span></span><span style=display:flex><span>    vector<span style=color:#666>&lt;</span>string<span style=color:#666>&gt;</span> res(n);
</span></span><span style=display:flex><span>    pair<span style=color:#666>&lt;</span><span style=color:#0b0;font-weight:700>float</span>,vector<span style=color:#666>&lt;</span><span style=color:#0b0;font-weight:700>int</span><span style=color:#666>&gt;&gt;</span> maxp <span style=color:#666>=</span> max_element(best[n].begin(),best[n].end());
</span></span><span style=display:flex><span>    <span style=color:#a2f;font-weight:700>for</span>(<span style=color:#0b0;font-weight:700>int</span> i<span style=color:#666>=</span><span style=color:#666>0</span>;i<span style=color:#666>&lt;</span>maxp.second.size();i<span style=color:#666>++</span>)
</span></span><span style=display:flex><span>        res[i] <span style=color:#666>=</span> labels[maxp.second[i]];
</span></span><span style=display:flex><span>    <span style=color:#a2f;font-weight:700>return</span> res;
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><h2 id=parsing>Parsing</h2><h3 id=context-free-grammar-parsing>Context Free Grammar Parsing</h3><p>We are given a CFG with terminals as POS tags from the language and vairables from segment labels. This grammar is converted to Chomsky form.</p><h4 id=cyk-algorithm>CYK Algorithm</h4><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-cpp data-lang=cpp><span style=display:flex><span><span style=color:#a2f;font-weight:700>class</span> <span style=color:#00f>node</span>{
</span></span><span style=display:flex><span>    string label;
</span></span><span style=display:flex><span>    node<span style=color:#666>*</span> left;
</span></span><span style=display:flex><span>    node<span style=color:#666>*</span> right;
</span></span><span style=display:flex><span>};
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>node<span style=color:#666>*</span> <span style=color:#00a000>CYK</span>(vector<span style=color:#666>&lt;</span>pair<span style=color:#666>&lt;</span>string,string<span style=color:#666>&gt;&gt;</span> sent, map<span style=color:#666>&lt;</span>pair<span style=color:#666>&lt;</span>string,string<span style=color:#666>&gt;</span>,string<span style=color:#666>&gt;</span> rules)
</span></span><span style=display:flex><span>{
</span></span><span style=display:flex><span>    <span style=color:#0b0;font-weight:700>int</span> n <span style=color:#666>=</span> pos_labels.size();
</span></span><span style=display:flex><span>    vector<span style=color:#666>&lt;</span>vector<span style=color:#666>&lt;</span>pair<span style=color:#666>&lt;</span><span style=color:#0b0;font-weight:700>int</span>,string<span style=color:#666>&gt;&gt;&gt;</span> dp(n, vector<span style=color:#666>&lt;</span>
</span></span><span style=display:flex><span>            pair<span style=color:#666>&lt;</span><span style=color:#0b0;font-weight:700>int</span>, string<span style=color:#666>&gt;&gt;</span>(n, make_pair(<span style=color:#666>0</span>,<span style=color:#b44>&#34;&#34;</span>)));
</span></span><span style=display:flex><span>    <span style=color:#a2f;font-weight:700>for</span>(<span style=color:#0b0;font-weight:700>int</span> i<span style=color:#666>=</span><span style=color:#666>0</span>;i<span style=color:#666>&lt;</span>n;i<span style=color:#666>++</span>)
</span></span><span style=display:flex><span>        <span style=color:#a2f;font-weight:700>for</span>(<span style=color:#0b0;font-weight:700>int</span> j<span style=color:#666>=</span>i;j<span style=color:#666>&gt;=</span><span style=color:#666>0</span>;j<span style=color:#666>--</span>)
</span></span><span style=display:flex><span>            <span style=color:#a2f;font-weight:700>if</span>(i<span style=color:#666>==</span>j)
</span></span><span style=display:flex><span>                dp[j][i] <span style=color:#666>=</span> make_pair(i, sent[i][<span style=color:#666>0</span>]);
</span></span><span style=display:flex><span>            <span style=color:#a2f;font-weight:700>else</span>{
</span></span><span style=display:flex><span>                dp[j][i] <span style=color:#666>=</span> make_pair(<span style=color:#666>-</span><span style=color:#666>1</span>, <span style=color:#b44>&#34;---&#34;</span>);
</span></span><span style=display:flex><span>                <span style=color:#a2f;font-weight:700>for</span>(<span style=color:#0b0;font-weight:700>int</span> k<span style=color:#666>=</span>j<span style=color:#666>+</span><span style=color:#666>1</span>;k<span style=color:#666>&lt;=</span>i;k<span style=color:#666>++</span>){
</span></span><span style=display:flex><span>                    pair<span style=color:#666>&lt;</span>string,string<span style=color:#666>&gt;</span> rule <span style=color:#666>=</span> make_pair(dp[j][k<span style=color:#666>-</span><span style=color:#666>1</span>].second,
</span></span><span style=display:flex><span>                                                         dp[k][i].second);
</span></span><span style=display:flex><span>                    <span style=color:#a2f;font-weight:700>if</span>(rules.find(rule)<span style=color:#666>!=</span>rules.end()){
</span></span><span style=display:flex><span>                        dp[j][i] <span style=color:#666>=</span> make_pair(k, rules[rule]);
</span></span><span style=display:flex><span>                        <span style=color:#a2f;font-weight:700>break</span>;
</span></span><span style=display:flex><span>                    }
</span></span><span style=display:flex><span>                }
</span></span><span style=display:flex><span>            }
</span></span><span style=display:flex><span>    node<span style=color:#666>*</span> r;
</span></span><span style=display:flex><span>    <span style=color:#080;font-style:italic>// make the tree
</span></span></span><span style=display:flex><span><span style=color:#080;font-style:italic></span>    <span style=color:#a2f;font-weight:700>return</span> r;
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><h4 id=shift-reduce-algorithm>Shift reduce algorithm</h4><p>Using a stack and working through a Push-down automaton based on the language.</p><h3 id=probabilistic-parsing>Probabilistic Parsing</h3><p>In the normal CFG related to the language, we add probability value to each rule. This can be found using the dataset.</p><p>Probability of a Parse Tree is defined as the product probabilities of all the rules used in the parse tree. This way we find the parse tree with highest probability.</p><p>We can also define the probability of a sentence as the sum of probabilities of its parse trees.
$$
P(S) = \sum_{t} P(t)\cdot P(S|t) = \sum_{t} P(t)
$$</p><h3 id=dependency-parsing>Dependency Parsing</h3><p>Instead of creating chunks of words we create dependency relations between words itself. This creates a tree of words as nodes.</p><h2 id=ffnnbp>FFNNBP</h2><p>Use softmax or sigmoid for sentiment analysis.</p><h2 id=wordnet>WordNet</h2><ul><li><em><strong>Syntagmatic</strong></em>: Based on relations such as Synonym, antonym, etc. <em>CAT</em> and <em>ANIMAL</em></li><li><em><strong>Paradigmatic</strong></em>: Based on Co-occurences. <em>CAT</em> and <em>MEW</em></li></ul><h3 id=wordnet-engineering>Wordnet Engineering</h3><dl><dt>Principles of Synset creation</dt><dd><ul><li>Minimality</li><li>Coverage</li><li>Replacibility</li></ul></dd></dl><p>These synsets are used to create Syntagmatic ConceptNets.</p><p>Calculate Lexical Semantic Association(LSA) i.e. matrix of co-occurence frequencies. Apply PCA to get Paradigmatic WordNets.</p><h3 id=using-wordnets>Using WordNets</h3><p>$$P(Context\ word | input\ word)=P(w_1|w_2)=\frac{e^{(u_{w_1}^Tu_{w_2})}}{\Sigma_k e^{(u_{w_1}^Tu_{w_k})}}$$</p><blockquote><p>Here $u_w$ is the word vector for $w$.</p></blockquote></article></main><footer id=footer><div><span>© 2022</span> - <span>2023</span></div><div><span>Powered by</span>
<a class=link href=https://gohugo.io/>Hugo</a>
<span>🍦 Theme</span>
<a class=link href=https://github.com/queensferryme/hugo-theme-texify>TeXify</a></div><div class=footnote><span>Follow me on <a class=link href=https://github.com/adityakadoo>GitHub</a> |
<a class=link href=https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh target=_blank rel=noopener>CC BY-NC-SA 4.0</a></span></div></footer></div></body></html>