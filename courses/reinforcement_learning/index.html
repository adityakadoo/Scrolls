<!doctype html><html lang=en-us><head><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=X-UA-Compatible content="ie=edge"><meta name=author content="Aditya Kadoo"><meta name=description content="Notes for CS 747"><link rel=icon href=https://adityakadoo.github.io/Scrolls/favicon.ico><meta name=keywords content=" hugo  latex  theme "><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css integrity=sha384-KiWOvVjnN8qwAZbuQyWDIbfCLFhLXNETzBQjA/92pIowpC0d2O3nppDGQVgwd2nB crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js integrity=sha384-0fdwu/T/EQMsQlrHCCHoH10pkPLlKA1jL5dFyUOvB3lfeT2540/2g6YgSi2BL14p crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/contrib/auto-render.min.js integrity=sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR crossorigin=anonymous></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"\\[",right:"\\]",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1}],ignoredTags:["script","noscript","style","textarea","pre","code","option"],throwOnError:!1})})</script><meta property="og:title" content="Reinforcement Learning"><meta property="og:description" content="Notes for CS 747"><meta property="og:type" content="article"><meta property="og:url" content="https://adityakadoo.github.io/Scrolls/courses/reinforcement_learning/"><meta property="article:section" content="courses"><meta property="article:published_time" content="2023-09-13T14:47:25+05:30"><meta property="article:modified_time" content="2023-09-13T14:47:25+05:30"><link rel=canonical href=https://adityakadoo.github.io/Scrolls/courses/reinforcement_learning/><meta itemprop=name content="Reinforcement Learning"><meta itemprop=description content="Notes for CS 747"><meta itemprop=datePublished content="2023-09-13T14:47:25+05:30"><meta itemprop=dateModified content="2023-09-13T14:47:25+05:30"><meta itemprop=wordCount content="2674"><meta itemprop=keywords content="Computer-Science,Machine-Learning,"><link media=screen rel=stylesheet href=https://adityakadoo.github.io/Scrolls/css/common.css><link media=screen rel=stylesheet href=https://adityakadoo.github.io/Scrolls/css/content.css><title>Reinforcement Learning - Scrolls</title>
<meta name=twitter:card content="summary"><meta name=twitter:title content="Reinforcement Learning"><meta name=twitter:description content="Notes for CS 747"><link rel=stylesheet href=https://adityakadoo.github.io/Scrolls/css/single.css></head><body><div id=wrapper><header id=header><h1>Reinforcement Learning</h1><nav><span class=nav-bar-item><a class=link href></a></span></nav><p>Notes for CS 747</p></header><main id=main class=post><article class=content><div class=tableofcontent><h2>Contents</h2><nav id=TableOfContents><ol><li><a href=#multi-armed-bandits>Multi-armed Bandits</a><ol><li><a href=#epsilon-greedy-algorithms>$\epsilon$-greedy Algorithms</a></li><li><a href=#regret>Regret</a><ol><li><a href=#lower-bound-on-regret>Lower bound on Regret</a></li></ol></li><li><a href=#optimal-regret-algorithms>Optimal Regret Algorithms</a><ol><li><a href=#ucb-algorithm>UCB Algorithm</a></li><li><a href=#kl-ucb-algorithm>KL-UCB Algorithm</a></li><li><a href=#thompson-sampling>Thompson Sampling</a></li></ol></li><li><a href=#bound-on-ucb-regret>Bound on UCB Regret</a></li></ol></li><li><a href=#markov-decision-problems>Markov Decision Problems</a><ol><li><a href=#bellman-equations>Bellman Equations</a><ol><li><a href=#episodic-tasks>Episodic Tasks</a></li></ol></li><li><a href=#optimal-policy-characterization>Optimal Policy Characterization</a><ol><li><a href=#banachs-fixed-point-theorem>Banach&rsquo;s Fixed-point Theorem</a></li><li><a href=#bellman-optimality-operator>Bellman Optimality Operator</a></li></ol></li><li><a href=#optimal-policy-algorithms>Optimal Policy Algorithms</a><ol><li><a href=#value-iteration>Value Iteration</a></li><li><a href=#linear-programming-formulation>Linear Programming Formulation</a></li><li><a href=#policy-improvement>Policy Improvement</a></li></ol></li><li><a href=#pi-variants>PI Variants</a><ol><li><a href=#howards-pi-with-k2>Howard&rsquo;s PI with $k=2$</a></li><li><a href=#batch-switching-policy-iteration>Batch-Switching Policy Iteration</a></li></ol></li></ol></li><li><a href=#reinforcement-learning>Reinforcement Learning</a><ol><li><a href=#problem-definitions>Problem Definitions</a><ol><li><a href=#mdp-assumptions>MDP Assumptions</a></li></ol></li><li><a href=#control-algorithms>Control Algorithms</a><ol><li><a href=#glie>GLIE</a></li><li><a href=#q-learning>Q-Learning</a></li><li><a href=#sarsa>Sarsa</a></li><li><a href=#expected-sarsa>Expected Sarsa</a></li><li><a href=#neural-network-based-hat-q>Neural Network based $\hat Q$</a></li><li><a href=#linear-td-based-hat-q-estimation>Linear TD based $\hat Q$ estimation</a></li></ol></li><li><a href=#prediction-algorithms>Prediction Algorithms</a><ol><li><a href=#monte-carlo-estimation>Monte Carlo Estimation</a></li><li><a href=#temporal-difference-learning-td0>Temporal Difference Learning: TD(0)</a></li><li><a href=#n-step-td-learning>$n$-step TD Learning</a></li><li><a href=#tdlambda-learning>TD($\lambda$) Learning</a></li><li><a href=#linear-tdlambda>Linear TD($\lambda$)</a></li></ol></li></ol></li><li><a href=#advanced-algorithms>Advanced Algorithms</a><ol><li><a href=#decision-time-planning>Decision Time Planning</a><ol><li><a href=#tree-search-on-mdps>Tree Search on MDPs</a></li><li><a href=#rollout-policies>Rollout Policies</a></li><li><a href=#monte-carlo-tree-search-uct-algorithm>Monte Carlo Tree search (UCT Algorithm)</a></li></ol></li><li><a href=#policy-search>Policy Search</a></li><li><a href=#stochastic-policies>Stochastic Policies</a><ol><li><a href=#policy-gradient>Policy Gradient</a></li><li><a href=#variance-reduction>Variance Reduction</a></li><li><a href=#actor-critic-method>Actor-Critic Method</a></li></ol></li></ol></li></ol></nav></div><h2 id=multi-armed-bandits>Multi-armed Bandits</h2><dl><dt><strong>Stochastic Multi-armed Bandits</strong></dt><dd><ul><li>$A$ is the set of $n$ arms</li><li>$\forall a\in A$, $a$ has an associated Bernoulli distribution with mean reward $p_a$</li><li>Highest mean is $p^\star$</li><li>Pull any arm to gain reward and repeat this $T$ (horizon) times</li></ul></dd><dt><strong>Algorithm for maximising reward</strong></dt><dd>For $t=0,1,2,\dots,T-1$:<ul><li>Given the <em>history</em> $h^t=(a^0,r^0,a^1,r^1,\dots,a^{t-1},r^{t-1})$</li><li>Pick an <em>arm</em> $a^t$</li><li>Obtain the <em>reward</em> $r^t$</li></ul></dd><dt><strong>Deterministic Algorithm</strong></dt><dd>Set of all histories $\to A$</dd><dt><strong>Randomized Algorithm</strong></dt><dd>Set of all histories $\to \Delta A$ a.k.a. set of all probability distributions over $A$</dd></dl><h3 id=epsilon-greedy-algorithms>$\epsilon$-greedy Algorithms</h3><dl><dt><strong>$\epsilon$-G1</strong></dt><dd><ul><li>$t\le\epsilon T\implies$ sample uniformly at random</li><li>$t=\lfloor\epsilon T\rfloor\implies$ identify $a^\star$ with the highest empirical mean</li><li>$t>\epsilon T\implies$ sample $a^\star$</li></ul></dd><dt><strong>$\epsilon$-G2</strong></dt><dd><ul><li>$t\le\epsilon T\implies$ sample uniformly at random</li><li>$t>\epsilon T\implies$ sample arm with highest empirical mean</li></ul></dd><dt><strong>$\epsilon$-G3</strong></dt><dd>With probability $\epsilon$ sample uniformly at random; with probability $1-\epsilon$ sample an arm with the highest empirical mean</dd></dl><h3 id=regret>Regret</h3><blockquote><p>Consider the plot of $\mathbb{E}[r^t]$ vs $t$.
It must be bounded between $y=p_{\min}=\min_{a\in A}p_a$ and $y=p^\star$.
On random uniform sampling of arms the graph will be $y=p_{\text{avg}}=\frac{1}{n}\sum_{a\in A}p_a$.
A reasonable learning algorithm will start at $p_{\text{avg}}$ and tend towards $p^\star$</p></blockquote><dl><dt><strong>Regret</strong></dt><dd>For horizon $T$ and given algorithm,
$$
R_T = Tp^\star - \sum_{t=0}^{T-1}\mathbb{E}[r^t]
$$</dd></dl><blockquote><p><strong>Goal</strong> : Find an algorithm for which $\lim_{T\to\infty}\frac{R_T}{T}=0$.
All $\epsilon$-Gi&rsquo;s have linear regret.</p></blockquote><p>How to achieve Sub-linear Regret?</p><ol><li><strong>Infinite Exploration</strong>: In the limit, each arm must be pulled an infinite number of times.</li><li><strong>Greed in the Limit</strong>: Let $\text{exploit}(T)$ denote the number of pulls that are greedy w.r.t. the empirical mean up to the horizon $T$. We want,
$$
\lim_{T\to\infty}\frac{\mathbb{E}[\text{exploit}(T)]}{T}=1
$$</li></ol><dl><dt><strong>Special Bandit Instances</strong> ($\bar{\mathcal{I}}$)</dt><dd>Set of all bandit instances with reward means $\le 1$</dd></dl><blockquote><p><strong>Result</strong> : An algorithm $L$ achieves sub-linear regret on all instances $I\in\bar{\mathcal{I}}$ $\iff$ $L$ does infinite exploration and is greedy in the limit.</p></blockquote><h4 id=lower-bound-on-regret>Lower bound on Regret</h4><blockquote><p><strong>Lai and Robbins&rsquo; theorem</strong> : Let $L$ be an algorithm such that $\forall I\in\bar{\mathcal{I}}$ and $\forall\alpha>0$, as $T\to\infty$,
$$R_T(L,I)=\mathcal{o}(T^\alpha)$$</p><p>Then $\forall I\in\bar{\mathcal{I}}$, as $T\to\infty$:
$$\frac{R_T(L,I)}{\ln(T)}\ge\sum_{a:p_a(I)\not =p^\star(I)}\frac{p^\star(I)-p_a(I)}{KL(p_a(I),p^\star(I))}$$
where for $x,y\in[0,1), KL(x,y)=x\ln\frac{x}{y}+(1-x)\ln\frac{1-x}{1-y}$</p></blockquote><h3 id=optimal-regret-algorithms>Optimal Regret Algorithms</h3><h4 id=ucb-algorithm>UCB Algorithm</h4><ul><li>At time $t$ for every arm $a$,
$$
\text{ucb}^t_a = \hat p^t_a + \sqrt{\frac{2\ln(t)}{u^t_a}}
$$</li><li>$\hat p^t_a$ is the empirical mean of the rewards from arm $a$ and $u^t_a$ is the number of times a has been sampled at time $t$.</li><li>Pull an arm $a$ for which $\text{ucb}^t_a$ is maximum.</li></ul><blockquote><p>UCB achieves $\mathcal{O}(\log(T))$ regret.</p></blockquote><h4 id=kl-ucb-algorithm>KL-UCB Algorithm</h4><ul><li>Define, where $c\ge 3$,
$$
\text{ucb-kl}^t_a = \max\{q\in[\hat p^t_a,1]\ |\ u^t_a\cdot \text{KL}(\hat p_a^t,q)\le\ln(t)+c\ln(\ln(t))\}
$$</li><li>Pull $\argmax_{a\in A}\text{ucb-kl}^t_a$</li></ul><h4 id=thompson-sampling>Thompson Sampling</h4><ul><li>For every arm $a$ with $s_a^t$ successful pulls and $f_a^t$ failed pulls draw a sample,
$$
x_a^t\sim\text{Beta}(s_a^t+1,f_a^t+1)
$$</li><li>Pull arm $a$ with max $x_a^t$</li></ul><blockquote><p>Both KL-UCB and Thompson Sampling manage to get optimal regret.</p></blockquote><h3 id=bound-on-ucb-regret>Bound on UCB Regret</h3><h2 id=markov-decision-problems>Markov Decision Problems</h2><dl><dt><strong>Markov Decision Problem</strong> $\langle S,A,T,R,\gamma\rangle$</dt><dd><ul><li><strong>Set of states</strong>: $S=\{s_1,s_2,\dots,s_n\}$</li><li><strong>Set of actions</strong>: $A=\{a_1,a_2,\dots,a_k\}$</li><li><strong>Transition function</strong>: $T(s,a,s^\prime)$ is the probability of reaching $s^\prime$ by starting at $s$ and taking action $a$. Hence $\forall s\in S,\forall a\in A,\ \sum_{s^\prime\in S}T(s,a,s^\prime)=1$.</li><li><strong>Reward function</strong>: $T(s,a,s^\prime)$ is the reward on reaching $s^\prime$ by starting at $s$ and taking action $a$. Assuming all rewards are from $[-R_{\max},R_{\max}]$ where $R_{\max}\ge 0$.</li><li><strong>Discount factor</strong>: $\gamma$</li></ul></dd><dt><strong>Policy</strong> ($\pi$)</dt><dd>$\pi:S\to A$</dd></dl><blockquote><p>If $\Pi$ is the set of all policies then $|\Pi|=k^n$</p></blockquote><dl><dt><strong>State values for Policy</strong> ($V^\pi$)</dt><dd>For $s\in S$, $V^\pi:S\to\R$,
$$
V^\pi(s)=\mathbb{E}_\pi[r^0+\gamma r^1+\gamma^2r^2+\dots|s^0=s]
$$</dd></dl><blockquote><p>Every MDP has an optimal policy $\pi^\star$ such that,
$$
\forall\pi\in\Pi,\forall s\in S:V^{\pi^\star}(s)\ge V^\pi(s)
$$</p><p><strong>MDP Planning Problem</strong> : Find $\pi^\star$.</p></blockquote><h3 id=bellman-equations>Bellman Equations</h3><blockquote><p>For $\pi\in\Pi,s\in S$,
$$
V^\pi(s) = \sum_{s^\prime\in S}T(s,\pi(s),s^\prime)\{R(s,\pi(s),s^\prime)+\gamma V^\pi(s^\prime)\}
$$</p><ul><li>$n$ equations $n$ variables</li><li>linear</li><li>Guaranteed solution for $\gamma&lt;1$</li></ul></blockquote><p>Therefore brute-force way of finding the state values for all $k^n$ possible policies and then picking the most dominant one is possible but has $\mathcal{O}(\text{poly}(n,k)\cdot k^n)$ time complexity.</p><h4 id=episodic-tasks>Episodic Tasks</h4><dl><dt><strong>Episodic Task</strong></dt><dd><ul><li>Has a new terminal state $s_\top$ from which there are no out going transitions on rewards.</li><li>For every non-terminal state and every policy there is a non-zero probability of reaching the terminal state in a finite number of steps.</li></ul></dd></dl><h5 id=definition-of-values>Definition of values</h5><dl><dt><strong>Infinite discounted reward</strong></dt><dd>$V^\pi(s)=\mathbb{E}_\pi[r^0+\gamma r+\gamma^2r^2+\dots|s^0=s]$</dd><dt><strong>Total reward</strong></dt><dd>$V^\pi(s)=\mathbb{E}_\pi[r^0+r^1+\dots+|s^0=s]$</dd><dt><strong>Finite horizon reward</strong></dt><dd>$V^\pi(s)=\mathbb{E}_\pi[r^0+r^1+\cdots+r^{H-1}|s^0=s]$ where $H\ge 1$</dd><dt><strong>Average reward</strong></dt><dd>$V^\pi(s)=\mathbb{E_{\pi}}[\lim_{m\to\infty}\frac{r^0+r^1+\cdots+R^{m-1}}{m}|s^0=s]$</dd></dl><h3 id=optimal-policy-characterization>Optimal Policy Characterization</h3><h4 id=banachs-fixed-point-theorem>Banach&rsquo;s Fixed-point Theorem</h4><dl><dt><strong>Banach Space</strong> ($X$)</dt><dd>A complete, normed vector space<ul><li><strong>Vector space</strong>: $X$</li><li><strong>Norm</strong>: $\|\cdot\|$</li><li><strong>Complete</strong>: $(X,\|\cdot\|)$ such that every Cauchy sequence has a limit in $X$</li></ul></dd><dt><strong>Contraction mapping</strong> ($Z,l$)</dt><dd>$Z:X\to X$ with contraction factor $0\le l&lt;1$ such that, $\forall u\in X,\forall v\in X,$
$$
\|Zv-Zu\|\le l\|v-u\|
$$</dd><dt><strong>Fixed-point</strong> ($x^\star$)</dt><dd>For $Z$, such that $Zx^\star=x^\star$</dd></dl><blockquote><p><strong>Banach&rsquo;s Fixed-point Theorem</strong> : For a contraction map $Z$ with contraction factor $l$ in a Banach space $(X,\|\cdot\|)$,</p><ol><li>$Z$ has a unique fixed point $x^\star\in X$</li><li>For $x\in X,m\le 0:\|Z^mx-x^\star\|\le l^m\|x-x^\star\|$</li></ol></blockquote><h4 id=bellman-optimality-operator>Bellman Optimality Operator</h4><dl><dt><strong>Bellman optimality operator</strong> ($B^\star$)</dt><dd>$B^\star:\R^n\to\R^n$ for an MDP is defined for $F\in\R^n,s\in S$ as,
$$
(B^\star(F))(s) = \max_{a\in A}\sum_{s^\prime\in S}T(s,a,s^\prime)\{R(s,a,s^\prime)+\gamma F(s^\prime)\}
$$</dd><dt><strong>Max norm</strong> ($\|\cdot\|_\infty$)</dt><dd>For $F=(f_1,f_2,\dots,f_n)\in\R^n$,
$$
\|F\|_\infty=\max\{|f_1|,|f_2|,\dots,|f_n|\}
$$</dd></dl><blockquote><p><strong>Result</strong> : $(\R^n,\|\cdot\|_\infty)$ is a Banach space.</p><p>$\therefore B^\star$ is a contraction map in $(\R^n,\|\cdot\|_\infty)$ with contraction factor $\gamma$.</p></blockquote><dl><dt><strong>Optimal Value Function</strong> $(V^\star)$</dt><dd>Denote the fixed point $V^\star:S\to\R$ (alternatively, $V^\star\in\R^n$) such that $B^\star(V^\star)=V^\star$. For $s\in S$,
$$
V^\star(s)=\max_{a\in A}\sum_{s^\prime\in S}T(s,a,s^\prime)\{R(s,a,s^\prime)+\gamma V^\star(s^\prime)\}
$$</dd></dl><h3 id=optimal-policy-algorithms>Optimal Policy Algorithms</h3><h4 id=value-iteration>Value Iteration</h4><ol><li>$V_0\leftarrow$ Arbitrary, element-wise bounded, $n$-length vector.</li><li>$t\leftarrow 0$</li><li>Repeat:<ol><li>For $s\in S$:<ol><li>$V_{t+1}(s)\leftarrow\max_{a\in A}\sum_{s^\prime\in S}T(s,a,s^\prime)(R(s,a,s^\prime+\gamma V_t(s^\prime)))$</li></ol></li><li>$t\leftarrow t+1$</li></ol></li><li>Until $V_t\equiv V_{t-1}$</li></ol><h4 id=linear-programming-formulation>Linear Programming Formulation</h4><dl><dt><strong>Vector Comparison</strong> ($\succeq,\succ$)</dt><dd><ul><li>For $X:S\to\R$ and $Y:S\to\R$ (equivalently $X,Y\in\R^n$) we define,
$$
X\succeq Y\iff \forall s\in S:X(s)\ge Y(s)\\
X\succ Y\iff X\succeq Y\And\exist s\in S:X(s)>Y(s)
$$</li><li>For policies $\pi_1,\pi_2\in\Pi$ we define,
$$
\pi_1\succeq\pi_2\iff V^{\pi_1}\succeq V^{\pi_2}\\
\pi_1\succ\pi_2\iff V^{\pi_1}\succ V^{\pi_2}\\
$$</li><li>2 policies can also be <em>incomparable</em> i.e. $\pi_1\not\succeq\pi_2$ and $\pi_2\not\succeq\pi_1$</li><li>$\pi_1\succeq\pi_2$ and $\pi_2\succeq\pi_1\iff V^{\pi_1}=V^{\pi_2}$</li></ul></dd></dl><blockquote><p><strong>Result</strong> : $B^\star$ preserves $\succeq$. $\forall X,Y:S\to\R^n$,
$$
X\succeq Y\implies B^\star(X)\succeq B^\star(Y)
$$</p><p>$\therefore$ For all $V\not =V^\star$ in the feasible set, $V\succ V^\star$.
$$
\implies\sum_{s\in S}V(s)>\sum_{s\in S}V^\star(s)
$$</p></blockquote><dl><dt><strong>Linear Programming Formulation</strong></dt><dd><ul><li>Maximise $\left(-\sum_{s\in S}V(s)\right)$,</li><li>Subject to $V(s)\ge\sum_{s^\prime\in S}T(s,a,s^\prime)\{R(s,a,s^\prime)+\gamma V(s^\prime)\},\ \forall s\in S,a\in A$.</li></ul></dd></dl><blockquote><p>This LP has $n$ variables and $nk$ constraints and the solution is $V^\star$.
The dual of this LP has nk variables with $n$ constraints and it&rsquo;s solution is $\pi^\star$.</p></blockquote><h4 id=policy-improvement>Policy Improvement</h4><dl><dt><strong>Action Value Function</strong> ($Q^\pi:S\times A\to\R$)</dt><dd>For $\pi\in\Pi,s\in S,a\in A$,
$$Q^\pi(s,a)=\mathbb{E}[r^0+\gamma r^1+\gamma^2r^2+\dots|s^0=s;a^0=a;a^t=\pi(s^t),\ \forall t\ge1$$</dd></dl><blockquote><p>For $s\in S,a\in A$,
$$
Q^\pi(s,a) = \sum_{s^\prime\in S}T(s,a,s^\prime)\{R(s,a,s^\prime)+\gamma V^\pi(s^\prime)\}
$$</p><p>$Q^\pi(s,\pi(s))=V^\pi(s)$</p><p>All optimal policies have the same optimal action value function $Q^\star$.</p></blockquote><dl><dt>$\text{IA}:\Pi\times S\to \mathcal{P}(A)$</dt><dd>For $\pi\in\Pi,s\in S$,
$$\text{IA}(\pi,s)=\{a\in A:Q^\pi(s,a)>V^\pi(s)\}$$</dd><dt>$\text{IS}:\Pi\to\mathcal{P}(S)$</dt><dd>For $\pi\in\Pi$,
$$\text{IS}(\pi)=\{s\in S:|\text{IA}(\pi,s)|\ge 1\}$$</dd></dl><blockquote><p><strong>Policy Improvement Theorem</strong> :</p><ol><li>If $\text{IS}(\pi)=\emptyset$ then $\pi$ is an optimal policy, else</li><li>if $\pi^\prime$ is obtained by policy improvement on $\pi$, then $\pi^\prime\succ\pi$.</li></ol><p>$\text{IS}(\pi^\star)=\emptyset\iff B^\star(V^{\pi^\star})=V^{\pi^\star}$</p></blockquote><dl><dt><strong>Bellman Operator</strong> ($B^\pi:\R^n\to\R^n$)</dt><dd>For $\pi\in\Pi,X:S\to\R,s\in S$,
$$(B^\pi(X))(s)=\sum_{s^\prime\in S}T(s,\pi(s),s^\prime)(R(s,\pi(s),s^\prime)+\gamma X(s^\prime))$$</dd></dl><blockquote><p>$B^\pi$ is a contraction mapping with contraction factor $\gamma$.</p><p>For $X:s\to\R,\ \lim_{l\to\infty}(B^\pi)^l(X)=V^\pi$
For $X:s\to\R,Y:S\to\R,\ X\succeq Y\implies B^\pi(X)\succeq B^\pi(Y)$</p><p>$B^{\pi^\prime}(V^\pi)(s)=Q^\pi(s,\pi^\prime(s))$</p></blockquote><h5 id=policy-iteration-algorithm>Policy Iteration Algorithm</h5><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=ln>1</span><span class=cl><span class=k>def</span> <span class=nf>Policy_iteration</span><span class=p>(</span><span class=n>mdp</span><span class=p>):</span>
</span></span><span class=line><span class=ln>2</span><span class=cl>  <span class=n>Pi</span> <span class=o>=</span> <span class=n>mdp</span><span class=o>.</span><span class=n>rand_policy</span><span class=p>()</span>
</span></span><span class=line><span class=ln>3</span><span class=cl>  <span class=k>while</span> <span class=n>mdp</span><span class=o>.</span><span class=n>is_improvable</span><span class=p>(</span><span class=n>Pi</span><span class=p>):</span>
</span></span><span class=line><span class=ln>4</span><span class=cl>    <span class=n>Pi</span> <span class=o>=</span> <span class=n>mdp</span><span class=o>.</span><span class=n>improve</span><span class=p>(</span><span class=n>Pi</span><span class=p>)</span>
</span></span><span class=line><span class=ln>5</span><span class=cl>  <span class=k>return</span> <span class=n>Pi</span>
</span></span></code></pre></div><h3 id=pi-variants>PI Variants</h3><dl><dt><strong>Howard&rsquo;s Policy Iteration</strong></dt><dd>Greedy; switch all improvable states.</dd><dt><strong>Random Policy Iteration</strong></dt><dd>Switch a non-empty subset of improvable states chosen uniformly at random.</dd><dt><strong>Simple Policy Iteration</strong></dt><dd>Assume a fixed indexing of states and always improve the state with the highest index.</dd><dt><strong>Upper Bound</strong> ($U(n,k)$)</dt><dd>For a set of PI Variants $\mathcal{L}$ and MDP $M$, the expected number of policy evalutaions performed by $L$ on $M$ if initialised at any $\pi$ is at most $U(n,k)$.</dd></dl><blockquote><table><thead><tr><th>PI Variant</th><th>Type</th><th>k=2</th><th>General k</th></tr></thead><tbody><tr><td>Howard&rsquo;s PI</td><td>Deterministic</td><td>$\mathcal{O}(\frac{2^n}{n})$</td><td>$\mathcal{O}(\frac{k^n}{n})$</td></tr><tr><td>Mansour and Singh&rsquo;s Random PI [MS99]</td><td>Randomized</td><td>$1.7172^n$</td><td>$\mathcal{O}(\frac{k}{2})^n$</td></tr><tr><td>Mansour and Singh&rsquo;s Random PI [HPZ14]</td><td>Randomized</td><td>$\text{poly}(n)\cdot 1.5^n$</td><td>&ndash;</td></tr></tbody></table></blockquote><dl><dt><strong>Lower Bound</strong> ($X(n,k)$)</dt><dd>For a set of PI Variants $\mathcal{L}$ and MDP $M$, the expected number of policy evalutaions performed by $L$ on $M$ if initialised at any $\pi$ is at least $X(n,k)$.</dd></dl><blockquote><ul><li>Howard&rsquo;s PI on $n$-state, 2-action MDPs : $\Omega(n)$</li><li>Simple PI on $n$-state, 2-action MDPs : $\Omega(2^n)$</li></ul></blockquote><h4 id=howards-pi-with-k2>Howard&rsquo;s PI with $k=2$</h4><blockquote><p>Non-optimal policies $\pi,\pi^\prime\in\Pi$ cannot have the same set of improvable states.</p><p>If $\pi$ has $m$ improvable states and $\pi$ states and $\pi,\pi^\prime$ (Howard&rsquo;s PI) then there exist $m$ policies $\pi^{\prime\prime}$ such that $\pi^\prime\succeq\pi^{\prime\prime}\succ\pi$.</p></blockquote><blockquote><p>Number of iterations taken by Howard&rsquo;s PI: $O(\frac{2^n}{n})$</p></blockquote><h4 id=batch-switching-policy-iteration>Batch-Switching Policy Iteration</h4><blockquote><p>Howard&rsquo;s Policy Iteration takes at most 3 iterations on 2-state 2-action MDP!</p></blockquote><dl><dt><strong>BSPI</strong></dt><dd>Partition states in 2-sized batches arranged from right to left. Improve the rightmost set containing an improvable state.</dd></dl><blockquote><p>BSPI of batch size 2 is bounded by $\mathcal{O}(\sqrt{3}^n)$.
Tighter bounds for higher batch-sizes.</p></blockquote><h2 id=reinforcement-learning>Reinforcement Learning</h2><h3 id=problem-definitions>Problem Definitions</h3><dl><dt><strong>MDP Histories</strong> ($H$)</dt><dd>$H = \{h^t\ |\ t\ge 0, h^t=(s^0, a^0, r^0, s^1,\dots,r^{t-1},s^t)\}$<ul><li>The environment decides the initial and state $s^0$.</li><li>After $i\ge 0$ transitions, the agent choses the action $a_i$.</li><li>The environment decides the reward $r_i$ and the next state $s_{i+1}$ based on an underlying MDP unknown to the agent.</li></ul></dd><dt><strong>Learning Algorithm</strong> ($L:H\to A$)</dt><dd>Takes a history $h^t$ and returns a particular action $L(h^t)$ for the agent</dd></dl><blockquote><p><strong>Control Problem</strong> : Come up with an $L$ such that,
$$
\lim_{|H|\to\infty}\frac{1}{|H|}\left(\sum_{0}^{|H|-1}\mathbb{P}[\ L(h^t)=\pi^\star(s^t)\ ]\right)=1
$$</p></blockquote><dl><dt><strong>Learning Algorithm</strong> ($\hat V:H\to (S\to\R)$)</dt><dd>Takes a history $h^t$ and returns a value function $\hat V(h^t)$</dd></dl><blockquote><p><strong>Prediction Problem</strong> : Given a policy $\pi$, come up with an $L$ such that,
$$
\lim_{t\to\infty}\hat V(h^t)=V^\pi
$$
where the histories $h^t$ are generate using an agent that follows the policy $\pi$.</p></blockquote><h4 id=mdp-assumptions>MDP Assumptions</h4><dl><dt><strong>Irreducible</strong></dt><dd>An MDP in which $\forall\pi\in\Pi$, there exists a directed path between any two states $s,s^\prime\in S$</dd><dt><strong>Aperiodic</strong></dt><dd>An MDP in which $\forall\pi\in\Pi$, $\forall s\in S$, $\gcd(Y^\pi(s))=1$.<ul><li>$X^\pi(s,t)$ is the set of all states that can be reached from $s$ in $t$ step following policy $\pi$.</li><li>$Y^\pi(s)$ is the set of all $t$ such that $s\in X^\pi(s,t)$.</li></ul></dd><dt><strong>Ergodic</strong></dt><dd>An MDP that is irreducible and aperiodic.<ul><li>This means for every policy $\pi$ there exists a unique steady state distribution $\mu^\pi:S\to\R$ subject to $\sum_{s\in S}\mu^\pi(s)=1$</li><li>For histories $h^t$ generated by an agent following policy $\pi$ from an arbitrary $s^0$,
$$
\mu^\pi(s)=\lim_{t\to\infty}\mathbb{P}[s^t=s]
$$</li></ul></dd></dl><h3 id=control-algorithms>Control Algorithms</h3><h4 id=glie>GLIE</h4><p>Model is valid only after all the possible pairs of states and action are picked at least once.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=ln>1</span><span class=cl><span class=k>def</span> <span class=nf>L</span><span class=p>(</span><span class=n>h_t</span><span class=p>):</span>
</span></span><span class=line><span class=ln>2</span><span class=cl>  <span class=n>modelValid</span><span class=p>,</span> <span class=n>pred_T</span><span class=p>,</span> <span class=n>pred_R</span> <span class=o>=</span> <span class=n>UpdateModel</span><span class=p>(</span><span class=n>h_t</span><span class=p>)</span>
</span></span><span class=line><span class=ln>3</span><span class=cl>  <span class=n>p</span> <span class=o>=</span> <span class=n>random</span><span class=o>.</span><span class=n>Uniform</span><span class=p>()</span>
</span></span><span class=line><span class=ln>4</span><span class=cl>  <span class=k>if</span> <span class=n>modelValid</span> <span class=ow>and</span> <span class=n>p</span> <span class=o>&lt;</span> <span class=mi>1</span> <span class=o>-</span> <span class=n>epsilon_t</span><span class=p>:</span>
</span></span><span class=line><span class=ln>5</span><span class=cl>    <span class=n>pi_star</span> <span class=o>=</span> <span class=n>MDPPlanner</span><span class=p>(</span><span class=n>S</span><span class=p>,</span> <span class=n>A</span><span class=p>,</span> <span class=n>pred_T</span><span class=p>,</span> <span class=n>pred_R</span><span class=p>,</span> <span class=n>gamma</span><span class=p>)</span>
</span></span><span class=line><span class=ln>6</span><span class=cl>    <span class=n>a_t</span> <span class=o>=</span> <span class=n>pi_star</span><span class=p>(</span><span class=n>s_t</span><span class=p>)</span>
</span></span><span class=line><span class=ln>7</span><span class=cl>  <span class=k>else</span><span class=p>:</span>
</span></span><span class=line><span class=ln>8</span><span class=cl>    <span class=n>a_t</span> <span class=o>=</span> <span class=n>random</span><span class=o>.</span><span class=n>Choice</span><span class=p>(</span><span class=n>A</span><span class=p>)</span>
</span></span><span class=line><span class=ln>9</span><span class=cl>  <span class=k>return</span> <span class=n>a_t</span>
</span></span></code></pre></div><h4 id=q-learning>Q-Learning</h4><ol><li>Maintain action state values $\hat Q:S\times A\to\R$.</li><li>For every action, make the following updates,
$$
\hat Q^{t+1}(s^t,a^t) = \hat Q^{t}(s^t,a^t)+\alpha_{t+1}(r^t+\gamma\max_{a\in A}\hat Q^t(s^{t+1},a) - \hat Q^t(s^t,a^t))
$$</li><li>Pick the action $\arg\max_{a\in A}\hat Q^T(s^T,a)$</li></ol><h4 id=sarsa>Sarsa</h4><ol><li>Maintain action state values $\hat Q:S\times A\to\R$.</li><li>For every action, make the following updates,
$$
\hat Q^{t+1}(s^t,a^t) = \hat Q^{t}(s^t,a^t)+\alpha_{t+1}(r^t+\gamma\hat Q^t(s^{t+1},a^{t+1}) - \hat Q^t(s^t,a^t))
$$</li><li>Pick the action $\arg\max_{a\in A}\hat Q^T(s^T,a)$</li></ol><h4 id=expected-sarsa>Expected Sarsa</h4><ol><li>Maintain action state values $\hat Q:S\times A\to\R$.</li><li>For every action, make the following updates,
$$
\hat Q^{t+1}(s^t,a^t) = \hat Q^{t}(s^t,a^t)+\alpha_{t+1}(r^t+\gamma\sum_{a\in A}\pi^t(s^{t+1},a)\hat Q^t(s^{t+1},a) - \hat Q^t(s^t,a^t))
$$</li><li>Pick the action $\arg\max_{a\in A}\hat Q^T(s^T,a)$</li></ol><h4 id=neural-network-based-hat-q>Neural Network based $\hat Q$</h4><p>When the state space is made up of variables that take up real values, these state vectors $s$ can be passed to a neural network to return $\hat Q(s,a)$ for all actions $a$.</p><h4 id=linear-td-based-hat-q-estimation>Linear TD based $\hat Q$ estimation</h4><p>$$
\hat Q(w,s,a) = \langle w, x(s,a)\rangle\\
\ \\
$$
This relation is used to perform <a href=#linear-tdlambda>linear TD</a> learning followed by anyone of the above 3 algorithms. Linear Sarsa($\lambda$) is popular.</p><h3 id=prediction-algorithms>Prediction Algorithms</h3><h4 id=monte-carlo-estimation>Monte Carlo Estimation</h4><dl><dt>$\boldsymbol{1}:S\times\N\times\N\to\{0,1\}$</dt><dd>$\boldsymbol{1}(s,i,j)$ is 1 when state $s$ is visited at least $j$ times on episode $i$ else 0</dd><dt>$G:S\times\N\times\N\to\R$</dt><dd>$G(s,i,j)$ is the discounted long term reward on episode $i$ from $j^\text{th}$ visit of state $s$</dd></dl><p>Working estimates of value function</p><ul><li>First visit:
$$
\hat V^N_{\text{First-visit}} = \frac{\sum_{i=1}^{N}G(s,i,1)}{\sum_{i=1}^{N}\boldsymbol{1}(s,i,1)}
$$</li><li>Every visit:
$$
\hat V^N_{\text{Every-visit}} = \frac{\sum_{i=1}^{N}\sum_{j=1}^{\infty}G(s,i,j)}{\sum_{i=1}^{N}\sum_{j=1}^{\infty}\boldsymbol{1}(s,i,j)}
$$</li><li>Second visit:
$$
\hat V^N_{\text{Second-visit}} = \frac{\sum_{i=1}^{N}G(s,i,2)}{\sum_{i=1}^{N}\boldsymbol{1}(s,i,2)}
$$</li></ul><blockquote><p>The last visit estimate doesn&rsquo;t converge to $V^\pi$.</p></blockquote><p>Online implementation,</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=ln>1</span><span class=cl><span class=k>def</span> <span class=nf>episodicUpdate</span><span class=p>(</span><span class=n>V</span><span class=p>,</span> <span class=n>t</span><span class=p>,</span> <span class=n>G</span><span class=p>):</span>
</span></span><span class=line><span class=ln>2</span><span class=cl>  <span class=k>if</span> <span class=n>t</span> <span class=o>==</span> <span class=mi>0</span><span class=p>:</span>
</span></span><span class=line><span class=ln>3</span><span class=cl>    <span class=k>return</span> <span class=p>[</span><span class=mi>0</span><span class=p>]</span><span class=o>*</span><span class=nb>len</span><span class=p>(</span><span class=n>S</span><span class=p>)</span>
</span></span><span class=line><span class=ln>4</span><span class=cl>  <span class=n>alpha</span> <span class=o>=</span> <span class=mi>1</span><span class=o>/</span><span class=n>t</span>
</span></span><span class=line><span class=ln>5</span><span class=cl>  <span class=k>for</span> <span class=n>s</span> <span class=ow>in</span> <span class=n>S</span><span class=p>:</span>
</span></span><span class=line><span class=ln>6</span><span class=cl>    <span class=n>V</span><span class=p>[</span><span class=n>s</span><span class=p>]</span> <span class=o>=</span> <span class=p>(</span><span class=mi>1</span><span class=o>-</span><span class=n>alpha</span><span class=p>)</span><span class=o>*</span><span class=n>V</span><span class=p>[</span><span class=n>s</span><span class=p>]</span> <span class=o>+</span> <span class=n>alpha</span><span class=o>*</span><span class=n>G</span><span class=p>[</span><span class=n>s</span><span class=p>]</span>
</span></span></code></pre></div><p>Here $\alpha_t$ is called the learning rate. The above algorithm works for any $\alpha_t$ that follows,</p><ul><li>$\sum_{t=1}^\infty\alpha_t=\infty$</li><li>$\sum_{t=1}^\infty\alpha_t^2&lt;\infty$</li></ul><h4 id=temporal-difference-learning-td0>Temporal Difference Learning: TD(0)</h4><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=ln> 1</span><span class=cl><span class=k>def</span> <span class=nf>hat_V</span><span class=p>(</span><span class=n>h_T</span><span class=p>):</span>
</span></span><span class=line><span class=ln> 2</span><span class=cl>  <span class=n>T</span> <span class=o>=</span> <span class=nb>len</span><span class=p>(</span><span class=n>h_T</span><span class=p>)</span><span class=o>//</span><span class=mi>3</span>
</span></span><span class=line><span class=ln> 3</span><span class=cl>  <span class=n>V</span> <span class=o>=</span> <span class=p>[</span><span class=mi>0</span><span class=p>]</span><span class=o>*</span><span class=nb>len</span><span class=p>(</span><span class=n>S</span><span class=p>)</span>
</span></span><span class=line><span class=ln> 4</span><span class=cl>  <span class=k>for</span> <span class=n>t</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span><span class=n>T</span><span class=p>):</span>
</span></span><span class=line><span class=ln> 5</span><span class=cl>    <span class=n>alpha</span> <span class=o>=</span> <span class=mi>1</span><span class=o>/</span><span class=n>t</span>
</span></span><span class=line><span class=ln> 6</span><span class=cl>    <span class=n>s</span> <span class=o>=</span> <span class=n>h_T</span><span class=p>[</span><span class=mi>3</span><span class=o>*</span><span class=n>t</span><span class=p>]</span>
</span></span><span class=line><span class=ln> 7</span><span class=cl>    <span class=n>r</span> <span class=o>=</span> <span class=n>h_T</span><span class=p>[</span><span class=mi>3</span><span class=o>*</span><span class=n>t</span><span class=o>+</span><span class=mi>2</span><span class=p>]</span>
</span></span><span class=line><span class=ln> 8</span><span class=cl>    <span class=n>s_next</span> <span class=o>=</span> <span class=n>h_T</span><span class=p>[</span><span class=mi>3</span><span class=o>*</span><span class=n>t</span><span class=o>+</span><span class=mi>3</span><span class=p>]</span>
</span></span><span class=line><span class=ln> 9</span><span class=cl>    <span class=n>V</span><span class=p>[</span><span class=n>s</span><span class=p>]</span> <span class=o>=</span> <span class=n>V</span><span class=p>[</span><span class=n>s</span><span class=p>]</span> <span class=o>+</span> <span class=n>alpha</span><span class=o>*</span><span class=p>(</span><span class=n>r</span> <span class=o>+</span> <span class=n>gamma</span><span class=o>*</span><span class=n>V</span><span class=p>[</span><span class=n>s_next</span><span class=p>]</span> <span class=o>-</span> <span class=n>V</span><span class=p>[</span><span class=n>s</span><span class=p>])</span>
</span></span><span class=line><span class=ln>10</span><span class=cl>  <span class=k>return</span> <span class=n>V</span>
</span></span></code></pre></div><ul><li>$\hat V^t(s^t)$ is the current estimate after $t$ steps</li><li>$r^t+\gamma\hat V^t(s^{t+1})$ is the new estimate</li><li><code>(r + gamma*V[s_next] - V[s])</code> is the temporal difference prediction error</li></ul><blockquote><p>The first-visit and every-visit monte carlo estimates are the same as least square error estimates for some particular error functions.</p><p>The TD(0) estimate is the same as the maximum likelihood estimate of the value function.</p></blockquote><h4 id=n-step-td-learning>$n$-step TD Learning</h4><dl><dt><strong>$n$-step Returns</strong> ($G_{k:k+n}$)</dt><dd>For a given history $h^t$,
$$
G_{k:k+n} = \sum_{i=0}^{n-1}\gamma^ir^{t+i}+\gamma^nV^{t+n-1}(s^{t+n})
$$</dd></dl><p>For episodic tasks when terminal state is encountered at $k+n^\prime$ such that $1\le n^\prime&lt;n$, take $G_{k:k+n}=G_{k:k+n^\prime}$.</p><dl><dt><strong>$n$-step TD</strong></dt><dd>Updates of the form,
$$
V^{t+n}(s^t)\leftarrow V^{t+n-1}(s^t) + \alpha_t(G_{t:t+n}-V^{t+n-1}(s^t))
$$</dd></dl><blockquote><p>Any convex linear normalized combination of the $n$-step returns can also be used.</p></blockquote><h4 id=tdlambda-learning>TD($\lambda$) Learning</h4><dl><dt><strong>$\lambda$ Return</strong> ($G^\lambda_t$)</dt><dd>For $\lambda\in(0,1]$,
$$
G_t^\lambda = (1-\lambda)\sum_{n=1}^{T-t-1}\lambda^{n-1}G_{t:t+n}+\lambda^{T-t-1}G_{t:T}
$$</dd></dl><p>Here $s^T=s_{\top}$. TD learning with these returns is called TD($\lambda$) learning.</p><h4 id=linear-tdlambda>Linear TD($\lambda$)</h4><dl><dt><strong>Features</strong> ($x:S\to\R^d$)</dt><dd>A feature vector extracted from the given state that has all the relevant real-valued parameters that determine the state. Usually $d\ll|S|$.</dd><dt><strong>Weights</strong> ($w\in\R^d$)</dt><dd>A vector such that,
$$
\hat V(w, s) = \langle w, x(s)\rangle
$$</dd></dl><p>Common best choice for $w$ is given by,
$$
w^\star = \argmin_{w\in\R^d} \text{MSVE}(w)\\
\ \\
\text{MSVE}(w) = \frac{1}{2}\sum_{s\in S}\mu^\pi(s)\cdot[V^\pi(s)-\hat V(w,s)]^2
$$</p><p>This $w^\star$ can be found using stochastic gradient descent as,
$$
w^{t+1} \leftarrow w^t + \alpha_{t+1}\sum_{s\in S}\mu^\pi(s)\cdot[V^\pi(s)-\hat V(w^t,s^t)]\cdot\nabla_{w}\hat V(w^t,s^t)
$$</p><p>In practice since $\mu^\pi(s)$ and $V^\pi(s)$ are unknown,
$$
w^{t+1} \leftarrow w^t + \alpha_{t+1}\cdot[G^\lambda_t-\langle w^t,x(s^t)\rangle]x(s^t)
$$
This is the Linear TD($\lambda$) algorithm.</p><blockquote><p>This algorithm converges as,
$$
\text{MSVE}(w^\infty_\lambda) \le \frac{1-\gamma\lambda}{1-\gamma}\cdot\text{MSVE}(w^\star)
$$</p></blockquote><blockquote><p><strong>Tile Coding</strong>: This is used when there is the features and the learning objective ($V$ or $Q$) is supposed to have a non-linear relationship. In this case every feature&rsquo;s space is separately divided into tiles and feature vectors that give boolean encoding for lying within a tile are used in place of each of the original features.
$$
\hat V(w,s) = \sum_{j=1}^{d}F_j(x_j(s))\\
\ \\
F_j(x_j(s)) = \langle w_j,f_j(x_j(s))\rangle\\
\ \\
f_{ji} = \begin{Bmatrix}
1 & x_j(s) \text{ lies in }i^{\text{th}}\text{ tile}\\
0 & \text{otherwise}
\end{Bmatrix}
$$</p></blockquote><h2 id=advanced-algorithms>Advanced Algorithms</h2><h3 id=decision-time-planning>Decision Time Planning</h3><h4 id=tree-search-on-mdps>Tree Search on MDPs</h4><ol><li>When at a state $s$, make a tree of states as nodes and actions as transitions such that different paths denote different possible trajectories.</li><li>Fix a height $h=\Theta(\frac{1}{1-\gamma})$ of the tree.</li><li>Set $Q^h=0$ for all the leaves.</li><li>For internal nodes with $d=h-1,h-2,\dots$
$$
V^d(s)\leftarrow\max_{a\in A}Q^{d+1}(s,a)\\
\ \\
Q^d(s,a)\leftarrow\sum_{s^\prime\in S}T(s,a,s^\prime)\{R(s,a,s^\prime)+\gamma V^d(s^\prime)\}
$$</li></ol><blockquote><p>Drawbacks:</p><ul><li>Tree needs to be too large.</li><li>Explores all clearly inferior branches.</li></ul></blockquote><h4 id=rollout-policies>Rollout Policies</h4><ul><li>From current state $s$, for each action $a\in A$, generate $N$ trajectories by taking $a$ from $s$ and thereafter following $\pi$.</li><li>Set $\hat Q(s,a)$ as average of episodic returns.</li><li>$\pi^\prime(s)=\argmax_{a\in A}\hat Q(s,a)$</li><li>Repeat this on the next state $s^\prime$ with $\pi^\prime$.</li></ul><h4 id=monte-carlo-tree-search-uct-algorithm>Monte Carlo Tree search (UCT Algorithm)</h4><p>Repeat $N$ times when at state $s_0$:</p><ul><li>Generate trajectories by calling model $M$.</li><li>From $s$ take action $\argmax_{a\in A}\text{ucb}(s,a)$ where,
$$
\text{ucb}(s,a) = \hat Q(s,a) + C_p\sqrt{\frac{\ln{t}}{\text{visits}(s,a)}}
$$</li><li>From leaves follow rollout policy $\pi$</li><li>Update $\hat Q,\text{ucb}, \text{visits}$ for all $(s,a)$ visited in the trajectory</li><li>$\pi(s)\leftarrow\argmax_{a\in A}\hat Q(s,a)$</li></ul><p>Finally take action $\argmax_{a\in A}\text{ucb}(s_0,a)$</p><h3 id=policy-search>Policy Search</h3><ul><li>Black-box optimization approach in the context of reinforcement learning</li><li>Ignores the underlying markovian structure of the MDP</li><li>Can be helpful when dealing with problems that don&rsquo;t coform well with the MDP formulation</li><li>Techniques involve grid search, random search, local search etc.</li><li>Only effective when search space is of low dimensionality</li></ul><h3 id=stochastic-policies>Stochastic Policies</h3><dl><dt><strong>Stochastic Policies</strong> ($\pi:S\times A\to[0,1]$)</dt><dd>Probability distribution over action at every state such that $\sum_{a\in A}\pi(s,a)=1,\ \forall s\in S$</dd></dl><h4 id=policy-gradient>Policy Gradient</h4><p>Let $\theta$ be a parameter for $\pi$ and $x$ is a feature function such that,
$$
\pi(s,a;\theta) = \frac{e^{\theta\cdot x(s,a)}}{\sum_{b\in A}e^{\theta\cdot x(s,b)}}
$$</p><p>Assuming $J(\theta)=V^\pi(s^0)$,
$$
\nabla_{\theta} J(\theta)=\mathbb{E_{\pi}}\left[\ \sum_{t=0}^{T-1}(\nabla_{\theta}\ln{\pi(s^t,a^t)})\cdot G_{t:T}\ \right]\\
\ \\
\theta\leftarrow\theta+\alpha\sum_{t=0}^{T-1}(\nabla_{\theta}\ln{\pi(s^t,a^t)})\cdot G_{t:T}
$$</p><h4 id=variance-reduction>Variance Reduction</h4><p>$$
\theta\leftarrow\theta+\alpha\sum_{t=0}^{T-1}(\nabla_{\theta}\ln{\pi(s^t,a^t)})\cdot (G_{t:T}-\hat V(s^t))
$$</p><h4 id=actor-critic-method>Actor-Critic Method</h4><ul><li>Actor updates $\theta$ and hence $\pi_\theta$</li><li>Critic evaluates $\hat V$ for $\pi_\theta$ (using say TD(0)) provides input for gradient descent updates
$$
\theta\leftarrow\theta+\alpha\sum_{t=0}^{T-1}(\nabla_{\theta}\ln{\pi(s^t,a^t)})\cdot (r^t + \hat V(s^{t+1})-\hat V(s^t))
$$</li></ul></article></main><footer id=footer><div><span>© 2022</span> - <span>2023</span></div><div><span>Powered by </span><a class=link href=https://gohugo.io/>Hugo</a>
<span>🍦 Theme </span><a class=link href=https://github.com/queensferryme/hugo-theme-texify>TeXify</a></div><div class=footnote><span>Follow me on <a class=link href=https://github.com/adityakadoo>GitHub</a> |
<a class=link href=https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh target=_blank rel=noopener>CC BY-NC-SA 4.0</a></span></div></footer></div><link media=screen rel=stylesheet href=https://adityakadoo.github.io/Scrolls/css/syntax.css></body></html>