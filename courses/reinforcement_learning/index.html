<!doctype html><html lang=en-us><head><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=X-UA-Compatible content="ie=edge"><meta name=author content="Aditya Kadoo"><meta name=description content="Notes for CS 747"><link rel=icon href=https://adityakadoo.github.io/Scrolls/favicon.ico><meta name=keywords content=" hugo  latex  theme "><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css integrity=sha384-KiWOvVjnN8qwAZbuQyWDIbfCLFhLXNETzBQjA/92pIowpC0d2O3nppDGQVgwd2nB crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js integrity=sha384-0fdwu/T/EQMsQlrHCCHoH10pkPLlKA1jL5dFyUOvB3lfeT2540/2g6YgSi2BL14p crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/contrib/auto-render.min.js integrity=sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR crossorigin=anonymous></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"\\[",right:"\\]",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1}],ignoredTags:["script","noscript","style","textarea","pre","code","option"],throwOnError:!1})})</script><meta property="og:title" content="Reinforcement Learning"><meta property="og:description" content="Notes for CS 747"><meta property="og:type" content="article"><meta property="og:url" content="https://adityakadoo.github.io/Scrolls/courses/reinforcement_learning/"><meta property="article:section" content="courses"><meta property="article:published_time" content="2023-09-13T14:47:25+05:30"><meta property="article:modified_time" content="2023-09-13T14:47:25+05:30"><link rel=canonical href=https://adityakadoo.github.io/Scrolls/courses/reinforcement_learning/><meta itemprop=name content="Reinforcement Learning"><meta itemprop=description content="Notes for CS 747"><meta itemprop=datePublished content="2023-09-13T14:47:25+05:30"><meta itemprop=dateModified content="2023-09-13T14:47:25+05:30"><meta itemprop=wordCount content="1367"><meta itemprop=keywords content="Computer-Science,Machine-Learning,"><link media=screen rel=stylesheet href=https://adityakadoo.github.io/Scrolls/css/common.css><link media=screen rel=stylesheet href=https://adityakadoo.github.io/Scrolls/css/content.css><title>Reinforcement Learning - Scrolls</title>
<meta name=twitter:card content="summary"><meta name=twitter:title content="Reinforcement Learning"><meta name=twitter:description content="Notes for CS 747"><link rel=stylesheet href=https://adityakadoo.github.io/Scrolls/css/single.css></head><body><div id=wrapper><header id=header><h1>Reinforcement Learning</h1><nav><span class=nav-bar-item><a class=link href></a></span></nav><p>Notes for CS 747</p></header><main id=main class=post><article class=content><div class=tableofcontent><h2>Contents</h2><nav id=TableOfContents><ol><li><a href=#multi-armed-bandits>Multi-armed Bandits</a><ol><li><a href=#epsilon-greedy-algorithms>$\epsilon$-greedy Algorithms</a></li><li><a href=#regret>Regret</a><ol><li><a href=#lower-bound-on-regret>Lower bound on Regret</a></li></ol></li><li><a href=#optimal-regret-algorithms>Optimal Regret Algorithms</a><ol><li><a href=#ucb-algorithm>UCB Algorithm</a></li><li><a href=#kl-ucb-algorithm>KL-UCB Algorithm</a></li><li><a href=#thompson-sampling>Thompson Sampling</a></li></ol></li><li><a href=#bound-on-ucb-regret>Bound on UCB Regret</a></li></ol></li><li><a href=#markov-decision-problems>Markov Decision Problems</a><ol><li><a href=#bellman-equations>Bellman Equations</a><ol><li><a href=#episodic-tasks>Episodic Tasks</a></li></ol></li><li><a href=#optimal-policy-characterization>Optimal Policy Characterization</a><ol><li><a href=#banachs-fixed-point-theorem>Banach&rsquo;s Fixed-point Theorem</a></li><li><a href=#bellman-optimality-operator>Bellman Optimality Operator</a></li></ol></li><li><a href=#optimal-policy-algorithms>Optimal Policy Algorithms</a><ol><li><a href=#value-iteration>Value Iteration</a></li><li><a href=#linear-programming-formulation>Linear Programming Formulation</a></li><li><a href=#policy-improvement>Policy Improvement</a></li></ol></li><li><a href=#pi-variants>PI Variants</a><ol><li><a href=#howards-pi-with-k2>Howard&rsquo;s PI with $k=2$</a></li><li><a href=#batch-switching-policy-iteration>Batch-Switching Policy Iteration</a></li></ol></li></ol></li></ol></nav></div><h2 id=multi-armed-bandits>Multi-armed Bandits</h2><dl><dt><strong>Stochastic Multi-armed Bandits</strong></dt><dd><ul><li>$A$ is the set of $n$ arms</li><li>$\forall a\in A$, $a$ has an associated Bernoulli distribution with mean reward $p_a$</li><li>Highest mean is $p^\star$</li><li>Pull any arm to gain reward and repeat this $T$ (horizon) times</li></ul></dd><dt><strong>Algorithm for maximising reward</strong></dt><dd>For $t=0,1,2,\dots,T-1$:<ul><li>Given the <em>history</em> $h^t=(a^0,r^0,a^1,r^1,\dots,a^{t-1},r^{t-1})$</li><li>Pick an <em>arm</em> $a^t$</li><li>Obtain the <em>reward</em> $r^t$</li></ul></dd><dt><strong>Deterministic Algorithm</strong></dt><dd>Set of all histories $\to A$</dd><dt><strong>Randomized Algorithm</strong></dt><dd>Set of all histories $\to \Delta A$ a.k.a. set of all probability distributions over $A$</dd></dl><h3 id=epsilon-greedy-algorithms>$\epsilon$-greedy Algorithms</h3><dl><dt><strong>$\epsilon$-G1</strong></dt><dd><ul><li>$t\le\epsilon T\implies$ sample uniformly at random</li><li>$t=\lfloor\epsilon T\rfloor\implies$ identify $a^\star$ with the highest empirical mean</li><li>$t>\epsilon T\implies$ sample $a^\star$</li></ul></dd><dt><strong>$\epsilon$-G2</strong></dt><dd><ul><li>$t\le\epsilon T\implies$ sample uniformly at random</li><li>$t>\epsilon T\implies$ sample arm with highest empirical mean</li></ul></dd><dt><strong>$\epsilon$-G3</strong></dt><dd>With probability $\epsilon$ sample uniformly at random; with probability $1-\epsilon$ sample an arm with the highest empirical mean</dd></dl><h3 id=regret>Regret</h3><blockquote><p>Consider the plot of $\mathbb{E}[r^t]$ vs $t$.
It must be bounded between $y=p_{\min}=\min_{a\in A}p_a$ and $y=p^\star$.
On random uniform sampling of arms the graph will be $y=p_{\text{avg}}=\frac{1}{n}\sum_{a\in A}p_a$.
A reasonable learning algorithm will start at $p_{\text{avg}}$ and tend towards $p^\star$</p></blockquote><dl><dt><strong>Regret</strong></dt><dd>For horizon $T$ and given algorithm,
$$
R_T = Tp^\star - \sum_{t=0}^{T-1}\mathbb{E}[r^t]
$$</dd></dl><blockquote><p><strong>Goal</strong> : Find an algorithm for which $\lim_{T\to\infty}\frac{R_T}{T}=0$.
All $\epsilon$-Gi&rsquo;s have linear regret.</p></blockquote><p>How to achieve Sub-linear Regret?</p><ol><li><strong>Infinite Exploration</strong>: In the limit, each arm must be pulled an infinite number of times.</li><li><strong>Greed in the Limit</strong>: Let $\text{exploit}(T)$ denote the number of pulls that are greedy w.r.t. the empirical mean up to the horizon $T$. We want,
$$
\lim_{T\to\infty}\frac{\mathbb{E}[\text{exploit}(T)]}{T}=1
$$</li></ol><dl><dt><strong>Special Bandit Instances</strong> ($\bar{\mathcal{I}}$)</dt><dd>Set of all bandit instances with reward means $\le 1$</dd></dl><blockquote><p><strong>Result</strong> : An algorithm $L$ achieves sub-linear regret on all instances $I\in\bar{\mathcal{I}}$ $\iff$ $L$ does infinite exploration and is greedy in the limit.</p></blockquote><h4 id=lower-bound-on-regret>Lower bound on Regret</h4><blockquote><p><strong>Lai and Robbins&rsquo; theorem</strong> : Let $L$ be an algorithm such that $\forall I\in\bar{\mathcal{I}}$ and $\forall\alpha>0$, as $T\to\infty$,
$$R_T(L,I)=\mathcal{o}(T^\alpha)$$</p><p>Then $\forall I\in\bar{\mathcal{I}}$, as $T\to\infty$:
$$\frac{R_T(L,I)}{\ln(T)}\ge\sum_{a:p_a(I)\not =p^\star(I)}\frac{p^\star(I)-p_a(I)}{KL(p_a(I),p^\star(I))}$$
where for $x,y\in[0,1), KL(x,y)=x\ln\frac{x}{y}+(1-x)\ln\frac{1-x}{1-y}$</p></blockquote><h3 id=optimal-regret-algorithms>Optimal Regret Algorithms</h3><h4 id=ucb-algorithm>UCB Algorithm</h4><ul><li>At time $t$ for every arm $a$,
$$
\text{ucb}^t_a = \hat p^t_a + \sqrt{\frac{2\ln(t)}{u^t_a}}
$$</li><li>$\hat p^t_a$ is the empirical mean of the rewards from arm $a$ and $u^t_a$ is the number of times a has been sampled at time $t$.</li><li>Pull an arm $a$ for which $\text{ucb}^t_a$ is maximum.</li></ul><blockquote><p>UCB achieves $\mathcal{O}(\log(T))$ regret.</p></blockquote><h4 id=kl-ucb-algorithm>KL-UCB Algorithm</h4><ul><li>Define, where $c\ge 3$,
$$
\text{ucb-kl}^t_a = \max\{q\in[\hat p^t_a,1]\ |\ u^t_a\cdot \text{KL}(\hat p_a^t,q)\le\ln(t)+c\ln(\ln(t))\}
$$</li><li>Pull $\argmax_{a\in A}\text{ucb-kl}^t_a$</li></ul><h4 id=thompson-sampling>Thompson Sampling</h4><ul><li>For every arm $a$ with $s_a^t$ successful pulls and $f_a^t$ failed pulls draw a sample,
$$
x_a^t\sim\text{Beta}(s_a^t+1,f_a^t+1)
$$</li><li>Pull arm $a$ with max $x_a^t$</li></ul><blockquote><p>Both KL-UCB and Thompson Sampling manage to get optimal regret.</p></blockquote><h3 id=bound-on-ucb-regret>Bound on UCB Regret</h3><h2 id=markov-decision-problems>Markov Decision Problems</h2><dl><dt><strong>Markov Decision Problem</strong> $\langle S,A,T,R,\gamma\rangle$</dt><dd><ul><li><strong>Set of states</strong>: $S=\{s_1,s_2,\dots,s_n\}$</li><li><strong>Set of actions</strong>: $A=\{a_1,a_2,\dots,a_k\}$</li><li><strong>Transition function</strong>: $T(s,a,s^\prime)$ is the probability of reaching $s^\prime$ by starting at $s$ and taking action $a$. Hence $\forall s\in S,\forall a\in A,\ \sum_{s^\prime\in S}T(s,a,s^\prime)=1$.</li><li><strong>Reward function</strong>: $T(s,a,s^\prime)$ is the reward on reaching $s^\prime$ by starting at $s$ and taking action $a$. Assuming all rewards are from $[-R_{\max},R_{\max}]$ where $R_{\max}\ge 0$.</li><li><strong>Discount factor</strong>: $\gamma$</li></ul></dd><dt><strong>Policy</strong> ($\pi$)</dt><dd>$\pi:S\to A$</dd></dl><blockquote><p>If $\Pi$ is the set of all policies then $|\Pi|=k^n$</p></blockquote><dl><dt><strong>State values for Policy</strong> ($V^\pi$)</dt><dd>For $s\in S$, $V^\pi:S\to\R$,
$$
V^\pi(s)=\mathbb{E}_\pi[r^0+\gamma r^1+\gamma^2r^2+\dots|s^0=s]
$$</dd></dl><blockquote><p>Every MDP has an optimal policy $\pi^\star$ such that,
$$
\forall\pi\in\Pi,\forall s\in S:V^{\pi^\star}(s)\ge V^\pi(s)
$$</p><p><strong>MDP Planning Problem</strong> : Find $\pi^\star$.</p></blockquote><h3 id=bellman-equations>Bellman Equations</h3><blockquote><p>For $\pi\in\Pi,s\in S$,
$$
V^\pi(s) = \sum_{s^\prime\in S}T(s,\pi(s),s^\prime)\{R(s,\pi(s),s^\prime)+\gamma V^\pi(s^\prime)\}
$$</p><ul><li>$n$ equations $n$ variables</li><li>linear</li><li>Guaranteed solution for $\gamma&lt;1$</li></ul></blockquote><p>Therefore brute-force way of finding the state values for all $k^n$ possible policies and then picking the most dominant one is possible but has $\mathcal{O}(\text{poly}(n,k)\cdot k^n)$ time complexity.</p><h4 id=episodic-tasks>Episodic Tasks</h4><dl><dt><strong>Episodic Task</strong></dt><dd><ul><li>Has a new terminal state $s_\top$ from which there are no out going transitions on rewards.</li><li>For every non-terminal state and every policy there is a non-zero probability of reaching the terminal state in a finite number of steps.</li></ul></dd></dl><h5 id=definition-of-values>Definition of values</h5><dl><dt><strong>Infinite discounted reward</strong></dt><dd>$V^\pi(s)=\mathbb{E}_\pi[r^0+\gamma r+\gamma^2r^2+\dots|s^0=s]$</dd><dt><strong>Total reward</strong></dt><dd>$V^\pi(s)=\mathbb{E}_\pi[r^0+r^1+\dots+|s^0=s]$</dd><dt><strong>Finite horizon reward</strong></dt><dd>$V^\pi(s)=\mathbb{E}_\pi[r^0+r^1+\cdots+r^{H-1}|s^0=s]$ where $H\ge 1$</dd><dt><strong>Average reward</strong></dt><dd>$V^\pi(s)=\mathbb{E_{\pi}}[\lim_{m\to\infty}\frac{r^0+r^1+\cdots+R^{m-1}}{m}|s^0=s]$</dd></dl><h3 id=optimal-policy-characterization>Optimal Policy Characterization</h3><h4 id=banachs-fixed-point-theorem>Banach&rsquo;s Fixed-point Theorem</h4><dl><dt><strong>Banach Space</strong> ($X$)</dt><dd>A complete, normed vector space<ul><li><strong>Vector space</strong>: $X$</li><li><strong>Norm</strong>: $\|\cdot\|$</li><li><strong>Complete</strong>: $(X,\|\cdot\|)$ such that every Cauchy sequence has a limit in $X$</li></ul></dd><dt><strong>Contraction mapping</strong> ($Z,l$)</dt><dd>$Z:X\to X$ with contraction factor $0\le l&lt;1$ such that, $\forall u\in X,\forall v\in X,$
$$
\|Zv-Zu\|\le l\|v-u\|
$$</dd><dt><strong>Fixed-point</strong> ($x^\star$)</dt><dd>For $Z$, such that $Zx^\star=x^\star$</dd></dl><blockquote><p><strong>Banach&rsquo;s Fixed-point Theorem</strong> : For a contraction map $Z$ with contraction factor $l$ in a Banach space $(X,\|\cdot\|)$,</p><ol><li>$Z$ has a unique fixed point $x^\star\in X$</li><li>For $x\in X,m\le 0:\|Z^mx-x^\star\|\le l^m\|x-x^\star\|$</li></ol></blockquote><h4 id=bellman-optimality-operator>Bellman Optimality Operator</h4><dl><dt><strong>Bellman optimality operator</strong> ($B^\star$)</dt><dd>$B^\star:\R^n\to\R^n$ for an MDP is defined for $F\in\R^n,s\in S$ as,
$$
(B^\star(F))(s) = \max_{a\in A}\sum_{s^\prime\in S}T(s,a,s^\prime)\{R(s,a,s^\prime)+\gamma F(s^\prime)\}
$$</dd><dt><strong>Max norm</strong> ($\|\cdot\|_\infty$)</dt><dd>For $F=(f_1,f_2,\dots,f_n)\in\R^n$,
$$
\|F\|_\infty=\max\{|f_1|,|f_2|,\dots,|f_n|\}
$$</dd></dl><blockquote><p><strong>Result</strong> : $(\R^n,\|\cdot\|_\infty)$ is a Banach space.</p><p>$\therefore B^\star$ is a contraction map in $(\R^n,\|\cdot\|_\infty)$ with contraction factor $\gamma$.</p></blockquote><dl><dt><strong>Optimal Value Function</strong> $(V^\star)$</dt><dd>Denote the fixed point $V^\star:S\to\R$ (alternatively, $V^\star\in\R^n$) such that $B^\star(V^\star)=V^\star$. For $s\in S$,
$$
V^\star(s)=\max_{a\in A}\sum_{s^\prime\in S}T(s,a,s^\prime)\{R(s,a,s^\prime)+\gamma V^\star(s^\prime)\}
$$</dd></dl><h3 id=optimal-policy-algorithms>Optimal Policy Algorithms</h3><h4 id=value-iteration>Value Iteration</h4><ol><li>$V_0\leftarrow$ Arbitrary, element-wise bounded, $n$-length vector.</li><li>$t\leftarrow 0$</li><li>Repeat:<ol><li>For $s\in S$:<ol><li>$V_{t+1}(s)\leftarrow\max_{a\in A}\sum_{s^\prime\in S}T(s,a,s^\prime)(R(s,a,s^\prime+\gamma V_t(s^\prime)))$</li></ol></li><li>$t\leftarrow t+1$</li></ol></li><li>Until $V_t\equiv V_{t-1}$</li></ol><h4 id=linear-programming-formulation>Linear Programming Formulation</h4><dl><dt><strong>Vector Comparison</strong> ($\succeq,\succ$)</dt><dd><ul><li>For $X:S\to\R$ and $Y:S\to\R$ (equivalently $X,Y\in\R^n$) we define,
$$
X\succeq Y\iff \forall s\in S:X(s)\ge Y(s)\\
X\succ Y\iff X\succeq Y\And\exist s\in S:X(s)>Y(s)
$$</li><li>For policies $\pi_1,\pi_2\in\Pi$ we define,
$$
\pi_1\succeq\pi_2\iff V^{\pi_1}\succeq V^{\pi_2}\\
\pi_1\succ\pi_2\iff V^{\pi_1}\succ V^{\pi_2}\\
$$</li><li>2 policies can also be <em>incomparable</em> i.e. $\pi_1\not\succeq\pi_2$ and $\pi_2\not\succeq\pi_1$</li><li>$\pi_1\succeq\pi_2$ and $\pi_2\succeq\pi_1\iff V^{\pi_1}=V^{\pi_2}$</li></ul></dd></dl><blockquote><p><strong>Result</strong> : $B^\star$ preserves $\succeq$. $\forall X,Y:S\to\R^n$,
$$
X\succeq Y\implies B^\star(X)\succeq B^\star(Y)
$$</p><p>$\therefore$ For all $V\not =V^\star$ in the feasible set, $V\succ V^\star$.
$$
\implies\sum_{s\in S}V(s)>\sum_{s\in S}V^\star(s)
$$</p></blockquote><dl><dt><strong>Linear Programming Formulation</strong></dt><dd><ul><li>Maximise $\left(-\sum_{s\in S}V(s)\right)$,</li><li>Subject to $V(s)\ge\sum_{s^\prime\in S}T(s,a,s^\prime)\{R(s,a,s^\prime)+\gamma V(s^\prime)\},\ \forall s\in S,a\in A$.</li></ul></dd></dl><blockquote><p>This LP has $n$ variables and $nk$ constraints and the solution is $V^\star$.
The dual of this LP has nk variables with $n$ constraints and it&rsquo;s solution is $\pi^\star$.</p></blockquote><h4 id=policy-improvement>Policy Improvement</h4><dl><dt><strong>Action Value Function</strong> ($Q^\pi:S\times A\to\R$)</dt><dd>For $\pi\in\Pi,s\in S,a\in A$,
$$Q^\pi(s,a)=\mathbb{E}[r^0+\gamma r^1+\gamma^2r^2+\dots|s^0=s;a^0=a;a^t=\pi(s^t),\ \forall t\ge1$$</dd></dl><blockquote><p>For $s\in S,a\in A$,
$$
Q^\pi(s,a) = \sum_{s^\prime\in S}T(s,a,s^\prime)\{R(s,a,s^\prime)+\gamma V^\pi(s^\prime)\}
$$</p><p>$Q^\pi(s,\pi(s))=V^\pi(s)$</p><p>All optimal policies have the same optimal action value function $Q^\star$.</p></blockquote><dl><dt>$\text{IA}:\Pi\times S\to \mathcal{P}(A)$</dt><dd>For $\pi\in\Pi,s\in S$,
$$\text{IA}(\pi,s)=\{a\in A:Q^\pi(s,a)>V^\pi(s)\}$$</dd><dt>$\text{IS}:\Pi\to\mathcal{P}(S)$</dt><dd>For $\pi\in\Pi$,
$$\text{IS}(\pi)=\{s\in S:|\text{IA}(\pi,s)|\ge 1\}$$</dd></dl><blockquote><p><strong>Policy Improvement Theorem</strong> :</p><ol><li>If $\text{IS}(\pi)=\emptyset$ then $\pi$ is an optimal policy, else</li><li>if $\pi^\prime$ is obtained by policy improvement on $\pi$, then $\pi^\prime\succ\pi$.</li></ol><p>$\text{IS}(\pi^\star)=\emptyset\iff B^\star(V^{\pi^\star})=V^{\pi^\star}$</p></blockquote><dl><dt><strong>Bellman Operator</strong> ($B^\pi:\R^n\to\R^n$)</dt><dd>For $\pi\in\Pi,X:S\to\R,s\in S$,
$$(B^\pi(X))(s)=\sum_{s^\prime\in S}T(s,\pi(s),s^\prime)(R(s,\pi(s),s^\prime)+\gamma X(s^\prime))$$</dd></dl><blockquote><p>$B^\pi$ is a contraction mapping with contraction factor $\gamma$.</p><p>For $X:s\to\R,\ \lim_{l\to\infty}(B^\pi)^l(X)=V^\pi$
For $X:s\to\R,Y:S\to\R,\ X\succeq Y\implies B^\pi(X)\succeq B^\pi(Y)$</p><p>$B^{\pi^\prime}(V^\pi)(s)=Q^\pi(s,\pi^\prime(s))$</p></blockquote><h5 id=policy-iteration-algorithm>Policy Iteration Algorithm</h5><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=ln>1</span><span class=cl><span class=k>def</span> <span class=nf>Policy_iteration</span><span class=p>(</span><span class=n>mdp</span><span class=p>):</span>
</span></span><span class=line><span class=ln>2</span><span class=cl>  <span class=n>Pi</span> <span class=o>=</span> <span class=n>mdp</span><span class=o>.</span><span class=n>rand_policy</span><span class=p>()</span>
</span></span><span class=line><span class=ln>3</span><span class=cl>  <span class=k>while</span> <span class=n>mdp</span><span class=o>.</span><span class=n>is_improvable</span><span class=p>(</span><span class=n>Pi</span><span class=p>):</span>
</span></span><span class=line><span class=ln>4</span><span class=cl>    <span class=n>Pi</span> <span class=o>=</span> <span class=n>mdp</span><span class=o>.</span><span class=n>improve</span><span class=p>(</span><span class=n>Pi</span><span class=p>)</span>
</span></span><span class=line><span class=ln>5</span><span class=cl>  <span class=k>return</span> <span class=n>Pi</span>
</span></span></code></pre></div><h3 id=pi-variants>PI Variants</h3><dl><dt><strong>Howard&rsquo;s Policy Iteration</strong></dt><dd>Greedy; switch all improvable states.</dd><dt><strong>Random Policy Iteration</strong></dt><dd>Switch a non-empty subset of improvable states chosen uniformly at random.</dd><dt><strong>Simple Policy Iteration</strong></dt><dd>Assume a fixed indexing of states and always improve the state with the highest index.</dd><dt><strong>Upper Bound</strong> ($U(n,k)$)</dt><dd>For a set of PI Variants $\mathcal{L}$ and MDP $M$, the expected number of policy evalutaions performed by $L$ on $M$ if initialised at any $\pi$ is at most $U(n,k)$.</dd></dl><blockquote><table><thead><tr><th>PI Variant</th><th>Type</th><th>k=2</th><th>General k</th></tr></thead><tbody><tr><td>Howard&rsquo;s PI</td><td>Deterministic</td><td>$\mathcal{O}(\frac{2^n}{n})$</td><td>$\mathcal{O}(\frac{k^n}{n})$</td></tr><tr><td>Mansour and Singh&rsquo;s Random PI [MS99]</td><td>Randomized</td><td>$1.7172^n$</td><td>$\mathcal{O}(\frac{k}{2})^n$</td></tr><tr><td>Mansour and Singh&rsquo;s Random PI [HPZ14]</td><td>Randomized</td><td>$\text{poly}(n)\cdot 1.5^n$</td><td>&ndash;</td></tr></tbody></table></blockquote><dl><dt><strong>Lower Bound</strong> ($X(n,k)$)</dt><dd>For a set of PI Variants $\mathcal{L}$ and MDP $M$, the expected number of policy evalutaions performed by $L$ on $M$ if initialised at any $\pi$ is at least $X(n,k)$.</dd></dl><blockquote><ul><li>Howard&rsquo;s PI on $n$-state, 2-action MDPs : $\Omega(n)$</li><li>Simple PI on $n$-state, 2-action MDPs : $\Omega(2^n)$</li></ul></blockquote><h4 id=howards-pi-with-k2>Howard&rsquo;s PI with $k=2$</h4><blockquote><p>Non-optimal policies $\pi,\pi^\prime\in\Pi$ cannot have the same set of improvable states.</p><p>If $\pi$ has $m$ improvable states and $\pi$ states and $\pi,\pi^\prime$ (Howard&rsquo;s PI) then there exist $m$ policies $\pi^{\prime\prime}$ such that $\pi^\prime\succeq\pi^{\prime\prime}\succ\pi$.</p></blockquote><blockquote><p>Number of iterations taken by Howard&rsquo;s PI: $O(\frac{2^n}{n})$</p></blockquote><h4 id=batch-switching-policy-iteration>Batch-Switching Policy Iteration</h4><blockquote><p>Howard&rsquo;s Policy Iteration takes at most 3 iterations on 2-state 2-action MDP!</p></blockquote><dl><dt><strong>BSPI</strong></dt><dd>Partition states in 2-sized batches arranged from right to left. Improve the rightmost set containing an improvable state.</dd></dl><blockquote><p>BSPI of batch size 2 is bounded by $\mathcal{O}(\sqrt{3}^n)$.
Tighter bounds for higher batch-sizes.</p></blockquote></article></main><footer id=footer><div><span>¬© 2022</span> - <span>2023</span></div><div><span>Powered by </span><a class=link href=https://gohugo.io/>Hugo</a>
<span>üç¶ Theme </span><a class=link href=https://github.com/queensferryme/hugo-theme-texify>TeXify</a></div><div class=footnote><span>Follow me on <a class=link href=https://github.com/adityakadoo>GitHub</a> |
<a class=link href=https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh target=_blank rel=noopener>CC BY-NC-SA 4.0</a></span></div></footer></div><link media=screen rel=stylesheet href=https://adityakadoo.github.io/Scrolls/css/syntax.css></body></html>