<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Machine-Learning on Scrolls</title><link>https://adityakadoo.github.io/Scrolls/tags/machine-learning/</link><description>Recent content in Machine-Learning on Scrolls</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Wed, 24 Aug 2022 09:49:01 +0530</lastBuildDate><atom:link href="https://adityakadoo.github.io/Scrolls/tags/machine-learning/index.xml" rel="self" type="application/rss+xml"/><item><title>Natural Language Processing</title><link>https://adityakadoo.github.io/Scrolls/courses/natural_language_processing/</link><pubDate>Wed, 24 Aug 2022 09:49:01 +0530</pubDate><guid>https://adityakadoo.github.io/Scrolls/courses/natural_language_processing/</guid><description>Part of Speech Tagging Viterbi Algorithm Parameters Input: A sequence of words and labels Output: A sequence of labels for every word Penn tag-set is generally used for POS tagging in english language.
Hidden Markov Model There are 2 kinds of probabilities:
Bigram Probabilities $(P(t_1|t_0))$ : Probability of current word being tag $t_1$ when previous word was tagged $t_0$. Lexical Probabilities $(P(w|t))$: Probability of word $w$ given it is tagged $t$.</description></item></channel></rss>