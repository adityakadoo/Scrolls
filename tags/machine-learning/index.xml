<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Machine-Learning on Scrolls</title><link>https://adityakadoo.github.io/Scrolls/tags/machine-learning/</link><description>Recent content in Machine-Learning on Scrolls</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><managingEditor>aditya1449kadoo@gmail.com (Aditya Kadoo)</managingEditor><webMaster>aditya1449kadoo@gmail.com (Aditya Kadoo)</webMaster><lastBuildDate>Wed, 13 Sep 2023 14:47:25 +0530</lastBuildDate><atom:link href="https://adityakadoo.github.io/Scrolls/tags/machine-learning/index.xml" rel="self" type="application/rss+xml"/><item><title>Reinforcement Learning</title><link>https://adityakadoo.github.io/Scrolls/courses/reinforcement_learning/</link><pubDate>Wed, 13 Sep 2023 14:47:25 +0530</pubDate><author>aditya1449kadoo@gmail.com (Aditya Kadoo)</author><guid>https://adityakadoo.github.io/Scrolls/courses/reinforcement_learning/</guid><description>&lt;h2 id="multi-armed-bandits">Multi-armed Bandits&lt;/h2>
&lt;dl>
&lt;dt>&lt;strong>Stochastic Multi-armed Bandits&lt;/strong>&lt;/dt>
&lt;dd>&lt;ul>
&lt;li>$A$ is the set of $n$ arms&lt;/li>
&lt;li>$\forall a\in A$, $a$ has an associated Bernoulli distribution with mean reward $p_a$&lt;/li>
&lt;li>Highest mean is $p^\star$&lt;/li>
&lt;li>Pull any arm to gain reward and repeat this $T$ (horizon) times&lt;/li>
&lt;/ul>
&lt;/dd>
&lt;dt>&lt;strong>Algorithm for maximising reward&lt;/strong>&lt;/dt>
&lt;dd>For $t=0,1,2,\dots,T-1$:
&lt;ul>
&lt;li>Given the &lt;em>history&lt;/em> $h^t=(a^0,r^0,a^1,r^1,\dots,a^{t-1},r^{t-1})$&lt;/li>
&lt;li>Pick an &lt;em>arm&lt;/em> $a^t$&lt;/li>
&lt;li>Obtain the &lt;em>reward&lt;/em> $r^t$&lt;/li>
&lt;/ul>
&lt;/dd>
&lt;dt>&lt;strong>Deterministic Algorithm&lt;/strong>&lt;/dt>
&lt;dd>Set of all histories $\to A$&lt;/dd>
&lt;dt>&lt;strong>Randomized Algorithm&lt;/strong>&lt;/dt>
&lt;dd>Set of all histories $\to \Delta A$ a.k.a. set of all probability distributions over $A$&lt;/dd>
&lt;/dl>
&lt;h3 id="epsilon-greedy-algorithms">$\epsilon$-greedy Algorithms&lt;/h3>
&lt;dl>
&lt;dt>&lt;strong>$\epsilon$-G1&lt;/strong>&lt;/dt>
&lt;dd>&lt;ul>
&lt;li>$t\le\epsilon T\implies$ sample uniformly at random&lt;/li>
&lt;li>$t=\lfloor\epsilon T\rfloor\implies$ identify $a^\star$ with the highest empirical mean&lt;/li>
&lt;li>$t&amp;gt;\epsilon T\implies$ sample $a^\star$&lt;/li>
&lt;/ul>
&lt;/dd>
&lt;dt>&lt;strong>$\epsilon$-G2&lt;/strong>&lt;/dt>
&lt;dd>&lt;ul>
&lt;li>$t\le\epsilon T\implies$ sample uniformly at random&lt;/li>
&lt;li>$t&amp;gt;\epsilon T\implies$ sample arm with highest empirical mean&lt;/li>
&lt;/ul>
&lt;/dd>
&lt;dt>&lt;strong>$\epsilon$-G3&lt;/strong>&lt;/dt>
&lt;dd>With probability $\epsilon$ sample uniformly at random; with probability $1-\epsilon$ sample an arm with the highest empirical mean&lt;/dd>
&lt;/dl>
&lt;h3 id="regret">Regret&lt;/h3>
&lt;blockquote>
&lt;p>Consider the plot of $\mathbb{E}[r^t]$ vs $t$.
It must be bounded between $y=p_{\min}=\min_{a\in A}p_a$ and $y=p^\star$.
On random uniform sampling of arms the graph will be $y=p_{\text{avg}}=\frac{1}{n}\sum_{a\in A}p_a$.
A reasonable learning algorithm will start at $p_{\text{avg}}$ and tend towards $p^\star$&lt;/p>
&lt;/blockquote>
&lt;dl>
&lt;dt>&lt;strong>Regret&lt;/strong>&lt;/dt>
&lt;dd>For horizon $T$ and given algorithm,
$$
R_T = Tp^\star - \sum_{t=0}^{T-1}\mathbb{E}[r^t]
$$&lt;/dd>
&lt;/dl>
&lt;blockquote>
&lt;p>&lt;strong>Goal&lt;/strong> : Find an algorithm for which $\lim_{T\to\infty}\frac{R_T}{T}=0$.
All $\epsilon$-Gi&amp;rsquo;s have linear regret.&lt;/p>
&lt;/blockquote>
&lt;p>How to achieve Sub-linear Regret?&lt;/p>
&lt;ol>
&lt;li>&lt;strong>Infinite Exploration&lt;/strong>: In the limit, each arm must be pulled an infinite number of times.&lt;/li>
&lt;li>&lt;strong>Greed in the Limit&lt;/strong>: Let $\text{exploit}(T)$ denote the number of pulls that are greedy w.r.t. the empirical mean up to the horizon $T$. We want,
$$
\lim_{T\to\infty}\frac{\mathbb{E}[\text{exploit}(T)]}{T}=1
$$&lt;/li>
&lt;/ol>
&lt;dl>
&lt;dt>&lt;strong>Special Bandit Instances&lt;/strong> ($\bar{\mathcal{I}}$)&lt;/dt>
&lt;dd>Set of all bandit instances with reward means $\le 1$&lt;/dd>
&lt;/dl>
&lt;blockquote>
&lt;p>&lt;strong>Result&lt;/strong> : An algorithm $L$ achieves sub-linear regret on all instances $I\in\bar{\mathcal{I}}$ $\iff$ $L$ does infinite exploration and is greedy in the limit.&lt;/p>
&lt;/blockquote>
&lt;h4 id="lower-bound-on-regret">Lower bound on Regret&lt;/h4>
&lt;blockquote>
&lt;p>&lt;strong>Lai and Robbins&amp;rsquo; theorem&lt;/strong> : Let $L$ be an algorithm such that $\forall I\in\bar{\mathcal{I}}$ and $\forall\alpha&amp;gt;0$, as $T\to\infty$,
$$R_T(L,I)=\mathcal{o}(T^\alpha)$$&lt;/p>
&lt;p>Then $\forall I\in\bar{\mathcal{I}}$, as $T\to\infty$:
$$\frac{R_T(L,I)}{\ln(T)}\ge\sum_{a:p_a(I)\not =p^\star(I)}\frac{p^\star(I)-p_a(I)}{KL(p_a(I),p^\star(I))}$$
where for $x,y\in[0,1), KL(x,y)=x\ln\frac{x}{y}+(1-x)\ln\frac{1-x}{1-y}$&lt;/p>
&lt;/blockquote>
&lt;h3 id="optimal-regret-algorithms">Optimal Regret Algorithms&lt;/h3>
&lt;h4 id="ucb-algorithm">UCB Algorithm&lt;/h4>
&lt;ul>
&lt;li>At time $t$ for every arm $a$,
$$
\text{ucb}^t_a = \hat p^t_a + \sqrt{\frac{2\ln(t)}{u^t_a}}
$$&lt;/li>
&lt;li>$\hat p^t_a$ is the empirical mean of the rewards from arm $a$ and $u^t_a$ is the number of times a has been sampled at time $t$.&lt;/li>
&lt;li>Pull an arm $a$ for which $\text{ucb}^t_a$ is maximum.&lt;/li>
&lt;/ul>
&lt;blockquote>
&lt;p>UCB achieves $\mathcal{O}(\log(T))$ regret.&lt;/p>
&lt;/blockquote>
&lt;h4 id="kl-ucb-algorithm">KL-UCB Algorithm&lt;/h4>
&lt;ul>
&lt;li>Define, where $c\ge 3$,
$$
\text{ucb-kl}^t_a = \max\{q\in[\hat p^t_a,1]\ |\ u^t_a\cdot \text{KL}(\hat p_a^t,q)\le\ln(t)+c\ln(\ln(t))\}
$$&lt;/li>
&lt;li>Pull $\argmax_{a\in A}\text{ucb-kl}^t_a$&lt;/li>
&lt;/ul>
&lt;h4 id="thompson-sampling">Thompson Sampling&lt;/h4>
&lt;ul>
&lt;li>For every arm $a$ with $s_a^t$ successful pulls and $f_a^t$ failed pulls draw a sample,
$$
x_a^t\sim\text{Beta}(s_a^t+1,f_a^t+1)
$$&lt;/li>
&lt;li>Pull arm $a$ with max $x_a^t$&lt;/li>
&lt;/ul>
&lt;blockquote>
&lt;p>Both KL-UCB and Thompson Sampling manage to get optimal regret.&lt;/p>
&lt;/blockquote>
&lt;h3 id="bound-on-ucb-regret">Bound on UCB Regret&lt;/h3>
&lt;h2 id="markov-decision-problems">Markov Decision Problems&lt;/h2>
&lt;dl>
&lt;dt>&lt;strong>Markov Decision Problem&lt;/strong> $\langle S,A,T,R,\gamma\rangle$&lt;/dt>
&lt;dd>&lt;ul>
&lt;li>&lt;strong>Set of states&lt;/strong>: $S=\{s_1,s_2,\dots,s_n\}$&lt;/li>
&lt;li>&lt;strong>Set of actions&lt;/strong>: $A=\{a_1,a_2,\dots,a_k\}$&lt;/li>
&lt;li>&lt;strong>Transition function&lt;/strong>: $T(s,a,s^\prime)$ is the probability of reaching $s^\prime$ by starting at $s$ and taking action $a$. Hence $\forall s\in S,\forall a\in A,\ \sum_{s^\prime\in S}T(s,a,s^\prime)=1$.&lt;/li>
&lt;li>&lt;strong>Reward function&lt;/strong>: $T(s,a,s^\prime)$ is the reward on reaching $s^\prime$ by starting at $s$ and taking action $a$. Assuming all rewards are from $[-R_{\max},R_{\max}]$ where $R_{\max}\ge 0$.&lt;/li>
&lt;li>&lt;strong>Discount factor&lt;/strong>: $\gamma$&lt;/li>
&lt;/ul>
&lt;/dd>
&lt;dt>&lt;strong>Policy&lt;/strong> ($\pi$)&lt;/dt>
&lt;dd>$\pi:S\to A$&lt;/dd>
&lt;/dl>
&lt;blockquote>
&lt;p>If $\Pi$ is the set of all policies then $|\Pi|=k^n$&lt;/p>
&lt;/blockquote>
&lt;dl>
&lt;dt>&lt;strong>State values for Policy&lt;/strong> ($V^\pi$)&lt;/dt>
&lt;dd>For $s\in S$, $V^\pi:S\to\R$,
$$
V^\pi(s)=\mathbb{E}_\pi[r^0+\gamma r^1+\gamma^2r^2+\dots|s^0=s]
$$&lt;/dd>
&lt;/dl>
&lt;blockquote>
&lt;p>Every MDP has an optimal policy $\pi^\star$ such that,
$$
\forall\pi\in\Pi,\forall s\in S:V^{\pi^\star}(s)\ge V^\pi(s)
$$&lt;/p>
&lt;p>&lt;strong>MDP Planning Problem&lt;/strong> : Find $\pi^\star$.&lt;/p>
&lt;/blockquote>
&lt;h3 id="bellman-equations">Bellman Equations&lt;/h3>
&lt;blockquote>
&lt;p>For $\pi\in\Pi,s\in S$,
$$
V^\pi(s) = \sum_{s^\prime\in S}T(s,\pi(s),s^\prime)\{R(s,\pi(s),s^\prime)+\gamma V^\pi(s^\prime)\}
$$&lt;/p>
&lt;ul>
&lt;li>$n$ equations $n$ variables&lt;/li>
&lt;li>linear&lt;/li>
&lt;li>Guaranteed solution for $\gamma&amp;lt;1$&lt;/li>
&lt;/ul>
&lt;/blockquote>
&lt;p>Therefore brute-force way of finding the state values for all $k^n$ possible policies and then picking the most dominant one is possible but has $\mathcal{O}(\text{poly}(n,k)\cdot k^n)$ time complexity.&lt;/p>
&lt;h4 id="episodic-tasks">Episodic Tasks&lt;/h4>
&lt;dl>
&lt;dt>&lt;strong>Episodic Task&lt;/strong>&lt;/dt>
&lt;dd>&lt;ul>
&lt;li>Has a new terminal state $s_\top$ from which there are no out going transitions on rewards.&lt;/li>
&lt;li>For every non-terminal state and every policy there is a non-zero probability of reaching the terminal state in a finite number of steps.&lt;/li>
&lt;/ul>
&lt;/dd>
&lt;/dl>
&lt;h5 id="definition-of-values">Definition of values&lt;/h5>
&lt;dl>
&lt;dt>&lt;strong>Infinite discounted reward&lt;/strong>&lt;/dt>
&lt;dd>$V^\pi(s)=\mathbb{E}_\pi[r^0+\gamma r+\gamma^2r^2+\dots|s^0=s]$&lt;/dd>
&lt;dt>&lt;strong>Total reward&lt;/strong>&lt;/dt>
&lt;dd>$V^\pi(s)=\mathbb{E}_\pi[r^0+r^1+\dots+|s^0=s]$&lt;/dd>
&lt;dt>&lt;strong>Finite horizon reward&lt;/strong>&lt;/dt>
&lt;dd>$V^\pi(s)=\mathbb{E}_\pi[r^0+r^1+\cdots+r^{H-1}|s^0=s]$ where $H\ge 1$&lt;/dd>
&lt;dt>&lt;strong>Average reward&lt;/strong>&lt;/dt>
&lt;dd>$V^\pi(s)=\mathbb{E_{\pi}}[\lim_{m\to\infty}\frac{r^0+r^1+\cdots+R^{m-1}}{m}|s^0=s]$&lt;/dd>
&lt;/dl>
&lt;h3 id="optimal-policy-characterization">Optimal Policy Characterization&lt;/h3>
&lt;h4 id="banachs-fixed-point-theorem">Banach&amp;rsquo;s Fixed-point Theorem&lt;/h4>
&lt;dl>
&lt;dt>&lt;strong>Banach Space&lt;/strong> ($X$)&lt;/dt>
&lt;dd>A complete, normed vector space
&lt;ul>
&lt;li>&lt;strong>Vector space&lt;/strong>: $X$&lt;/li>
&lt;li>&lt;strong>Norm&lt;/strong>: $\|\cdot\|$&lt;/li>
&lt;li>&lt;strong>Complete&lt;/strong>: $(X,\|\cdot\|)$ such that every Cauchy sequence has a limit in $X$&lt;/li>
&lt;/ul>
&lt;/dd>
&lt;dt>&lt;strong>Contraction mapping&lt;/strong> ($Z,l$)&lt;/dt>
&lt;dd>$Z:X\to X$ with contraction factor $0\le l&amp;lt;1$ such that, $\forall u\in X,\forall v\in X,$
$$
\|Zv-Zu\|\le l\|v-u\|
$$&lt;/dd>
&lt;dt>&lt;strong>Fixed-point&lt;/strong> ($x^\star$)&lt;/dt>
&lt;dd>For $Z$, such that $Zx^\star=x^\star$&lt;/dd>
&lt;/dl>
&lt;blockquote>
&lt;p>&lt;strong>Banach&amp;rsquo;s Fixed-point Theorem&lt;/strong> : For a contraction map $Z$ with contraction factor $l$ in a Banach space $(X,\|\cdot\|)$,&lt;/p>
&lt;ol>
&lt;li>$Z$ has a unique fixed point $x^\star\in X$&lt;/li>
&lt;li>For $x\in X,m\le 0:\|Z^mx-x^\star\|\le l^m\|x-x^\star\|$&lt;/li>
&lt;/ol>
&lt;/blockquote>
&lt;h4 id="bellman-optimality-operator">Bellman Optimality Operator&lt;/h4>
&lt;dl>
&lt;dt>&lt;strong>Bellman optimality operator&lt;/strong> ($B^\star$)&lt;/dt>
&lt;dd>$B^\star:\R^n\to\R^n$ for an MDP is defined for $F\in\R^n,s\in S$ as,
$$
(B^\star(F))(s) = \max_{a\in A}\sum_{s^\prime\in S}T(s,a,s^\prime)\{R(s,a,s^\prime)+\gamma F(s^\prime)\}
$$&lt;/dd>
&lt;dt>&lt;strong>Max norm&lt;/strong> ($\|\cdot\|_\infty$)&lt;/dt>
&lt;dd>For $F=(f_1,f_2,\dots,f_n)\in\R^n$,
$$
\|F\|_\infty=\max\{|f_1|,|f_2|,\dots,|f_n|\}
$$&lt;/dd>
&lt;/dl>
&lt;blockquote>
&lt;p>&lt;strong>Result&lt;/strong> : $(\R^n,\|\cdot\|_\infty)$ is a Banach space.&lt;/p>
&lt;p>$\therefore B^\star$ is a contraction map in $(\R^n,\|\cdot\|_\infty)$ with contraction factor $\gamma$.&lt;/p>
&lt;/blockquote>
&lt;dl>
&lt;dt>&lt;strong>Optimal Value Function&lt;/strong> $(V^\star)$&lt;/dt>
&lt;dd>Denote the fixed point $V^\star:S\to\R$ (alternatively, $V^\star\in\R^n$) such that $B^\star(V^\star)=V^\star$. For $s\in S$,
$$
V^\star(s)=\max_{a\in A}\sum_{s^\prime\in S}T(s,a,s^\prime)\{R(s,a,s^\prime)+\gamma V^\star(s^\prime)\}
$$&lt;/dd>
&lt;/dl>
&lt;h3 id="optimal-policy-algorithms">Optimal Policy Algorithms&lt;/h3>
&lt;h4 id="value-iteration">Value Iteration&lt;/h4>
&lt;ol>
&lt;li>$V_0\leftarrow$ Arbitrary, element-wise bounded, $n$-length vector.&lt;/li>
&lt;li>$t\leftarrow 0$&lt;/li>
&lt;li>Repeat:
&lt;ol>
&lt;li>For $s\in S$:
&lt;ol>
&lt;li>$V_{t+1}(s)\leftarrow\max_{a\in A}\sum_{s^\prime\in S}T(s,a,s^\prime)(R(s,a,s^\prime+\gamma V_t(s^\prime)))$&lt;/li>
&lt;/ol>
&lt;/li>
&lt;li>$t\leftarrow t+1$&lt;/li>
&lt;/ol>
&lt;/li>
&lt;li>Until $V_t\equiv V_{t-1}$&lt;/li>
&lt;/ol>
&lt;h4 id="linear-programming-formulation">Linear Programming Formulation&lt;/h4>
&lt;dl>
&lt;dt>&lt;strong>Vector Comparison&lt;/strong> ($\succeq,\succ$)&lt;/dt>
&lt;dd>&lt;ul>
&lt;li>For $X:S\to\R$ and $Y:S\to\R$ (equivalently $X,Y\in\R^n$) we define,
$$
X\succeq Y\iff \forall s\in S:X(s)\ge Y(s)\\
X\succ Y\iff X\succeq Y\And\exist s\in S:X(s)&amp;gt;Y(s)
$$&lt;/li>
&lt;li>For policies $\pi_1,\pi_2\in\Pi$ we define,
$$
\pi_1\succeq\pi_2\iff V^{\pi_1}\succeq V^{\pi_2}\\
\pi_1\succ\pi_2\iff V^{\pi_1}\succ V^{\pi_2}\\
$$&lt;/li>
&lt;li>2 policies can also be &lt;em>incomparable&lt;/em> i.e. $\pi_1\not\succeq\pi_2$ and $\pi_2\not\succeq\pi_1$&lt;/li>
&lt;li>$\pi_1\succeq\pi_2$ and $\pi_2\succeq\pi_1\iff V^{\pi_1}=V^{\pi_2}$&lt;/li>
&lt;/ul>
&lt;/dd>
&lt;/dl>
&lt;blockquote>
&lt;p>&lt;strong>Result&lt;/strong> : $B^\star$ preserves $\succeq$. $\forall X,Y:S\to\R^n$,
$$
X\succeq Y\implies B^\star(X)\succeq B^\star(Y)
$$&lt;/p>
&lt;p>$\therefore$ For all $V\not =V^\star$ in the feasible set, $V\succ V^\star$.
$$
\implies\sum_{s\in S}V(s)&amp;gt;\sum_{s\in S}V^\star(s)
$$&lt;/p>
&lt;/blockquote>
&lt;dl>
&lt;dt>&lt;strong>Linear Programming Formulation&lt;/strong>&lt;/dt>
&lt;dd>&lt;ul>
&lt;li>Maximise $\left(-\sum_{s\in S}V(s)\right)$,&lt;/li>
&lt;li>Subject to $V(s)\ge\sum_{s^\prime\in S}T(s,a,s^\prime)\{R(s,a,s^\prime)+\gamma V(s^\prime)\},\ \forall s\in S,a\in A$.&lt;/li>
&lt;/ul>
&lt;/dd>
&lt;/dl>
&lt;blockquote>
&lt;p>This LP has $n$ variables and $nk$ constraints and the solution is $V^\star$.
The dual of this LP has nk variables with $n$ constraints and it&amp;rsquo;s solution is $\pi^\star$.&lt;/p>
&lt;/blockquote>
&lt;h4 id="policy-improvement">Policy Improvement&lt;/h4>
&lt;dl>
&lt;dt>&lt;strong>Action Value Function&lt;/strong> ($Q^\pi:S\times A\to\R$)&lt;/dt>
&lt;dd>For $\pi\in\Pi,s\in S,a\in A$,
$$Q^\pi(s,a)=\mathbb{E}[r^0+\gamma r^1+\gamma^2r^2+\dots|s^0=s;a^0=a;a^t=\pi(s^t),\ \forall t\ge1$$&lt;/dd>
&lt;/dl>
&lt;blockquote>
&lt;p>For $s\in S,a\in A$,
$$
Q^\pi(s,a) = \sum_{s^\prime\in S}T(s,a,s^\prime)\{R(s,a,s^\prime)+\gamma V^\pi(s^\prime)\}
$$&lt;/p>
&lt;p>$Q^\pi(s,\pi(s))=V^\pi(s)$&lt;/p>
&lt;p>All optimal policies have the same optimal action value function $Q^\star$.&lt;/p>
&lt;/blockquote>
&lt;dl>
&lt;dt>$\text{IA}:\Pi\times S\to \mathcal{P}(A)$&lt;/dt>
&lt;dd>For $\pi\in\Pi,s\in S$,
$$\text{IA}(\pi,s)=\{a\in A:Q^\pi(s,a)&amp;gt;V^\pi(s)\}$$&lt;/dd>
&lt;dt>$\text{IS}:\Pi\to\mathcal{P}(S)$&lt;/dt>
&lt;dd>For $\pi\in\Pi$,
$$\text{IS}(\pi)=\{s\in S:|\text{IA}(\pi,s)|\ge 1\}$$&lt;/dd>
&lt;/dl>
&lt;blockquote>
&lt;p>&lt;strong>Policy Improvement Theorem&lt;/strong> :&lt;/p>
&lt;ol>
&lt;li>If $\text{IS}(\pi)=\emptyset$ then $\pi$ is an optimal policy, else&lt;/li>
&lt;li>if $\pi^\prime$ is obtained by policy improvement on $\pi$, then $\pi^\prime\succ\pi$.&lt;/li>
&lt;/ol>
&lt;p>$\text{IS}(\pi^\star)=\emptyset\iff B^\star(V^{\pi^\star})=V^{\pi^\star}$&lt;/p>
&lt;/blockquote>
&lt;dl>
&lt;dt>&lt;strong>Bellman Operator&lt;/strong> ($B^\pi:\R^n\to\R^n$)&lt;/dt>
&lt;dd>For $\pi\in\Pi,X:S\to\R,s\in S$,
$$(B^\pi(X))(s)=\sum_{s^\prime\in S}T(s,\pi(s),s^\prime)(R(s,\pi(s),s^\prime)+\gamma X(s^\prime))$$&lt;/dd>
&lt;/dl>
&lt;blockquote>
&lt;p>$B^\pi$ is a contraction mapping with contraction factor $\gamma$.&lt;/p>
&lt;p>For $X:s\to\R,\ \lim_{l\to\infty}(B^\pi)^l(X)=V^\pi$
For $X:s\to\R,Y:S\to\R,\ X\succeq Y\implies B^\pi(X)\succeq B^\pi(Y)$&lt;/p>
&lt;p>$B^{\pi^\prime}(V^\pi)(s)=Q^\pi(s,\pi^\prime(s))$&lt;/p>
&lt;/blockquote>
&lt;h5 id="policy-iteration-algorithm">Policy Iteration Algorithm&lt;/h5>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="ln">1&lt;/span>&lt;span class="cl">&lt;span class="k">def&lt;/span> &lt;span class="nf">Policy_iteration&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">mdp&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="ln">2&lt;/span>&lt;span class="cl"> &lt;span class="n">Pi&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">mdp&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">rand_policy&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="ln">3&lt;/span>&lt;span class="cl"> &lt;span class="k">while&lt;/span> &lt;span class="n">mdp&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">is_improvable&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">Pi&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="ln">4&lt;/span>&lt;span class="cl"> &lt;span class="n">Pi&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">mdp&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">improve&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">Pi&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="ln">5&lt;/span>&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="n">Pi&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h3 id="pi-variants">PI Variants&lt;/h3>
&lt;dl>
&lt;dt>&lt;strong>Howard&amp;rsquo;s Policy Iteration&lt;/strong>&lt;/dt>
&lt;dd>Greedy; switch all improvable states.&lt;/dd>
&lt;dt>&lt;strong>Random Policy Iteration&lt;/strong>&lt;/dt>
&lt;dd>Switch a non-empty subset of improvable states chosen uniformly at random.&lt;/dd>
&lt;dt>&lt;strong>Simple Policy Iteration&lt;/strong>&lt;/dt>
&lt;dd>Assume a fixed indexing of states and always improve the state with the highest index.&lt;/dd>
&lt;dt>&lt;strong>Upper Bound&lt;/strong> ($U(n,k)$)&lt;/dt>
&lt;dd>For a set of PI Variants $\mathcal{L}$ and MDP $M$, the expected number of policy evalutaions performed by $L$ on $M$ if initialised at any $\pi$ is at most $U(n,k)$.&lt;/dd>
&lt;/dl>
&lt;blockquote>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>PI Variant&lt;/th>
&lt;th>Type&lt;/th>
&lt;th>k=2&lt;/th>
&lt;th>General k&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>Howard&amp;rsquo;s PI&lt;/td>
&lt;td>Deterministic&lt;/td>
&lt;td>$\mathcal{O}(\frac{2^n}{n})$&lt;/td>
&lt;td>$\mathcal{O}(\frac{k^n}{n})$&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Mansour and Singh&amp;rsquo;s Random PI [MS99]&lt;/td>
&lt;td>Randomized&lt;/td>
&lt;td>$1.7172^n$&lt;/td>
&lt;td>$\mathcal{O}(\frac{k}{2})^n$&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Mansour and Singh&amp;rsquo;s Random PI [HPZ14]&lt;/td>
&lt;td>Randomized&lt;/td>
&lt;td>$\text{poly}(n)\cdot 1.5^n$&lt;/td>
&lt;td>&amp;ndash;&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;/blockquote>
&lt;dl>
&lt;dt>&lt;strong>Lower Bound&lt;/strong> ($X(n,k)$)&lt;/dt>
&lt;dd>For a set of PI Variants $\mathcal{L}$ and MDP $M$, the expected number of policy evalutaions performed by $L$ on $M$ if initialised at any $\pi$ is at least $X(n,k)$.&lt;/dd>
&lt;/dl>
&lt;blockquote>
&lt;ul>
&lt;li>Howard&amp;rsquo;s PI on $n$-state, 2-action MDPs : $\Omega(n)$&lt;/li>
&lt;li>Simple PI on $n$-state, 2-action MDPs : $\Omega(2^n)$&lt;/li>
&lt;/ul>
&lt;/blockquote>
&lt;h4 id="howards-pi-with-k2">Howard&amp;rsquo;s PI with $k=2$&lt;/h4>
&lt;blockquote>
&lt;p>Non-optimal policies $\pi,\pi^\prime\in\Pi$ cannot have the same set of improvable states.&lt;/p>
&lt;p>If $\pi$ has $m$ improvable states and $\pi$ states and $\pi,\pi^\prime$ (Howard&amp;rsquo;s PI) then there exist $m$ policies $\pi^{\prime\prime}$ such that $\pi^\prime\succeq\pi^{\prime\prime}\succ\pi$.&lt;/p>
&lt;/blockquote>
&lt;blockquote>
&lt;p>Number of iterations taken by Howard&amp;rsquo;s PI: $O(\frac{2^n}{n})$&lt;/p>
&lt;/blockquote>
&lt;h4 id="batch-switching-policy-iteration">Batch-Switching Policy Iteration&lt;/h4>
&lt;blockquote>
&lt;p>Howard&amp;rsquo;s Policy Iteration takes at most 3 iterations on 2-state 2-action MDP!&lt;/p>
&lt;/blockquote>
&lt;dl>
&lt;dt>&lt;strong>BSPI&lt;/strong>&lt;/dt>
&lt;dd>Partition states in 2-sized batches arranged from right to left. Improve the rightmost set containing an improvable state.&lt;/dd>
&lt;/dl>
&lt;blockquote>
&lt;p>BSPI of batch size 2 is bounded by $\mathcal{O}(\sqrt{3}^n)$.
Tighter bounds for higher batch-sizes.&lt;/p>
&lt;/blockquote>
&lt;h2 id="reinforcement-learning">Reinforcement Learning&lt;/h2>
&lt;h3 id="problem-definitions">Problem Definitions&lt;/h3>
&lt;dl>
&lt;dt>&lt;strong>MDP Histories&lt;/strong> ($H$)&lt;/dt>
&lt;dd>$H = \{h^t\ |\ t\ge 0, h^t=(s^0, a^0, r^0, s^1,\dots,r^{t-1},s^t)\}$
&lt;ul>
&lt;li>The environment decides the initial and state $s^0$.&lt;/li>
&lt;li>After $i\ge 0$ transitions, the agent choses the action $a_i$.&lt;/li>
&lt;li>The environment decides the reward $r_i$ and the next state $s_{i+1}$ based on an underlying MDP unknown to the agent.&lt;/li>
&lt;/ul>
&lt;/dd>
&lt;dt>&lt;strong>Learning Algorithm&lt;/strong> ($L:H\to A$)&lt;/dt>
&lt;dd>Takes a history $h^t$ and returns a particular action $L(h^t)$ for the agent&lt;/dd>
&lt;/dl>
&lt;blockquote>
&lt;p>&lt;strong>Control Problem&lt;/strong> : Come up with an $L$ such that,
$$
\lim_{|H|\to\infty}\frac{1}{|H|}\left(\sum_{0}^{|H|-1}\mathbb{P}[\ L(h^t)=\pi^\star(s^t)\ ]\right)=1
$$&lt;/p>
&lt;/blockquote>
&lt;dl>
&lt;dt>&lt;strong>Learning Algorithm&lt;/strong> ($\hat V:H\to (S\to\R)$)&lt;/dt>
&lt;dd>Takes a history $h^t$ and returns a value function $\hat V(h^t)$&lt;/dd>
&lt;/dl>
&lt;blockquote>
&lt;p>&lt;strong>Prediction Problem&lt;/strong> : Given a policy $\pi$, come up with an $L$ such that,
$$
\lim_{t\to\infty}\hat V(h^t)=V^\pi
$$
where the histories $h^t$ are generate using an agent that follows the policy $\pi$.&lt;/p>
&lt;/blockquote>
&lt;h4 id="mdp-assumptions">MDP Assumptions&lt;/h4>
&lt;dl>
&lt;dt>&lt;strong>Irreducible&lt;/strong>&lt;/dt>
&lt;dd>An MDP in which $\forall\pi\in\Pi$, there exists a directed path between any two states $s,s^\prime\in S$&lt;/dd>
&lt;dt>&lt;strong>Aperiodic&lt;/strong>&lt;/dt>
&lt;dd>An MDP in which $\forall\pi\in\Pi$, $\forall s\in S$, $\gcd(Y^\pi(s))=1$.
&lt;ul>
&lt;li>$X^\pi(s,t)$ is the set of all states that can be reached from $s$ in $t$ step following policy $\pi$.&lt;/li>
&lt;li>$Y^\pi(s)$ is the set of all $t$ such that $s\in X^\pi(s,t)$.&lt;/li>
&lt;/ul>
&lt;/dd>
&lt;dt>&lt;strong>Ergodic&lt;/strong>&lt;/dt>
&lt;dd>An MDP that is irreducible and aperiodic.
&lt;ul>
&lt;li>This means for every policy $\pi$ there exists a unique steady state distribution $\mu^\pi:S\to\R$ subject to $\sum_{s\in S}\mu^\pi(s)=1$&lt;/li>
&lt;li>For histories $h^t$ generated by an agent following policy $\pi$ from an arbitrary $s^0$,
$$
\mu^\pi(s)=\lim_{t\to\infty}\mathbb{P}[s^t=s]
$$&lt;/li>
&lt;/ul>
&lt;/dd>
&lt;/dl>
&lt;h3 id="control-algorithms">Control Algorithms&lt;/h3>
&lt;h4 id="glie">GLIE&lt;/h4>
&lt;p>Model is valid only after all the possible pairs of states and action are picked at least once.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="ln">1&lt;/span>&lt;span class="cl">&lt;span class="k">def&lt;/span> &lt;span class="nf">L&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">h_t&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="ln">2&lt;/span>&lt;span class="cl"> &lt;span class="n">modelValid&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">pred_T&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">pred_R&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">UpdateModel&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">h_t&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="ln">3&lt;/span>&lt;span class="cl"> &lt;span class="n">p&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">random&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Uniform&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="ln">4&lt;/span>&lt;span class="cl"> &lt;span class="k">if&lt;/span> &lt;span class="n">modelValid&lt;/span> &lt;span class="ow">and&lt;/span> &lt;span class="n">p&lt;/span> &lt;span class="o">&amp;lt;&lt;/span> &lt;span class="mi">1&lt;/span> &lt;span class="o">-&lt;/span> &lt;span class="n">epsilon_t&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="ln">5&lt;/span>&lt;span class="cl"> &lt;span class="n">pi_star&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">MDPPlanner&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">S&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">A&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">pred_T&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">pred_R&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">gamma&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="ln">6&lt;/span>&lt;span class="cl"> &lt;span class="n">a_t&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">pi_star&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">s_t&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="ln">7&lt;/span>&lt;span class="cl"> &lt;span class="k">else&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="ln">8&lt;/span>&lt;span class="cl"> &lt;span class="n">a_t&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">random&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Choice&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">A&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="ln">9&lt;/span>&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="n">a_t&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h4 id="q-learning">Q-Learning&lt;/h4>
&lt;ol>
&lt;li>Maintain action state values $\hat Q:S\times A\to\R$.&lt;/li>
&lt;li>For every action, make the following updates,
$$
\hat Q^{t+1}(s^t,a^t) = \hat Q^{t}(s^t,a^t)+\alpha_{t+1}(r^t+\gamma\max_{a\in A}\hat Q^t(s^{t+1},a) - \hat Q^t(s^t,a^t))
$$&lt;/li>
&lt;li>Pick the action $\arg\max_{a\in A}\hat Q^T(s^T,a)$&lt;/li>
&lt;/ol>
&lt;h4 id="sarsa">Sarsa&lt;/h4>
&lt;ol>
&lt;li>Maintain action state values $\hat Q:S\times A\to\R$.&lt;/li>
&lt;li>For every action, make the following updates,
$$
\hat Q^{t+1}(s^t,a^t) = \hat Q^{t}(s^t,a^t)+\alpha_{t+1}(r^t+\gamma\hat Q^t(s^{t+1},a^{t+1}) - \hat Q^t(s^t,a^t))
$$&lt;/li>
&lt;li>Pick the action $\arg\max_{a\in A}\hat Q^T(s^T,a)$&lt;/li>
&lt;/ol>
&lt;h4 id="expected-sarsa">Expected Sarsa&lt;/h4>
&lt;ol>
&lt;li>Maintain action state values $\hat Q:S\times A\to\R$.&lt;/li>
&lt;li>For every action, make the following updates,
$$
\hat Q^{t+1}(s^t,a^t) = \hat Q^{t}(s^t,a^t)+\alpha_{t+1}(r^t+\gamma\sum_{a\in A}\pi^t(s^{t+1},a)\hat Q^t(s^{t+1},a) - \hat Q^t(s^t,a^t))
$$&lt;/li>
&lt;li>Pick the action $\arg\max_{a\in A}\hat Q^T(s^T,a)$&lt;/li>
&lt;/ol>
&lt;h4 id="neural-network-based-hat-q">Neural Network based $\hat Q$&lt;/h4>
&lt;p>When the state space is made up of variables that take up real values, these state vectors $s$ can be passed to a neural network to return $\hat Q(s,a)$ for all actions $a$.&lt;/p>
&lt;h4 id="linear-td-based-hat-q-estimation">Linear TD based $\hat Q$ estimation&lt;/h4>
&lt;p>$$
\hat Q(w,s,a) = \langle w, x(s,a)\rangle\\
\ \\
$$
This relation is used to perform &lt;a href="#linear-tdlambda">linear TD&lt;/a> learning followed by anyone of the above 3 algorithms. Linear Sarsa($\lambda$) is popular.&lt;/p>
&lt;h3 id="prediction-algorithms">Prediction Algorithms&lt;/h3>
&lt;h4 id="monte-carlo-estimation">Monte Carlo Estimation&lt;/h4>
&lt;dl>
&lt;dt>$\boldsymbol{1}:S\times\N\times\N\to\{0,1\}$&lt;/dt>
&lt;dd>$\boldsymbol{1}(s,i,j)$ is 1 when state $s$ is visited at least $j$ times on episode $i$ else 0&lt;/dd>
&lt;dt>$G:S\times\N\times\N\to\R$&lt;/dt>
&lt;dd>$G(s,i,j)$ is the discounted long term reward on episode $i$ from $j^\text{th}$ visit of state $s$&lt;/dd>
&lt;/dl>
&lt;p>Working estimates of value function&lt;/p>
&lt;ul>
&lt;li>First visit:
$$
\hat V^N_{\text{First-visit}} = \frac{\sum_{i=1}^{N}G(s,i,1)}{\sum_{i=1}^{N}\boldsymbol{1}(s,i,1)}
$$&lt;/li>
&lt;li>Every visit:
$$
\hat V^N_{\text{Every-visit}} = \frac{\sum_{i=1}^{N}\sum_{j=1}^{\infty}G(s,i,j)}{\sum_{i=1}^{N}\sum_{j=1}^{\infty}\boldsymbol{1}(s,i,j)}
$$&lt;/li>
&lt;li>Second visit:
$$
\hat V^N_{\text{Second-visit}} = \frac{\sum_{i=1}^{N}G(s,i,2)}{\sum_{i=1}^{N}\boldsymbol{1}(s,i,2)}
$$&lt;/li>
&lt;/ul>
&lt;blockquote>
&lt;p>The last visit estimate doesn&amp;rsquo;t converge to $V^\pi$.&lt;/p>
&lt;/blockquote>
&lt;p>Online implementation,&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="ln">1&lt;/span>&lt;span class="cl">&lt;span class="k">def&lt;/span> &lt;span class="nf">episodicUpdate&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">V&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">t&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">G&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="ln">2&lt;/span>&lt;span class="cl"> &lt;span class="k">if&lt;/span> &lt;span class="n">t&lt;/span> &lt;span class="o">==&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="ln">3&lt;/span>&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="p">[&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">]&lt;/span>&lt;span class="o">*&lt;/span>&lt;span class="nb">len&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">S&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="ln">4&lt;/span>&lt;span class="cl"> &lt;span class="n">alpha&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="o">/&lt;/span>&lt;span class="n">t&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="ln">5&lt;/span>&lt;span class="cl"> &lt;span class="k">for&lt;/span> &lt;span class="n">s&lt;/span> &lt;span class="ow">in&lt;/span> &lt;span class="n">S&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="ln">6&lt;/span>&lt;span class="cl"> &lt;span class="n">V&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">s&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="n">alpha&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">*&lt;/span>&lt;span class="n">V&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">s&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="n">alpha&lt;/span>&lt;span class="o">*&lt;/span>&lt;span class="n">G&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">s&lt;/span>&lt;span class="p">]&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Here $\alpha_t$ is called the learning rate. The above algorithm works for any $\alpha_t$ that follows,&lt;/p>
&lt;ul>
&lt;li>$\sum_{t=1}^\infty\alpha_t=\infty$&lt;/li>
&lt;li>$\sum_{t=1}^\infty\alpha_t^2&amp;lt;\infty$&lt;/li>
&lt;/ul>
&lt;h4 id="temporal-difference-learning-td0">Temporal Difference Learning: TD(0)&lt;/h4>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="ln"> 1&lt;/span>&lt;span class="cl">&lt;span class="k">def&lt;/span> &lt;span class="nf">hat_V&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">h_T&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="ln"> 2&lt;/span>&lt;span class="cl"> &lt;span class="n">T&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="nb">len&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">h_T&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">//&lt;/span>&lt;span class="mi">3&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="ln"> 3&lt;/span>&lt;span class="cl"> &lt;span class="n">V&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="p">[&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">]&lt;/span>&lt;span class="o">*&lt;/span>&lt;span class="nb">len&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">S&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="ln"> 4&lt;/span>&lt;span class="cl"> &lt;span class="k">for&lt;/span> &lt;span class="n">t&lt;/span> &lt;span class="ow">in&lt;/span> &lt;span class="nb">range&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="n">T&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="ln"> 5&lt;/span>&lt;span class="cl"> &lt;span class="n">alpha&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="o">/&lt;/span>&lt;span class="n">t&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="ln"> 6&lt;/span>&lt;span class="cl"> &lt;span class="n">s&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">h_T&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="mi">3&lt;/span>&lt;span class="o">*&lt;/span>&lt;span class="n">t&lt;/span>&lt;span class="p">]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="ln"> 7&lt;/span>&lt;span class="cl"> &lt;span class="n">r&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">h_T&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="mi">3&lt;/span>&lt;span class="o">*&lt;/span>&lt;span class="n">t&lt;/span>&lt;span class="o">+&lt;/span>&lt;span class="mi">2&lt;/span>&lt;span class="p">]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="ln"> 8&lt;/span>&lt;span class="cl"> &lt;span class="n">s_next&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">h_T&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="mi">3&lt;/span>&lt;span class="o">*&lt;/span>&lt;span class="n">t&lt;/span>&lt;span class="o">+&lt;/span>&lt;span class="mi">3&lt;/span>&lt;span class="p">]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="ln"> 9&lt;/span>&lt;span class="cl"> &lt;span class="n">V&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">s&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">V&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">s&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="n">alpha&lt;/span>&lt;span class="o">*&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">r&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="n">gamma&lt;/span>&lt;span class="o">*&lt;/span>&lt;span class="n">V&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">s_next&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">-&lt;/span> &lt;span class="n">V&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">s&lt;/span>&lt;span class="p">])&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="ln">10&lt;/span>&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="n">V&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;ul>
&lt;li>$\hat V^t(s^t)$ is the current estimate after $t$ steps&lt;/li>
&lt;li>$r^t+\gamma\hat V^t(s^{t+1})$ is the new estimate&lt;/li>
&lt;li>&lt;code>(r + gamma*V[s_next] - V[s])&lt;/code> is the temporal difference prediction error&lt;/li>
&lt;/ul>
&lt;blockquote>
&lt;p>The first-visit and every-visit monte carlo estimates are the same as least square error estimates for some particular error functions.&lt;/p>
&lt;p>The TD(0) estimate is the same as the maximum likelihood estimate of the value function.&lt;/p>
&lt;/blockquote>
&lt;h4 id="n-step-td-learning">$n$-step TD Learning&lt;/h4>
&lt;dl>
&lt;dt>&lt;strong>$n$-step Returns&lt;/strong> ($G_{k:k+n}$)&lt;/dt>
&lt;dd>For a given history $h^t$,
$$
G_{k:k+n} = \sum_{i=0}^{n-1}\gamma^ir^{t+i}+\gamma^nV^{t+n-1}(s^{t+n})
$$&lt;/dd>
&lt;/dl>
&lt;p>For episodic tasks when terminal state is encountered at $k+n^\prime$ such that $1\le n^\prime&amp;lt;n$, take $G_{k:k+n}=G_{k:k+n^\prime}$.&lt;/p>
&lt;dl>
&lt;dt>&lt;strong>$n$-step TD&lt;/strong>&lt;/dt>
&lt;dd>Updates of the form,
$$
V^{t+n}(s^t)\leftarrow V^{t+n-1}(s^t) + \alpha_t(G_{t:t+n}-V^{t+n-1}(s^t))
$$&lt;/dd>
&lt;/dl>
&lt;blockquote>
&lt;p>Any convex linear normalized combination of the $n$-step returns can also be used.&lt;/p>
&lt;/blockquote>
&lt;h4 id="tdlambda-learning">TD($\lambda$) Learning&lt;/h4>
&lt;dl>
&lt;dt>&lt;strong>$\lambda$ Return&lt;/strong> ($G^\lambda_t$)&lt;/dt>
&lt;dd>For $\lambda\in(0,1]$,
$$
G_t^\lambda = (1-\lambda)\sum_{n=1}^{T-t-1}\lambda^{n-1}G_{t:t+n}+\lambda^{T-t-1}G_{t:T}
$$&lt;/dd>
&lt;/dl>
&lt;p>Here $s^T=s_{\top}$. TD learning with these returns is called TD($\lambda$) learning.&lt;/p>
&lt;h4 id="linear-tdlambda">Linear TD($\lambda$)&lt;/h4>
&lt;dl>
&lt;dt>&lt;strong>Features&lt;/strong> ($x:S\to\R^d$)&lt;/dt>
&lt;dd>A feature vector extracted from the given state that has all the relevant real-valued parameters that determine the state. Usually $d\ll|S|$.&lt;/dd>
&lt;dt>&lt;strong>Weights&lt;/strong> ($w\in\R^d$)&lt;/dt>
&lt;dd>A vector such that,
$$
\hat V(w, s) = \langle w, x(s)\rangle
$$&lt;/dd>
&lt;/dl>
&lt;p>Common best choice for $w$ is given by,
$$
w^\star = \argmin_{w\in\R^d} \text{MSVE}(w)\\
\ \\
\text{MSVE}(w) = \frac{1}{2}\sum_{s\in S}\mu^\pi(s)\cdot[V^\pi(s)-\hat V(w,s)]^2
$$&lt;/p>
&lt;p>This $w^\star$ can be found using stochastic gradient descent as,
$$
w^{t+1} \leftarrow w^t + \alpha_{t+1}\sum_{s\in S}\mu^\pi(s)\cdot[V^\pi(s)-\hat V(w^t,s^t)]\cdot\nabla_{w}\hat V(w^t,s^t)
$$&lt;/p>
&lt;p>In practice since $\mu^\pi(s)$ and $V^\pi(s)$ are unknown,
$$
w^{t+1} \leftarrow w^t + \alpha_{t+1}\cdot[G^\lambda_t-\langle w^t,x(s^t)\rangle]x(s^t)
$$
This is the Linear TD($\lambda$) algorithm.&lt;/p>
&lt;blockquote>
&lt;p>This algorithm converges as,
$$
\text{MSVE}(w^\infty_\lambda) \le \frac{1-\gamma\lambda}{1-\gamma}\cdot\text{MSVE}(w^\star)
$$&lt;/p>
&lt;/blockquote>
&lt;blockquote>
&lt;p>&lt;strong>Tile Coding&lt;/strong>: This is used when there is the features and the learning objective ($V$ or $Q$) is supposed to have a non-linear relationship. In this case every feature&amp;rsquo;s space is separately divided into tiles and feature vectors that give boolean encoding for lying within a tile are used in place of each of the original features.
$$
\hat V(w,s) = \sum_{j=1}^{d}F_j(x_j(s))\\
\ \\
F_j(x_j(s)) = \langle w_j,f_j(x_j(s))\rangle\\
\ \\
f_{ji} = \begin{Bmatrix}
1 &amp;amp; x_j(s) \text{ lies in }i^{\text{th}}\text{ tile}\\
0 &amp;amp; \text{otherwise}
\end{Bmatrix}
$$&lt;/p>
&lt;/blockquote>
&lt;h2 id="advanced-algorithms">Advanced Algorithms&lt;/h2>
&lt;h3 id="decision-time-planning">Decision Time Planning&lt;/h3>
&lt;h4 id="tree-search-on-mdps">Tree Search on MDPs&lt;/h4>
&lt;ol>
&lt;li>When at a state $s$, make a tree of states as nodes and actions as transitions such that different paths denote different possible trajectories.&lt;/li>
&lt;li>Fix a height $h=\Theta(\frac{1}{1-\gamma})$ of the tree.&lt;/li>
&lt;li>Set $Q^h=0$ for all the leaves.&lt;/li>
&lt;li>For internal nodes with $d=h-1,h-2,\dots$
$$
V^d(s)\leftarrow\max_{a\in A}Q^{d+1}(s,a)\\
\ \\
Q^d(s,a)\leftarrow\sum_{s^\prime\in S}T(s,a,s^\prime)\{R(s,a,s^\prime)+\gamma V^d(s^\prime)\}
$$&lt;/li>
&lt;/ol>
&lt;blockquote>
&lt;p>Drawbacks:&lt;/p>
&lt;ul>
&lt;li>Tree needs to be too large.&lt;/li>
&lt;li>Explores all clearly inferior branches.&lt;/li>
&lt;/ul>
&lt;/blockquote>
&lt;h4 id="rollout-policies">Rollout Policies&lt;/h4>
&lt;ul>
&lt;li>From current state $s$, for each action $a\in A$, generate $N$ trajectories by taking $a$ from $s$ and thereafter following $\pi$.&lt;/li>
&lt;li>Set $\hat Q(s,a)$ as average of episodic returns.&lt;/li>
&lt;li>$\pi^\prime(s)=\argmax_{a\in A}\hat Q(s,a)$&lt;/li>
&lt;li>Repeat this on the next state $s^\prime$ with $\pi^\prime$.&lt;/li>
&lt;/ul>
&lt;h4 id="monte-carlo-tree-search-uct-algorithm">Monte Carlo Tree search (UCT Algorithm)&lt;/h4>
&lt;p>Repeat $N$ times when at state $s_0$:&lt;/p>
&lt;ul>
&lt;li>Generate trajectories by calling model $M$.&lt;/li>
&lt;li>From $s$ take action $\argmax_{a\in A}\text{ucb}(s,a)$ where,
$$
\text{ucb}(s,a) = \hat Q(s,a) + C_p\sqrt{\frac{\ln{t}}{\text{visits}(s,a)}}
$$&lt;/li>
&lt;li>From leaves follow rollout policy $\pi$&lt;/li>
&lt;li>Update $\hat Q,\text{ucb}, \text{visits}$ for all $(s,a)$ visited in the trajectory&lt;/li>
&lt;li>$\pi(s)\leftarrow\argmax_{a\in A}\hat Q(s,a)$&lt;/li>
&lt;/ul>
&lt;p>Finally take action $\argmax_{a\in A}\text{ucb}(s_0,a)$&lt;/p>
&lt;h3 id="policy-search">Policy Search&lt;/h3>
&lt;ul>
&lt;li>Black-box optimization approach in the context of reinforcement learning&lt;/li>
&lt;li>Ignores the underlying markovian structure of the MDP&lt;/li>
&lt;li>Can be helpful when dealing with problems that don&amp;rsquo;t coform well with the MDP formulation&lt;/li>
&lt;li>Techniques involve grid search, random search, local search etc.&lt;/li>
&lt;li>Only effective when search space is of low dimensionality&lt;/li>
&lt;/ul>
&lt;h3 id="stochastic-policies">Stochastic Policies&lt;/h3>
&lt;dl>
&lt;dt>&lt;strong>Stochastic Policies&lt;/strong> ($\pi:S\times A\to[0,1]$)&lt;/dt>
&lt;dd>Probability distribution over action at every state such that $\sum_{a\in A}\pi(s,a)=1,\ \forall s\in S$&lt;/dd>
&lt;/dl>
&lt;h4 id="policy-gradient">Policy Gradient&lt;/h4>
&lt;p>Let $\theta$ be a parameter for $\pi$ and $x$ is a feature function such that,
$$
\pi(s,a;\theta) = \frac{e^{\theta\cdot x(s,a)}}{\sum_{b\in A}e^{\theta\cdot x(s,b)}}
$$&lt;/p>
&lt;p>Assuming $J(\theta)=V^\pi(s^0)$,
$$
\nabla_{\theta} J(\theta)=\mathbb{E_{\pi}}\left[\ \sum_{t=0}^{T-1}(\nabla_{\theta}\ln{\pi(s^t,a^t)})\cdot G_{t:T}\ \right]\\
\ \\
\theta\leftarrow\theta+\alpha\sum_{t=0}^{T-1}(\nabla_{\theta}\ln{\pi(s^t,a^t)})\cdot G_{t:T}
$$&lt;/p>
&lt;h4 id="variance-reduction">Variance Reduction&lt;/h4>
&lt;p>$$
\theta\leftarrow\theta+\alpha\sum_{t=0}^{T-1}(\nabla_{\theta}\ln{\pi(s^t,a^t)})\cdot (G_{t:T}-\hat V(s^t))
$$&lt;/p>
&lt;h4 id="actor-critic-method">Actor-Critic Method&lt;/h4>
&lt;ul>
&lt;li>Actor updates $\theta$ and hence $\pi_\theta$&lt;/li>
&lt;li>Critic evaluates $\hat V$ for $\pi_\theta$ (using say TD(0)) provides input for gradient descent updates
$$
\theta\leftarrow\theta+\alpha\sum_{t=0}^{T-1}(\nabla_{\theta}\ln{\pi(s^t,a^t)})\cdot (r^t + \hat V(s^{t+1})-\hat V(s^t))
$$&lt;/li>
&lt;/ul></description></item><item><title>Artificial Intelligence and Machine Learning</title><link>https://adityakadoo.github.io/Scrolls/courses/ai_ml/</link><pubDate>Mon, 05 Sep 2022 06:52:35 +0530</pubDate><author>aditya1449kadoo@gmail.com (Aditya Kadoo)</author><guid>https://adityakadoo.github.io/Scrolls/courses/ai_ml/</guid><description>&lt;h2 id="probability">Probability&lt;/h2>
&lt;h3 id="basic-terms">Basic Terms&lt;/h3>
&lt;h4 id="sample-space">Sample Space&lt;/h4>
&lt;dl>
&lt;dt>Sample Space $(S)$&lt;/dt>
&lt;dd>The set of all possible outcomes of an experiment.
$$
P(S)=1,P(\empty)=0
$$&lt;/dd>
&lt;/dl>
&lt;h4 id="probability-distribution">Probability Distribution&lt;/h4>
&lt;dl>
&lt;dt>Probability Distribution $(p)$&lt;/dt>
&lt;dd>A function that gives the probabilities of occurence of different possible outcomes of an experiment.
$$
p:S\rightarrow[0,1]\\
\sum_{x\in S}p(x)=1
$$&lt;/dd>
&lt;/dl>
&lt;h4 id="event">Event&lt;/h4>
&lt;dl>
&lt;dt>Event $(E)$&lt;/dt>
&lt;dd>A set of outcomes of an experiement i.e. a subset of the sample space.
$$
E\sube S
$$&lt;/dd>
&lt;/dl>
&lt;h4 id="probability-of-an-event">Probability of an Event&lt;/h4>
&lt;dl>
&lt;dt>Probability of an event $P(E)$&lt;/dt>
&lt;dd>The likelihood of an event happening. Mathematically given as,
$$
P(E)=\sum_{x\in S}p(x)\\
P(\overline E)=1-P(E),\ \overline E=S-E
P(E_1\cup E_2)=P(E_1)+P(E_2)-P(E_1\cap E_2)
$$&lt;/dd>
&lt;/dl>
&lt;h4 id="expectation">Expectation&lt;/h4>
&lt;dl>
&lt;dt>Expectation $E[.]$&lt;/dt>
&lt;dd>For a RV $X$ on $\R$ with PMF $P$ expectation is defined as,
$$
E[X]=\sum_xP(X=x)
$$&lt;/dd>
&lt;/dl>
&lt;blockquote>
&lt;p>&lt;strong>Linearity&lt;/strong>: $E[\alpha X+\beta Y]=\alpha E[X]+\beta E[Y]$&lt;/p>
&lt;/blockquote>
&lt;blockquote>
&lt;p>&lt;strong>MSE minimizer&lt;/strong>: Solution for $E[(X-z)^2]$ is $z=E[X]$&lt;/p>
&lt;/blockquote>
&lt;blockquote>
&lt;p>&lt;strong>Expectation of Product&lt;/strong>: $E[XY]=E[X]E[Y]$&lt;/p>
&lt;/blockquote>
&lt;h4 id="variance">Variance&lt;/h4>
&lt;dl>
&lt;dt>Varaince $\text{Var}[.]$&lt;/dt>
&lt;dd>For a RV $X$,
$$
\text{Var}[X]=E[X^2]-E[X]^2
$$&lt;/dd>
&lt;/dl>
&lt;blockquote>
&lt;p>$\text{Var}[\alpha X+\beta]=\alpha^2\text{Var}[X]$&lt;/p>
&lt;/blockquote>
&lt;blockquote>
&lt;p>$\text{Var}[X+Y]=\text{Var}[X]+\text{Var}[Y]$ if $X$ and $Y$ are independent&lt;/p>
&lt;/blockquote>
&lt;h4 id="covariance">Covariance&lt;/h4>
&lt;dl>
&lt;dt>Covariance $(\text{Cov}[.,.])$&lt;/dt>
&lt;dd>For RVs $X$ and $Y$, covariance is defined as
$$
\text{Cov}[X,Y]=E[XY]-E[X]E[Y]
$$&lt;/dd>
&lt;/dl>
&lt;blockquote>
&lt;p>$\text{Cov}[X,X]=\text{Var}[X]$&lt;/p>
&lt;/blockquote>
&lt;blockquote>
&lt;p>$\text{Cov}[X+Z,Y]=\text{Cov}[X,Y]+\text{Cov}[Z,Y]$&lt;/p>
&lt;/blockquote>
&lt;blockquote>
&lt;p>$\text{Cov}[X,Y]=0 \iff X$ and $Y$ are independent&lt;/p>
&lt;/blockquote>
&lt;h3 id="important-results">Important Results&lt;/h3>
&lt;h4 id="union-bound">Union Bound&lt;/h4>
&lt;p>$$
P(E_1\cup E_2)\le P(E_1)+P(E_2)
$$&lt;/p>
&lt;h4 id="disjoint-events">Disjoint Events&lt;/h4>
&lt;p>$$
P(\bigcup_{i=1}^nE_i)=\sum_{i=1}^np(x)
$$&lt;/p>
&lt;h4 id="conditional-probability">Conditional Probability&lt;/h4>
&lt;p>For two event $E_1$ and $E_2$,
$$
P(E_1|E_2)=\dfrac{P(E_1\cap E_2)}{P(E_2)}
$$&lt;/p>
&lt;h4 id="bayes-theorem">Bayes&amp;rsquo; Theorem&lt;/h4>
&lt;p>$$
P(E_1|E_2)=\dfrac{P(E_2|E_1)P(E_1)}{P(E_2)}
$$&lt;/p>
&lt;h4 id="marginal-distribution">Marginal Distribution&lt;/h4>
&lt;p>For 2 random variables $X$ and $Y$ the joint distribution is $P(X,Y)$ and the probability that $X=x$ is given by,
$$
P(X=x)=\sum_YP(X=x,Y=y)
$$&lt;/p>
&lt;h4 id="independent-random-variables">Independent Random Variables&lt;/h4>
&lt;p>$X$ and $Y$ are independent w.r.t. each other if,
$$
P(X=x,Y=y)=P(X=x)\cdot P(Y=y)
$$&lt;/p>
&lt;h4 id="chebyshevs-inequality">Chebyshev&amp;rsquo;s Inequality&lt;/h4>
&lt;p>If $X$ is a RV with mean $\mu$ and variance $\sigma^2$ then $\forall\alpha&amp;gt;0$
$$
P[|X-\mu|\ge\alpha]\le\dfrac{\sigma^2}{\alpha^2}
$$&lt;/p>
&lt;h4 id="convolution">Convolution&lt;/h4>
&lt;p>When $Z=X+Y$,
$$
P(Z=z)=\sum_xP(X=x)P(Y=z-x)
$$&lt;/p>
&lt;h3 id="data-to-pdf">Data to PDF&lt;/h3>
&lt;p>Consider a PDF $f:\R\rightarrow \R_0^+$ which needs to be found. We can generate samples from this PDF.
$$
E_f[x^k]=\lim_{n\rightarrow\infty}\dfrac{\sum_{i=1}^NX_i^k}{N}
$$
We define the moment generating function $M(\omega)$ as,
$$
M(\omega)=\int_{-\infty}^{\infty}e^{i\omega x}f(x)dx\\
=1+i\omega E[x]-\dfrac{\omega^2E[x^2]}{2!}-i\dfrac{\omega^3E[x^3]}{3!}\cdots
$$
By using the Inverse Fourier Transform we get,
$$
f(x)=\dfrac{1}{s\pi}\int_{-\infty}^{\infty}e^{-i\omega x}M(\omega)d\omega
$$&lt;/p>
&lt;h2 id="linear-algebra">Linear Algebra&lt;/h2>
&lt;h3 id="vectors-and-matrices">Vectors and Matrices&lt;/h3>
&lt;dl>
&lt;dt>Vector $v$&lt;/dt>
&lt;dd>Ordered sequence of numbers.&lt;/dd>
&lt;dt>Linearly Independent&lt;/dt>
&lt;dd>A set of vectors is LI if one of them can&amp;rsquo;t be reconstructed by taking linear combination of others.&lt;/dd>
&lt;dt>Matrix $A,B,&amp;hellip;$&lt;/dt>
&lt;dd>Ordered sequence of vectors.&lt;/dd>
&lt;/dl>
&lt;h3 id="linear-equations">Linear Equations&lt;/h3>
&lt;p>$$
Ax=B
$$
here $A\in\R^{m\times n}$, $x\in\R^n$ and $b\in\R^m$. This is solved using Gaussian Elimination.&lt;/p>
&lt;h3 id="vector-spaces">Vector Spaces&lt;/h3>
&lt;dl>
&lt;dt>Vector Space $(\mathcal V)$&lt;/dt>
&lt;dd>A set of vectors qualifies as a vector space if it is closed under the operation of summation and multiplication.&lt;/dd>
&lt;dt>Column Space $\mathcal C(A)$&lt;/dt>
&lt;dd>The vector space spanned by the column vectors of matrix $A$.&lt;/dd>
&lt;dt>Null Space $\mathcal N(A)$&lt;/dt>
&lt;dd>Solutions of the equation $Ax=0$.&lt;/dd>
&lt;dt>Rank $r(A)$&lt;/dt>
&lt;dd>The maximal number of linearly independent columns of matrix $A$.&lt;/dd>
&lt;/dl>
&lt;blockquote>
&lt;p>&lt;strong>Rank-Nullity Theorem&lt;/strong>: $r(\mathcal C(A))+r(\mathcal N(A))=\text{dim}(A)$&lt;/p>
&lt;/blockquote>
&lt;blockquote>
&lt;p>A square matrix $A$ of dimensions $n\times n$ is invertible iff $r(A)=n$&lt;/p>
&lt;/blockquote>
&lt;blockquote>
&lt;p>$\lim_{k\rightarrow\infty}W^k=\dfrac{\boldsymbol{1}\boldsymbol{1}^T}{n}$ iff&lt;/p>
&lt;/blockquote>
&lt;p>$$
\boldsymbol{1}^TW=\boldsymbol{1}^T\\
W\boldsymbol{1}=\boldsymbol{1}\\
\rho(W-\dfrac{\boldsymbol{1}\boldsymbol{1}^T}{n})&amp;lt;1
$$&lt;/p>
&lt;blockquote>
&lt;p>where $\rho(.)$ denotes the spectral radius of a matrix i.e. maximum of the absolute values of its eigenvalues.&lt;/p>
&lt;/blockquote>
&lt;h2 id="loss-function-design">Loss Function Design&lt;/h2>
&lt;h3 id="datasets">Datasets&lt;/h3>
&lt;dl>
&lt;dt>Train set&lt;/dt>
&lt;dd>The dataset on which we train the model.&lt;/dd>
&lt;dt>Validation set&lt;/dt>
&lt;dd>For hyper-parameter tuning&lt;/dd>
&lt;dt>Test set&lt;/dt>
&lt;dd>For judging the model&amp;rsquo;s accuracy on unseen data.&lt;/dd>
&lt;/dl>
&lt;p>Consider a classification task where we have a dataset $D$ with $(x_i,y_i)$ values. $y_i\in\{-1,1\}$ and $x_i\in\R^d$ is the feature vector. Lets say
$$
Y=H(X)
$$
We must find $H(.)$.&lt;/p>
&lt;h3 id="loss-minimization">Loss Minimization&lt;/h3>
&lt;p>We define a Loss Function $L(.)$ that takes a function $H:\R^d\rightarrow\{1,-1\}$ needs to be minimised.
$$
H^*=\text{arg}_H\text{min}L(H)
$$&lt;/p>
&lt;p>Following are possible examples of $L$ given by,&lt;/p>
&lt;ul>
&lt;li>General Hypothesis: $L(H) = \sum_{i\in D}\mathbb{I}(H(x_i)\ne y_i)$&lt;/li>
&lt;li>Constant Hypothesis: $L(H) = \sum_{i\in D}\mathbb{I}(c\ne y_i)$&lt;/li>
&lt;li>Linear Hypothesis: $L(H) = \sum_{i\in D}\mathbb{I}(w^Tx_i+b\ne y_i)$&lt;/li>
&lt;/ul>
&lt;p>Assuming Linear Hypothesis we can make following modifications:&lt;/p>
&lt;ul>
&lt;li>Linear Hypothesis with Absolute Difference:
$$
\{w^*,b^*\}=\text{arg}_{w,b}\text{min}\sum_D|w^Tx_i+b-y_i|
$$&lt;/li>
&lt;li>Linear Hypothesis with Signum and Indicator Cost:
$$
\{w^*,b^*\}=\text{arg}_{w,b}\text{min}\sum_D\mathbb{I}(\text{sgn}(w^Tx_i+b)\ne y_i)
$$&lt;/li>
&lt;li>Linear Hypothesis with Sigmoid Mapping:
$$
f(x_i)=\dfrac{1}{1+e^{-(w^Tx_i+b)}}
$$
$$
\{w^*,b^*\}=\text{arg}_{w,b}\text{min}\sum_D\mathbb{I}(f(x_i)\ne \dfrac{y_i+1}{2})
$$&lt;/li>
&lt;li>Linear, Sigmoid and ReLU:
$$
\{w^*,b^*\}=\text{arg}_{w,b}\text{min}\sum_D\text{max}(0,(\dfrac{1}{2}-f(x_i)\cdot y_i))
$$&lt;/li>
&lt;/ul>
&lt;p>Another example from Probabilistic Analysis:&lt;/p>
&lt;ul>
&lt;li>Binary Cross Entropy Loss:
$$
\{w^*,b^*\}=\text{arg}_{w,b}\text{min}\sum_D[-(\dfrac{y_i+1}{2})\log(f(x_i))-(1-\dfrac{y_i+1}{2})\log(1-f(x_i))]
$$&lt;/li>
&lt;/ul>
&lt;h3 id="accounting-for-noise">Accounting for Noise&lt;/h3>
&lt;p>When $L=\sum_{i\in D}\max(0,-y_i(w^Tx_i+b))$, if $w^Tx_i+b$ takes a very small positive or negative value then the loss function should not consider this reliable as it could be the result of noise in measurement. Thus to deal with such values, we can add a $\plusmn1$ around the decision boundary.&lt;/p>
&lt;h2 id="regression">Regression&lt;/h2>
&lt;p>Consider the problem of housing price prediction. We have a feature vector with $n$ features given by the column vector $x\in\R^{n\times1}$. The price of a house is modelled by the RV $Y$. We need to find a function $f:\R^n\rightarrow\R$ that models the relation between $X$ and $Y$.&lt;/p>
&lt;h3 id="mean-squared-loss">Mean Squared Loss&lt;/h3>
&lt;p>Assuming Gaussian noise between $f(x_i)$ and $y_i$ we get,
$$
y_i=f(x_i)+e_i\\
e_i=y_i-f(x_i)
$$
where $e\sim\mathcal{N}(0,\sigma^2)$. Maximizing the likelihood of $e_i$ we get the Mean Squared Loss.&lt;/p>
&lt;blockquote>
&lt;p>Similarly we get an $L_1$ Loss if we model noise as Laplacian distribution.&lt;/p>
&lt;/blockquote>
&lt;h3 id="solving-of-linear-regression-wrt-msl">Solving of Linear Regression wrt MSL&lt;/h3>
&lt;p>Assuming $f(x)=w^tx+b$ and,
$$
(w^*,b^*)=\arg\min\sum_{i\in D}(y_i-w^tx_i-b)^2
$$&lt;/p>
&lt;blockquote>
&lt;p>$b^*=E[Y]$&lt;/p>
&lt;/blockquote>
&lt;p>$$
w^*=\arg\dfrac{d((y-w^tx)^2)}{dw}=0
$$
On solving this we get,&lt;/p>
&lt;blockquote>
&lt;p>$w^*=\dfrac{y}{\lambda}(I-\dfrac{xx^T}{\lambda+||x||^2_2})x$&lt;/p>
&lt;/blockquote>
&lt;p>As $\lambda\rightarrow0$ this expression doesn&amp;rsquo;t give a solution. Another way of writing this solution is,&lt;/p>
&lt;blockquote>
&lt;p>$w^*=(X^TX)^{-1}X^TY$&lt;/p>
&lt;/blockquote>
&lt;h3 id="invertibility-of-xtx">Invertibility of $X^TX$&lt;/h3>
&lt;dl>
&lt;dt>Condition number&lt;/dt>
&lt;dd>The ratio of minimum eigenvalue to maximum eigenvalue.
$$
\text{Cond}(A)=\dfrac{\min(eigen(A))}{\max(eigen(A))}
$$&lt;/dd>
&lt;/dl>
&lt;p>A high condition number means the matrix can be inverted. A way to do this is to add a factor of $\lambda I$ to the matrix $X^TX$ since this lower bounds the condition number.This is also the solution of a particular Loss function as shown below.&lt;/p>
&lt;h2 id="regularisation">Regularisation&lt;/h2>
&lt;p>There are different types of regularisation techniques such as:&lt;/p>
&lt;ul>
&lt;li>L1 regularisation&lt;/li>
&lt;li>L2 regularisation&lt;/li>
&lt;li>Dropout regularisation&lt;/li>
&lt;/ul>
&lt;h3 id="l2-regularisation">L2 regularisation&lt;/h3>
&lt;p>$$
w^*=\sum_{i\in D}(y_i-w^Tx_i)^2+\lambda||w||^2
$$&lt;/p>
&lt;p>On solving we get,
$$
w^*=(X^TX+\lambda I)^{-1}X^TY
$$&lt;/p>
&lt;h3 id="overfitting">Overfitting&lt;/h3>
&lt;p>Overfitting occures when the model is constrained to the training set and not able to perform well on the test set, here the gap between the training error and testing error is large.&lt;/p></description></item><item><title>Natural Language Processing</title><link>https://adityakadoo.github.io/Scrolls/courses/natural_language_processing/</link><pubDate>Wed, 24 Aug 2022 09:49:01 +0530</pubDate><author>aditya1449kadoo@gmail.com (Aditya Kadoo)</author><guid>https://adityakadoo.github.io/Scrolls/courses/natural_language_processing/</guid><description>&lt;h2 id="part-of-speech-tagging">Part of Speech Tagging&lt;/h2>
&lt;h3 id="hmm-based-tagging">HMM-based Tagging&lt;/h3>
&lt;h4 id="parameters">Parameters&lt;/h4>
&lt;ul>
&lt;li>&lt;em>Input&lt;/em>: A sequence of words and labels&lt;/li>
&lt;li>&lt;em>Output&lt;/em>: A sequence of labels for every word&lt;/li>
&lt;/ul>
&lt;blockquote>
&lt;p>Penn tag-set is generally used for POS tagging in english language.&lt;/p>
&lt;/blockquote>
&lt;h4 id="hidden-markov-model">Hidden Markov Model&lt;/h4>
&lt;p>There are 2 kinds of probabilities:&lt;/p>
&lt;ol>
&lt;li>Bigram Probabilities $(P(t_1|t_0))$ : Probability of current word being tag $t_1$ when previous word was tagged $t_0$.&lt;/li>
&lt;li>Lexical Probabilities $(P(w|t))$: Probability of word $w$ given it is tagged $t$.&lt;/li>
&lt;/ol>
&lt;blockquote>
&lt;p>By Markov assumption, current word&amp;rsquo;s tag only depends on previous word&amp;rsquo;s tag.&lt;/p>
&lt;/blockquote>
&lt;h4 id="viterbi-algorithm">Viterbi Algorithm&lt;/h4>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-cpp" data-lang="cpp">&lt;span class="line">&lt;span class="ln"> 1&lt;/span>&lt;span class="cl">&lt;span class="cp">#define BProb(t1,t0) () &lt;/span>&lt;span class="c1">// Bigram
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="ln"> 2&lt;/span>&lt;span class="cl">&lt;span class="c1">&lt;/span>&lt;span class="cp">#define LProb(w,t) () &lt;/span>&lt;span class="c1">// Lexical
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="ln"> 3&lt;/span>&lt;span class="cl">&lt;span class="c1">&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="ln"> 4&lt;/span>&lt;span class="cl">&lt;span class="n">vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">string&lt;/span>&lt;span class="o">&amp;gt;&lt;/span> &lt;span class="n">viterbi&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">string&lt;/span>&lt;span class="o">&amp;gt;&lt;/span> &lt;span class="n">sentence&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">string&lt;/span>&lt;span class="o">&amp;gt;&lt;/span> &lt;span class="n">labels&lt;/span>&lt;span class="p">){&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="ln"> 5&lt;/span>&lt;span class="cl"> &lt;span class="kt">int&lt;/span> &lt;span class="n">n&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">sentence&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">size&lt;/span>&lt;span class="p">();&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="ln"> 6&lt;/span>&lt;span class="cl"> &lt;span class="kt">int&lt;/span> &lt;span class="n">l&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">labels&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">size&lt;/span>&lt;span class="p">();&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="ln"> 7&lt;/span>&lt;span class="cl"> &lt;span class="c1">// labels[0] = &amp;#34;^&amp;#34;
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="ln"> 8&lt;/span>&lt;span class="cl">&lt;span class="c1">&lt;/span> &lt;span class="n">vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="kt">float&lt;/span>&lt;span class="o">&amp;gt;&amp;gt;&lt;/span> &lt;span class="n">dp&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">n&lt;/span>&lt;span class="o">+&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="kt">float&lt;/span>&lt;span class="o">&amp;gt;&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">l&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">));&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="ln"> 9&lt;/span>&lt;span class="cl"> &lt;span class="n">vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">string&lt;/span>&lt;span class="o">&amp;gt;&amp;gt;&lt;/span> &lt;span class="n">plabel&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">n&lt;/span>&lt;span class="o">+&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="kt">int&lt;/span>&lt;span class="o">&amp;gt;&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">l&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">));&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="ln">10&lt;/span>&lt;span class="cl"> &lt;span class="n">dp&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">][&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="ln">11&lt;/span>&lt;span class="cl"> &lt;span class="k">for&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="kt">int&lt;/span> &lt;span class="n">i&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">;&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">n&lt;/span>&lt;span class="p">;&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="o">++&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="ln">12&lt;/span>&lt;span class="cl"> &lt;span class="k">for&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="kt">int&lt;/span> &lt;span class="n">j&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">;&lt;/span>&lt;span class="n">j&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">l&lt;/span>&lt;span class="p">;&lt;/span>&lt;span class="n">j&lt;/span>&lt;span class="o">++&lt;/span>&lt;span class="p">);&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="ln">13&lt;/span>&lt;span class="cl"> &lt;span class="k">for&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="kt">int&lt;/span> &lt;span class="n">k&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">;&lt;/span>&lt;span class="n">k&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">l&lt;/span>&lt;span class="p">;&lt;/span>&lt;span class="n">k&lt;/span>&lt;span class="o">++&lt;/span>&lt;span class="p">){&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="ln">14&lt;/span>&lt;span class="cl"> &lt;span class="kt">float&lt;/span> &lt;span class="n">p&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">dp&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">][&lt;/span>&lt;span class="n">k&lt;/span>&lt;span class="p">]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="ln">15&lt;/span>&lt;span class="cl"> &lt;span class="o">*&lt;/span> &lt;span class="n">BProb&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">labels&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">j&lt;/span>&lt;span class="p">],&lt;/span>&lt;span class="n">labels&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">k&lt;/span>&lt;span class="p">])&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="ln">16&lt;/span>&lt;span class="cl"> &lt;span class="o">*&lt;/span> &lt;span class="n">LProb&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">sentence&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">],&lt;/span>&lt;span class="n">labels&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">j&lt;/span>&lt;span class="p">]);&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="ln">17&lt;/span>&lt;span class="cl"> &lt;span class="k">if&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">dp&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="o">+&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">][&lt;/span>&lt;span class="n">j&lt;/span>&lt;span class="p">]&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">p&lt;/span>&lt;span class="p">){&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="ln">18&lt;/span>&lt;span class="cl"> &lt;span class="n">dp&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="o">+&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">][&lt;/span>&lt;span class="n">j&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">p&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="ln">19&lt;/span>&lt;span class="cl"> &lt;span class="n">plabel&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="o">+&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">][&lt;/span>&lt;span class="n">j&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">k&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="ln">20&lt;/span>&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="ln">21&lt;/span>&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="ln">22&lt;/span>&lt;span class="cl"> &lt;span class="n">vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">string&lt;/span>&lt;span class="o">&amp;gt;&lt;/span> &lt;span class="n">res&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">sentence&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">begin&lt;/span>&lt;span class="p">(),&lt;/span>&lt;span class="n">sentence&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">end&lt;/span>&lt;span class="p">());&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="ln">23&lt;/span>&lt;span class="cl"> &lt;span class="kt">int&lt;/span> &lt;span class="n">ptr&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="ln">24&lt;/span>&lt;span class="cl"> &lt;span class="k">for&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="kt">int&lt;/span> &lt;span class="n">i&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">;&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">l&lt;/span>&lt;span class="p">;&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="o">++&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="ln">25&lt;/span>&lt;span class="cl"> &lt;span class="n">ptr&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">dp&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">n&lt;/span>&lt;span class="p">][&lt;/span>&lt;span class="n">ptr&lt;/span>&lt;span class="p">]&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">dp&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">n&lt;/span>&lt;span class="p">][&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">?&lt;/span> &lt;span class="nl">i&lt;/span> &lt;span class="p">:&lt;/span> &lt;span class="n">ptr&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="ln">26&lt;/span>&lt;span class="cl"> &lt;span class="n">res&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">n&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">res&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">n&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">]&lt;/span>&lt;span class="o">+&lt;/span>&lt;span class="s">&amp;#34;_.&amp;#34;&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="ln">27&lt;/span>&lt;span class="cl"> &lt;span class="k">for&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="kt">int&lt;/span> &lt;span class="n">i&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">n&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">;&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="o">&amp;gt;&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">;&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="o">--&lt;/span>&lt;span class="p">){&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="ln">28&lt;/span>&lt;span class="cl"> &lt;span class="n">ptr&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">slabel&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">][&lt;/span>&lt;span class="n">ptr&lt;/span>&lt;span class="p">];&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="ln">29&lt;/span>&lt;span class="cl"> &lt;span class="n">res&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">+=&lt;/span> &lt;span class="s">&amp;#34;_&amp;#34;&lt;/span>&lt;span class="o">+&lt;/span>&lt;span class="n">labels&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">ptr&lt;/span>&lt;span class="p">];&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="ln">30&lt;/span>&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="ln">31&lt;/span>&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="n">res&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="ln">32&lt;/span>&lt;span class="cl">&lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h3 id="discriminative-learning">Discriminative Learning&lt;/h3>
&lt;p>HMM based POS tagging cannot handle &amp;ldquo;free word order&amp;rdquo; and &amp;ldquo;agglutination&amp;rdquo; well.&lt;/p>
&lt;h4 id="feature-engineering">Feature Engineering&lt;/h4>
&lt;ol>
&lt;li>Word-based features
&lt;ul>
&lt;li>$f_{21}$: Dictionary index of the current word&lt;/li>
&lt;li>$f_{22}$: Dictionary index of the previous word&lt;/li>
&lt;li>$f_{23}$: Dictionary index of the next word&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Part of Speech tag-based features
&lt;ul>
&lt;li>$f_{24}$: Index of POS of previous word&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Morphology-based features
&lt;ul>
&lt;li>$f_{25}$: does the current word have a noun suffix like &amp;rsquo;s&amp;rsquo;, &amp;rsquo;es&amp;rsquo;, &amp;lsquo;ies&amp;rsquo;, etc.&lt;/li>
&lt;li>$f_{26}$: does the current word have a verbal suffix like &amp;rsquo;d&amp;rsquo;, &amp;rsquo;ed&amp;rsquo;, &amp;rsquo;t&amp;rsquo;, etc.&lt;/li>
&lt;li>$f_{27}$ and $f_{28}$: above two for previous word.&lt;/li>
&lt;li>$f_{29}$ and $f_{2,10}$: above two for next word.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ol>
&lt;h4 id="morphology">Morphology&lt;/h4>
&lt;dl>
&lt;dt>Morphemes&lt;/dt>
&lt;dd>Smallest meaning-bearing units forming a word.
e.g.: In &amp;ldquo;quickly&amp;rdquo;, &amp;ldquo;quick&amp;rdquo; and &amp;ldquo;ly&amp;rdquo;.&lt;/dd>
&lt;/dl>
&lt;ul>
&lt;li>&lt;strong>Analytic Languages&lt;/strong>: Morphemes largely separate from one another.&lt;/li>
&lt;li>&lt;strong>Synthetic Languages&lt;/strong>: Joins the morphemes.&lt;/li>
&lt;/ul>
&lt;dl>
&lt;dt>Syncretism&lt;/dt>
&lt;dd>Overloading of roles per morpheme is called &lt;em>&lt;strong>syncretism&lt;/strong>&lt;/em>.
e.g.: &amp;ldquo;will go&amp;rdquo;: since number and person are indeterminate here&lt;/dd>
&lt;/dl>
&lt;h4 id="maximum-entropy-markov-model">Maximum Entropy Markov Model&lt;/h4>
&lt;p>$$
P(t_i=t|F_i)=\dfrac{e^{\sum_{j=1.k}\lambda_jf_{ij}}}{{\sum_{t&amp;rsquo;\in S}}e^{\sum_{j=1.k}\lambda_jf_{ij}(t&amp;rsquo;)}}
$$&lt;/p>
&lt;h4 id="beam-search-algorithm">Beam Search Algorithm&lt;/h4>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-cpp" data-lang="cpp">&lt;span class="line">&lt;span class="ln"> 1&lt;/span>&lt;span class="cl">&lt;span class="cp">#define Prob(t,w) ()
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="ln"> 2&lt;/span>&lt;span class="cl">&lt;span class="cp">&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="ln"> 3&lt;/span>&lt;span class="cl">&lt;span class="n">vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">string&lt;/span>&lt;span class="o">&amp;gt;&lt;/span> &lt;span class="n">beam_search&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">string&lt;/span>&lt;span class="o">&amp;gt;&lt;/span> &lt;span class="n">sentence&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">string&lt;/span>&lt;span class="o">&amp;gt;&lt;/span> &lt;span class="n">labels&lt;/span>&lt;span class="p">){&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="ln"> 4&lt;/span>&lt;span class="cl"> &lt;span class="kt">int&lt;/span> &lt;span class="n">n&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">sentence&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">size&lt;/span>&lt;span class="p">();&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="ln"> 5&lt;/span>&lt;span class="cl"> &lt;span class="kt">int&lt;/span> &lt;span class="n">l&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">labels&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">size&lt;/span>&lt;span class="p">();&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="ln"> 6&lt;/span>&lt;span class="cl"> &lt;span class="kt">int&lt;/span> &lt;span class="n">k&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">3&lt;/span>&lt;span class="p">;&lt;/span> &lt;span class="c1">// Beam size
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="ln"> 7&lt;/span>&lt;span class="cl">&lt;span class="c1">&lt;/span> &lt;span class="c1">// labels[0] = &amp;#34;^&amp;#34;
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="ln"> 8&lt;/span>&lt;span class="cl">&lt;span class="c1">&lt;/span> &lt;span class="n">vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">pair&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="kt">float&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="n">vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="kt">int&lt;/span>&lt;span class="o">&amp;gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span> &lt;span class="n">best&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">n&lt;/span>&lt;span class="o">+&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">pair&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="kt">float&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="ln"> 9&lt;/span>&lt;span class="cl"> &lt;span class="n">vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="kt">int&lt;/span>&lt;span class="o">&amp;gt;&amp;gt;&amp;gt;&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">k&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">make_pair&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="n">vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="kt">int&lt;/span>&lt;span class="o">&amp;gt;&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">n&lt;/span>&lt;span class="p">))));&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="ln">10&lt;/span>&lt;span class="cl"> &lt;span class="n">best&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">][&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">].&lt;/span>&lt;span class="n">first&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="ln">11&lt;/span>&lt;span class="cl"> &lt;span class="n">best&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">][&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">].&lt;/span>&lt;span class="n">second&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="ln">12&lt;/span>&lt;span class="cl"> &lt;span class="k">for&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="kt">int&lt;/span> &lt;span class="n">i&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">;&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">k&lt;/span>&lt;span class="p">;&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="o">++&lt;/span>&lt;span class="p">){&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="ln">13&lt;/span>&lt;span class="cl"> &lt;span class="n">best&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">][&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">].&lt;/span>&lt;span class="n">first&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="ln">14&lt;/span>&lt;span class="cl"> &lt;span class="n">best&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">][&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">].&lt;/span>&lt;span class="n">second&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">min&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="n">l&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">);&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="ln">15&lt;/span>&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="ln">16&lt;/span>&lt;span class="cl"> &lt;span class="k">for&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="kt">int&lt;/span> &lt;span class="n">i&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">;&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">n&lt;/span>&lt;span class="p">;&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="o">++&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="ln">17&lt;/span>&lt;span class="cl"> &lt;span class="k">for&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="kt">int&lt;/span> &lt;span class="n">j&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">;&lt;/span>&lt;span class="n">j&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">k&lt;/span>&lt;span class="p">;&lt;/span>&lt;span class="n">j&lt;/span>&lt;span class="o">++&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="ln">18&lt;/span>&lt;span class="cl"> &lt;span class="k">for&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="kt">int&lt;/span> &lt;span class="n">u&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">;&lt;/span>&lt;span class="n">u&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">l&lt;/span>&lt;span class="p">;&lt;/span>&lt;span class="n">u&lt;/span>&lt;span class="o">++&lt;/span>&lt;span class="p">){&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="ln">19&lt;/span>&lt;span class="cl"> &lt;span class="n">pair&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="kt">float&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="n">vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="kt">int&lt;/span>&lt;span class="o">&amp;gt;&amp;gt;&lt;/span> &lt;span class="n">p&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">make_pair&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">best&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">][&lt;/span>&lt;span class="n">j&lt;/span>&lt;span class="p">].&lt;/span>&lt;span class="n">first&lt;/span>&lt;span class="o">*&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="ln">20&lt;/span>&lt;span class="cl"> &lt;span class="n">Prob&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">best&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">][&lt;/span>&lt;span class="n">j&lt;/span>&lt;span class="p">].&lt;/span>&lt;span class="n">second&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">i&lt;/span>&lt;span class="o">+&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">label&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">u&lt;/span>&lt;span class="p">]),&lt;/span>&lt;span class="n">best&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">][&lt;/span>&lt;span class="n">j&lt;/span>&lt;span class="p">].&lt;/span>&lt;span class="n">second&lt;/span>&lt;span class="p">);&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="ln">21&lt;/span>&lt;span class="cl"> &lt;span class="n">labls&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">u&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="ln">22&lt;/span>&lt;span class="cl"> &lt;span class="k">for&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="kt">int&lt;/span> &lt;span class="n">v&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">;&lt;/span>&lt;span class="n">v&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">k&lt;/span>&lt;span class="p">;&lt;/span>&lt;span class="n">v&lt;/span>&lt;span class="o">++&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="ln">23&lt;/span>&lt;span class="cl"> &lt;span class="k">if&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">p&lt;/span>&lt;span class="o">&amp;gt;&lt;/span>&lt;span class="n">best&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="o">+&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">][&lt;/span>&lt;span class="n">v&lt;/span>&lt;span class="p">])&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="ln">24&lt;/span>&lt;span class="cl"> &lt;span class="n">swap&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">p&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="n">best&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="o">+&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">][&lt;/span>&lt;span class="n">v&lt;/span>&lt;span class="p">]);&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="ln">25&lt;/span>&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="ln">26&lt;/span>&lt;span class="cl"> &lt;span class="n">vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">string&lt;/span>&lt;span class="o">&amp;gt;&lt;/span> &lt;span class="n">res&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">n&lt;/span>&lt;span class="p">);&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="ln">27&lt;/span>&lt;span class="cl"> &lt;span class="n">pair&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="kt">float&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="n">vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="kt">int&lt;/span>&lt;span class="o">&amp;gt;&amp;gt;&lt;/span> &lt;span class="n">maxp&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">max_element&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">best&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">n&lt;/span>&lt;span class="p">].&lt;/span>&lt;span class="n">begin&lt;/span>&lt;span class="p">(),&lt;/span>&lt;span class="n">best&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">n&lt;/span>&lt;span class="p">].&lt;/span>&lt;span class="n">end&lt;/span>&lt;span class="p">());&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="ln">28&lt;/span>&lt;span class="cl"> &lt;span class="k">for&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="kt">int&lt;/span> &lt;span class="n">i&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">;&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">maxp&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">second&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">size&lt;/span>&lt;span class="p">();&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="o">++&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="ln">29&lt;/span>&lt;span class="cl"> &lt;span class="n">res&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">labels&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">maxp&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">second&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">]];&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="ln">30&lt;/span>&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="n">res&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="ln">31&lt;/span>&lt;span class="cl">&lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="parsing">Parsing&lt;/h2>
&lt;h3 id="context-free-grammar-parsing">Context Free Grammar Parsing&lt;/h3>
&lt;p>We are given a CFG with terminals as POS tags from the language and vairables from segment labels. This grammar is converted to Chomsky form.&lt;/p>
&lt;h4 id="cyk-algorithm">CYK Algorithm&lt;/h4>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-cpp" data-lang="cpp">&lt;span class="line">&lt;span class="ln"> 1&lt;/span>&lt;span class="cl">&lt;span class="k">class&lt;/span> &lt;span class="nc">node&lt;/span>&lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="ln"> 2&lt;/span>&lt;span class="cl"> &lt;span class="n">string&lt;/span> &lt;span class="n">label&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="ln"> 3&lt;/span>&lt;span class="cl"> &lt;span class="n">node&lt;/span>&lt;span class="o">*&lt;/span> &lt;span class="n">left&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="ln"> 4&lt;/span>&lt;span class="cl"> &lt;span class="n">node&lt;/span>&lt;span class="o">*&lt;/span> &lt;span class="n">right&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="ln"> 5&lt;/span>&lt;span class="cl">&lt;span class="p">};&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="ln"> 6&lt;/span>&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="ln"> 7&lt;/span>&lt;span class="cl">&lt;span class="n">node&lt;/span>&lt;span class="o">*&lt;/span> &lt;span class="nf">CYK&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">pair&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">string&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="n">string&lt;/span>&lt;span class="o">&amp;gt;&amp;gt;&lt;/span> &lt;span class="n">sent&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">map&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">pair&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">string&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="n">string&lt;/span>&lt;span class="o">&amp;gt;&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="n">string&lt;/span>&lt;span class="o">&amp;gt;&lt;/span> &lt;span class="n">rules&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="ln"> 8&lt;/span>&lt;span class="cl">&lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="ln"> 9&lt;/span>&lt;span class="cl"> &lt;span class="kt">int&lt;/span> &lt;span class="n">n&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">pos_labels&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">size&lt;/span>&lt;span class="p">();&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="ln">10&lt;/span>&lt;span class="cl"> &lt;span class="n">vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">pair&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="kt">int&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="n">string&lt;/span>&lt;span class="o">&amp;gt;&amp;gt;&amp;gt;&lt;/span> &lt;span class="n">dp&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">n&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="ln">11&lt;/span>&lt;span class="cl"> &lt;span class="n">pair&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="kt">int&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">string&lt;/span>&lt;span class="o">&amp;gt;&amp;gt;&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">n&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">make_pair&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="s">&amp;#34;&amp;#34;&lt;/span>&lt;span class="p">)));&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="ln">12&lt;/span>&lt;span class="cl"> &lt;span class="k">for&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="kt">int&lt;/span> &lt;span class="n">i&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">;&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">n&lt;/span>&lt;span class="p">;&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="o">++&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="ln">13&lt;/span>&lt;span class="cl"> &lt;span class="k">for&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="kt">int&lt;/span> &lt;span class="n">j&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">;&lt;/span>&lt;span class="n">j&lt;/span>&lt;span class="o">&amp;gt;=&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">;&lt;/span>&lt;span class="n">j&lt;/span>&lt;span class="o">--&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="ln">14&lt;/span>&lt;span class="cl"> &lt;span class="k">if&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="o">==&lt;/span>&lt;span class="n">j&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="ln">15&lt;/span>&lt;span class="cl"> &lt;span class="n">dp&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">j&lt;/span>&lt;span class="p">][&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">make_pair&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">sent&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">][&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">]);&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="ln">16&lt;/span>&lt;span class="cl"> &lt;span class="k">else&lt;/span>&lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="ln">17&lt;/span>&lt;span class="cl"> &lt;span class="n">dp&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">j&lt;/span>&lt;span class="p">][&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">make_pair&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="s">&amp;#34;---&amp;#34;&lt;/span>&lt;span class="p">);&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="ln">18&lt;/span>&lt;span class="cl"> &lt;span class="k">for&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="kt">int&lt;/span> &lt;span class="n">k&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">j&lt;/span>&lt;span class="o">+&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">;&lt;/span>&lt;span class="n">k&lt;/span>&lt;span class="o">&amp;lt;=&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">;&lt;/span>&lt;span class="n">k&lt;/span>&lt;span class="o">++&lt;/span>&lt;span class="p">){&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="ln">19&lt;/span>&lt;span class="cl"> &lt;span class="n">pair&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">string&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="n">string&lt;/span>&lt;span class="o">&amp;gt;&lt;/span> &lt;span class="n">rule&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">make_pair&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">dp&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">j&lt;/span>&lt;span class="p">][&lt;/span>&lt;span class="n">k&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">].&lt;/span>&lt;span class="n">second&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="ln">20&lt;/span>&lt;span class="cl"> &lt;span class="n">dp&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">k&lt;/span>&lt;span class="p">][&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">].&lt;/span>&lt;span class="n">second&lt;/span>&lt;span class="p">);&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="ln">21&lt;/span>&lt;span class="cl"> &lt;span class="k">if&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">rules&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">find&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">rule&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">!=&lt;/span>&lt;span class="n">rules&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">end&lt;/span>&lt;span class="p">()){&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="ln">22&lt;/span>&lt;span class="cl"> &lt;span class="n">dp&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">j&lt;/span>&lt;span class="p">][&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">make_pair&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">k&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">rules&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">rule&lt;/span>&lt;span class="p">]);&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="ln">23&lt;/span>&lt;span class="cl"> &lt;span class="k">break&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="ln">24&lt;/span>&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="ln">25&lt;/span>&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="ln">26&lt;/span>&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="ln">27&lt;/span>&lt;span class="cl"> &lt;span class="n">node&lt;/span>&lt;span class="o">*&lt;/span> &lt;span class="n">r&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="ln">28&lt;/span>&lt;span class="cl"> &lt;span class="c1">// make the tree
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="ln">29&lt;/span>&lt;span class="cl">&lt;span class="c1">&lt;/span> &lt;span class="k">return&lt;/span> &lt;span class="n">r&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="ln">30&lt;/span>&lt;span class="cl">&lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h4 id="shift-reduce-algorithm">Shift reduce algorithm&lt;/h4>
&lt;p>Using a stack and working through a Push-down automaton based on the language.&lt;/p>
&lt;h3 id="probabilistic-parsing">Probabilistic Parsing&lt;/h3>
&lt;p>In the normal CFG related to the language, we add probability value to each rule. This can be found using the dataset.&lt;/p>
&lt;p>Probability of a Parse Tree is defined as the product probabilities of all the rules used in the parse tree. This way we find the parse tree with highest probability.&lt;/p>
&lt;p>We can also define the probability of a sentence as the sum of probabilities of its parse trees.
$$
P(S) = \sum_{t} P(t)\cdot P(S|t) = \sum_{t} P(t)
$$&lt;/p>
&lt;h3 id="dependency-parsing">Dependency Parsing&lt;/h3>
&lt;p>Instead of creating chunks of words we create dependency relations between words itself. This creates a tree of words as nodes.&lt;/p>
&lt;h2 id="ffnnbp">FFNNBP&lt;/h2>
&lt;p>Use softmax or sigmoid for sentiment analysis.&lt;/p>
&lt;h2 id="wordnet">WordNet&lt;/h2>
&lt;ul>
&lt;li>&lt;em>&lt;strong>Syntagmatic&lt;/strong>&lt;/em>: Based on relations such as Synonym, antonym, etc. &lt;em>CAT&lt;/em> and &lt;em>ANIMAL&lt;/em>&lt;/li>
&lt;li>&lt;em>&lt;strong>Paradigmatic&lt;/strong>&lt;/em>: Based on Co-occurences. &lt;em>CAT&lt;/em> and &lt;em>MEW&lt;/em>&lt;/li>
&lt;/ul>
&lt;h3 id="wordnet-engineering">Wordnet Engineering&lt;/h3>
&lt;dl>
&lt;dt>Principles of Synset creation&lt;/dt>
&lt;dd>&lt;ul>
&lt;li>Minimality&lt;/li>
&lt;li>Coverage&lt;/li>
&lt;li>Replacibility&lt;/li>
&lt;/ul>
&lt;/dd>
&lt;/dl>
&lt;p>These synsets are used to create Syntagmatic ConceptNets.&lt;/p>
&lt;p>Calculate Lexical Semantic Association(LSA) i.e. matrix of co-occurence frequencies. Apply PCA to get Paradigmatic WordNets.&lt;/p>
&lt;h3 id="using-wordnets">Using WordNets&lt;/h3>
&lt;p>$$P(Context\ word | input\ word)=P(w_1|w_2)=\frac{e^{(u_{w_1}^Tu_{w_2})}}{\Sigma_k e^{(u_{w_1}^Tu_{w_k})}}$$&lt;/p>
&lt;blockquote>
&lt;p>Here $u_w$ is the word vector for $w$.&lt;/p>
&lt;/blockquote></description></item></channel></rss>